"id","front","back","source_file","tags"
"20260206-1105-21-1","Was ist Data Mining?","Der Prozess, aus großen Datenmengen interessante Muster und nützliches Wissen zu entdecken.","Kapitel 1 - Folien.txt","week-1 ws-25-26 definition data-mining source::kapitel-1-folien"
"20260206-1105-21-2","Welches Ziel verfolgt Data Mining?","Versteckte Muster und Strukturen in Datenbeständen mit statistischen Methoden aufdecken, um neue Erkenntnisse, Querverbindungen und Trends zu erkennen.","Kapitel 1 - Folien.txt","week-1 ws-25-26 goal data-mining source::kapitel-1-folien"
"20260206-1105-21-3","Nenne typische Anwendungen von Data Mining.","Warenkorbanalysen, Kreditwürdigkeits- und Betrugserkennung in Banken/Versicherungen, personalisierte Medikamente und Nebenwirkungsanalyse in der Pharmaindustrie, Muster in Genaktivitäten/-mutationen (Medizin/Bioinformatik).","Kapitel 1 - Folien.txt","week-1 ws-25-26 applications data-mining source::kapitel-1-folien"
"20260206-1105-21-4","Aus welchen Bereichen speist sich Data Mining?","Datenbanken, Statistik/Datenanalyse und Maschinelles Lernen.","Kapitel 1 - Folien.txt","week-1 ws-25-26 roots data-mining source::kapitel-1-folien"
"20260206-1105-21-5","Welche Herausforderungen sind typisch für Data Mining?","Große Datenmenge, hohe Dimensionalität, Heterogenität, Komplexität, Verteiltheit.","Kapitel 1 - Folien.txt","week-1 ws-25-26 challenges data-mining source::kapitel-1-folien""20260206-1112-05-1","Was ist das Ziel von Clustering?","Datenpunkte so zu gruppieren, dass die paarweisen Distanzen innerhalb eines Clusters klein und zwischen verschiedenen Clustern groß sind.","Kapitel 2 - Folien.txt","week-2 ws-25-26 clustering objective data-mining source::kapitel-2-folien"
"20260206-1112-05-2","Wie ist die euklidische Distanz zwischen zwei Punkten definiert?","Als Wurzel der Summe der quadrierten Koordinatendifferenzen.","Kapitel 2 - Folien.txt","week-2 ws-25-26 distance-metrics clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-3","Wie ist die Manhattan-Distanz zwischen zwei Punkten definiert?","Als Summe der absoluten Koordinatendifferenzen.","Kapitel 2 - Folien.txt","week-2 ws-25-26 distance-metrics clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-4","Wann ist eine direkte Visualisierung von Clustering-Ergebnissen möglich?","Nur bei zwei Dimensionen.","Kapitel 2 - Folien.txt","week-2 ws-25-26 clustering visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-5","Was ist die Interquartilsabstand (IQR) in einem Boxplot?","Die Differenz zwischen dem dritten und dem ersten Quartil: IQR = Q3 - Q1.","Kapitel 2 - Folien.txt","week-2 ws-25-26 boxplot visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-6","Wie werden die Whiskers in einem Boxplot definiert?","Sie reichen von Q1 - 1,5*IQR bis Q3 + 1,5*IQR; der Whisker endet am letzten Datenpunkt innerhalb dieses Bereichs.","Kapitel 2 - Folien.txt","week-2 ws-25-26 boxplot visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-7","Wann gilt ein Datenpunkt im Boxplot als Ausreißer?","Wenn er außerhalb der Whiskers liegt.","Kapitel 2 - Folien.txt","week-2 ws-25-26 boxplot visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-8","Warum ist Clustering für große Datenmengen rechnerisch anspruchsvoll?","Es gibt k^N mögliche Zuordnungen der N Punkte zu k Clustern, und paarweise Ähnlichkeitsberechnungen kosten O(N^2).","Kapitel 2 - Folien.txt","week-2 ws-25-26 clustering complexity data-mining source::kapitel-2-folien"
"20260206-1112-05-9","Was besagt der Fluch der hohen Dimensionen für Distanzen?","In sehr hohen Dimensionen haben fast alle Paare von Datenpunkten eine ähnliche Distanz.","Kapitel 2 - Folien.txt","week-2 ws-25-26 curse-of-dimensionality clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-10","Was ist der Unterschied zwischen agglomerativem und divisivem hierarchischem Clustering?","Agglomerativ (Bottom-up) startet mit Einzelpunkten und fusioniert Cluster; divisiv (Top-down) startet mit einem Cluster und teilt ihn schrittweise.","Kapitel 2 - Folien.txt","week-2 ws-25-26 hierarchical-clustering clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-11","Was zeigt ein Dendrogramm in der hierarchischen Clusteranalyse?","Die Blätter sind einzelne Datenpunkte; die Höhe einer Vereinigung entspricht der Distanz zwischen den Clustern.","Kapitel 2 - Folien.txt","week-2 ws-25-26 hierarchical-clustering dendrogram data-mining source::kapitel-2-folien"
"20260206-1112-05-12","Welche zwei grundlegenden Entscheidungen sind für hierarchische Clusteranalyse nötig?","Definition der Distanz zwischen Clustern und eine Stoppregel.","Kapitel 2 - Folien.txt","week-2 ws-25-26 hierarchical-clustering clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-13","Wie definieren die gängigen Linkage-Methoden die Distanz zwischen zwei Clustern?","Single: minimale Paar-Distanz; Complete: maximale Paar-Distanz; Average: durchschnittliche Paar-Distanz; Centroid: Distanz der Centroiden; Ward: Zunahme der Varianz beim Zusammenführen.","Kapitel 2 - Folien.txt","week-2 ws-25-26 linkage hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-14","Welche Linkage-Methoden sind direkt in nicht-euklidischen Räumen anwendbar?","Single, Complete und Average Linkage.","Kapitel 2 - Folien.txt","week-2 ws-25-26 linkage hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-15","Warum sind Centroid- und Ward-Linkage nicht immer direkt anwendbar?","Sie erfordern die Berechnung eines Centroiden, was z. B. bei String-Daten nicht direkt möglich ist.","Kapitel 2 - Folien.txt","week-2 ws-25-26 linkage hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-16","Was sind typische Stoppregeln bei agglomerativem Clustering?","Vorgegebene Anzahl von Clustern; maximale Distanz im neu entstandenen Cluster übersteigt Schwellenwert; durchschnittliche maximale Distanz steigt stark an; Cluster-Dichte fällt unter einen Schwellenwert.","Kapitel 2 - Folien.txt","week-2 ws-25-26 hierarchical-clustering stopping-criteria data-mining source::kapitel-2-folien"
"20260206-1112-05-17","Warum sollten Attribute vor dem Clustering standardisiert werden?","Damit unterschiedliche Skalen (z. B. Einkommen vs. Körpergröße) die Distanzberechnung nicht dominieren; Standardisierung auf Mittelwert 0 und Standardabweichung 1.","Kapitel 2 - Folien.txt","week-2 ws-25-26 preprocessing clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-18","Was ist ein Clustroid in nicht-euklidischen Räumen?","Ein Punkt aus dem Cluster mit minimaler Summe aller Distanzen zu den anderen Punkten oder minimaler maximaler Distanz zu den anderen Punkten.","Kapitel 2 - Folien.txt","week-2 ws-25-26 clustering non-euclidean data-mining source::kapitel-2-folien"
"20260206-1112-05-19","Wie ist die Zeitkomplexität von hierarchischem Clustering?","Single Linkage: O(N^2); andere Verfahren mit Priority Queue: O(N^2 log N); naiv können sie O(N^3) erreichen.","Kapitel 2 - Folien.txt","week-2 ws-25-26 complexity hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-20","Was ist die Zielsetzung von k-Means-Clustering?","Eine Partition in k Cluster mit möglichst kleiner durchschnittlicher Distanz innerhalb der Cluster.","Kapitel 2 - Folien.txt","week-2 ws-25-26 k-means clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-21","Wie läuft der k-Means-Algorithmus ab?","Initiale k Centroiden wählen, Punkte dem nächstgelegenen Centroid zuordnen, Centroiden neu berechnen, wiederholen bis Konvergenz.","Kapitel 2 - Folien.txt","week-2 ws-25-26 k-means algorithm data-mining source::kapitel-2-folien"
"20260206-1112-05-22","Welche Initialisierungsstrategien für k-Means sind üblich?","Zufällige Zuordnung und Centroid-Berechnung; Auswahl von k Punkten mit maximalen paarweisen Distanzen; hierarchisches Clustering auf Stichprobe und Wahl der Clustroiden.","Kapitel 2 - Folien.txt","week-2 ws-25-26 k-means initialization data-mining source::kapitel-2-folien"
"20260206-1112-05-23","Warum wird k-Means oft mit mehreren Initialisierungen ausgeführt?","Der Algorithmus konvergiert zwar, kann aber in einem lokalen Minimum landen; daher wählt man das beste Ergebnis aus mehreren Starts.","Kapitel 2 - Folien.txt","week-2 ws-25-26 k-means optimization data-mining source::kapitel-2-folien"
"20260206-1112-05-24","Wie kann man die Anzahl der Cluster k heuristisch wählen?","k schrittweise erhöhen (z. B. 2, 4, 8, 16, …) und dort verfeinern, wo sich relevante Änderungen zeigen; binäre Suche zur Reduktion des Aufwands.","Kapitel 2 - Folien.txt","week-2 ws-25-26 k-means model-selection data-mining source::kapitel-2-folien"
"20260206-1112-05-25","Was ist der BFR-Algorithmus?","Eine k-Means-Variante für sehr große Datensätze, die nicht in den Hauptspeicher passen; Cluster werden als multivariat normalverteilt mit unabhängigen Dimensionen angenommen.","Kapitel 2 - Folien.txt","week-2 ws-25-26 bfr clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-26","Wie werden Cluster im BFR-Algorithmus effizient repräsentiert?","Mit 2d + 1 Zahlen: n, SUM_1..SUM_d und SUMSQ_1..SUMSQ_d.","Kapitel 2 - Folien.txt","week-2 ws-25-26 bfr clustering statistics data-mining source::kapitel-2-folien"
"20260206-1112-05-27","Wie berechnet man Centroid und Varianz aus den BFR-Statistiken?","Centroid = SUM / n; Varianz = SUMSQ / n - (SUM / n)^2 (komponentenweise).","Kapitel 2 - Folien.txt","week-2 ws-25-26 bfr clustering statistics data-mining source::kapitel-2-folien"
"20260206-1112-05-28","Welche drei Mengen verwendet der BFR-Algorithmus?","Discard Set (DS): zugeordnete Punkte; Retained Set (RS): noch nicht zugeordnete Punkte; Compression Set (CS): zusammengefasste Mini-Cluster aus RS.","Kapitel 2 - Folien.txt","week-2 ws-25-26 bfr clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-29","Wie läuft der BFR-Algorithmus grob ab?","Initialisiere k Cluster; lade Daten in Chunks; ordne Punkte den DS-Clustern zu, wenn Distanz unter Schwellenwert; clustere verbleibende Punkte und bilde CS; merge CS ggf. mit DS; am Ende ordne CS und RS den nächstliegenden DS-Clustern zu.","Kapitel 2 - Folien.txt","week-2 ws-25-26 bfr algorithm data-mining source::kapitel-2-folien"
"20260206-1112-05-30","Nach welchem Kriterium wird ein Punkt im BFR-Algorithmus einem Cluster zugeordnet?","Minimaler Mahalanobis-Abstand zum Centroid und unter einem Schwellenwert.","Kapitel 2 - Folien.txt","week-2 ws-25-26 bfr clustering distance-metrics data-mining source::kapitel-2-folien"
"20260206-1112-05-31","Wie ist der Mahalanobis-Abstand im BFR-Kontext definiert und welche Schwellenwerte sind typisch?","M(x, c) = Σ_i ((x_i - c_i)/σ_i)^2; ca. 68% der Punkte haben M < d, ca. 95% haben M < 2d.","Kapitel 2 - Folien.txt","week-2 ws-25-26 bfr mahalanobis distance-metrics data-mining source::kapitel-2-folien"
"20260206-1112-05-32","Welche Einschränkungen von BFR adressiert der CURE-Algorithmus?","BFR setzt normalverteilte Cluster mit unabhängigen Dimensionen voraus; CURE erlaubt Cluster beliebiger Form.","Kapitel 2 - Folien.txt","week-2 ws-25-26 cure bfr clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-33","Wie funktioniert der CURE-Algorithmus in Grundzügen?","Zufallsstichprobe ziehen, hierarchisch clustern (ohne Centroid bevorzugt), repräsentative Punkte pro Cluster wählen (weit auseinander), diese Richtung Centroid um einen Anteil verschieben, Cluster anhand geringer max. Distanz der Repräsentanten zusammenführen, alle Punkte dem nächsten Repräsentanten zuordnen.","Kapitel 2 - Folien.txt","week-2 ws-25-26 cure clustering algorithm data-mining source::kapitel-2-folien"
"20260206-1122-46-1","Was sind zentrale Ziele der Dimensionsreduktion?","Aufdeckung versteckter Korrelationen Entfernung redundanter und verrauschter Merkmale Leichtere Interpretation und Visualisierung Schnellere Speicherung und Verarbeitung ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 dimension-reduction goals data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-2","Was ist das Grundprinzip der Dimensionsreduktion über Projektion?","Liegen Datenpunkte nahe einem d-dimensionalen Unterraum, werden sie durch ihre Projektionen auf diesen Unterraum repräsentiert.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 dimension-reduction principle data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-3","Wie werden die Achsen des Unterraums bei der varianz-basierten Dimensionsreduktion gewählt?","1. Faktor: Richtung mit der größten Streuung (maximale Varianz). Weitere Faktoren: jeweils orthogonal zu den vorherigen und mit maximaler verbleibender Varianz. ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 dimension-reduction factors data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-4","Was ist die Hauptkomponentenanalyse (PCA)?","Ein Verfahren der multivariaten Statistik und Linearen Algebra, das Daten durch eine lineare Transformation auf Hauptkomponenten projiziert und so eine beste lineare Approximation liefert.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca definition linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-5","Welche drei Hauptschritte umfasst die PCA?","Translation (Zentrierung): Mittelwert der Spalten auf 0 setzen. Rotation: Kovarianzmatrix bilden und Eigenvektoren bestimmen. Projektion: Daten auf die (wichtigsten) Hauptachsen projizieren. ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca procedure linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-6","Welche Rolle spielen Eigenvektoren und Eigenwerte in der PCA?","Eigenvektoren (Ladungsvektoren) definieren die Hauptachsen und sind orthonormal. Eigenwerte geben die Varianz entlang der jeweiligen Hauptachse an. Die Komponenten werden nach absteigenden Eigenwerten sortiert. ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca eigenvectors linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-7","Wie wählt man in der PCA die Anzahl der Hauptkomponenten?","Wähle so viele Komponenten, dass der kumulierte erklärte Varianzanteil hoch ist und eine weitere Komponente den Anteil nur noch gering erhöht.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca model-selection statistics data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-8","Was ist die Singulärwertzerlegung (SVD)?","Eine Zerlegung einer Matrix in U, Σ und Vᵀ: U und V haben orthonormale Spalten, Σ ist diagonal mit nicht-negativen Singulärwerten, r ist der Rang der Matrix.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd definition linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-9","Inwiefern ist SVD eine Verallgemeinerung der PCA?","SVD verallgemeinert PCA auf allgemeine (auch rechteckige) Matrizen; PCA lässt sich als spezielle Anwendung der SVD verstehen.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd pca relationship linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-10","Wie nutzt man SVD zur Dimensionsreduktion?","Man behält die größten Singulärwerte und setzt kleine Singulärwerte auf 0; so erhält man eine Rang-k-Approximation der Daten.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd dimension-reduction linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-11","Welches Kriterium wird für die Wahl von k bei SVD genannt?","Behalte typischerweise 80–90&nbsp;% der Energie (Summe der quadrierten Singulärwerte), um einen guten Kompromiss zwischen Kompression und Genauigkeit zu erhalten.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd model-selection linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1128-43-1","Was ist das Ziel von Empfehlungssystemen in der Regel?","Unbekannte Bewertungen zu schätzen, insbesondere die hohen Bewertungen, um passende Objekte zu empfehlen.","Kapitel 4 - Folien.txt","week-4 ws-25-26 recommendation-systems objective data-mining source::kapitel-4---folien"
"20260206-1128-43-2","Was ist eine Nutzenmatrix in Empfehlungssystemen?","Eine Matrix aus Nutzern (Zeilen), Objekten (Spalten) und Bewertungen in den Zellen.","Kapitel 4 - Folien.txt","week-4 ws-25-26 recommendation-systems utility-matrix data-mining source::kapitel-4---folien"
"20260206-1128-43-3","Was ist die Idee der inhaltsbasierten Analyse in Empfehlungssystemen?","Empfehle einem Nutzer ähnliche Objekte, wenn er bereits ähnliche Objekte positiv bewertet hat.","Kapitel 4 - Folien.txt","week-4 ws-25-26 content-based recommendation-systems data-mining source::kapitel-4---folien"
"20260206-1128-43-4","Was ist ein Objektprofil in der inhaltsbasierten Analyse?","Eine Menge von Merkmalen eines Objekts (z. B. Schauspieler, Regisseur, Genre).","Kapitel 4 - Folien.txt","week-4 ws-25-26 content-based object-profile data-mining source::kapitel-4---folien"
"20260206-1128-43-5","Wie wird ein Nutzerprofil bei der inhaltsbasierten Analyse typischerweise aufgebaut?","Durch gewichtete Kombination der Objektprofile, wobei die Gewichte aus zentrierten Nutzerbewertungen stammen.","Kapitel 4 - Folien.txt","week-4 ws-25-26 content-based user-profile data-mining source::kapitel-4---folien"
"20260206-1128-43-6","Welche Ähnlichkeitsmetrik wird in der inhaltsbasierten Analyse häufig genutzt?","Die Kosinus-Ähnlichkeit zwischen Nutzerprofil und Objektprofil (Wertebereich \(\([-1,1]\)\)).","Kapitel 4 - Folien.txt","week-4 ws-25-26 content-based cosine-similarity data-mining source::kapitel-4---folien"
"20260206-1128-43-7","Welche typischen Probleme hat die inhaltsbasierte Analyse?","Interessen müssen im Objektprofil abbildbar sein.Komplexe Zusammenhänge zwischen Interessen werden schlecht erfasst.Merkmale müssen manuell gewählt werden (evtl. fehlen wichtige Merkmale).","Kapitel 4 - Folien.txt","week-4 ws-25-26 content-based limitations data-mining source::kapitel-4---folien"
"20260206-1128-43-8","Was ist kollaboratives Filtern (nutzerbasiert)?","Schätze Bewertungen eines Nutzers aus den Bewertungen ähnlicher Nutzer (Nutzergruppen mit ähnlichen Präferenzen).","Kapitel 4 - Folien.txt","week-4 ws-25-26 collaborative-filtering user-based data-mining source::kapitel-4---folien"
"20260206-1128-43-9","Was ist kollaboratives Filtern für Objekte (item-based)?","Schätze Bewertungen aus den Bewertungen von ähnlichen Objekten, die der Nutzer bereits bewertet hat.","Kapitel 4 - Folien.txt","week-4 ws-25-26 collaborative-filtering item-based data-mining source::kapitel-4---folien"
"20260206-1128-43-10","Welche zwei Schätzvarianten werden beim item-basierten kollaborativen Filtern genannt?","Mittelwert der Bewertungen der k ähnlichsten Objekte.Gewichteter Mittelwert nach Ähnlichkeit der Objekte.","Kapitel 4 - Folien.txt","week-4 ws-25-26 collaborative-filtering item-based estimation data-mining source::kapitel-4---folien"
"20260206-1128-43-11","Wie wird die Ähnlichkeit zwischen zwei Objekten beim item-basierten kollaborativen Filtern typischerweise berechnet?","Über die Kosinus-Ähnlichkeit der Bewertungsvektoren der Objekte; bei zentrierten Bewertungen entspricht sie dem Pearson-Korrelationskoeffizienten.","Kapitel 4 - Folien.txt","week-4 ws-25-26 collaborative-filtering similarity data-mining source::kapitel-4---folien"
"20260206-1128-43-12","Nenne je einen Vorteil von kollaborativem Filtern und inhaltsbasierter Analyse.","Kollaboratives Filtern: Keine manuelle Merkmalsauswahl nötig; verschiedene Nutzerinteressen besser trennbar (item-based).Inhaltsbasiert: Benötigt keine Daten anderer Nutzer und kann neue/unpopuläre Objekte empfehlen; Empfehlungen sind erklärbar über Merkmale.","Kapitel 4 - Folien.txt","week-4 ws-25-26 recommendation-systems comparison data-mining source::kapitel-4---folien"
"20260206-1305-00-1","Was ist der Support einer Elementmenge \(I\)?","Der Support von \(I\) ist der Anteil der Warenkörbe, die alle Elemente aus \(I\) enthalten.","Kapitel 5 - Folien.txt","week-5 ws-25-26 support frequent-itemsets association-rules data-mining source::kapitel-5---folien"
"20260206-1305-00-2","Wann ist eine Elementmenge \(I\) eine häufige Elementmenge?","Wenn ihr Support mindestens den Schwellenwert s erreicht: sup(I) >= s.","Kapitel 5 - Folien.txt","week-5 ws-25-26 frequent-itemsets support association-rules data-mining source::kapitel-5---folien"
"20260206-1305-00-3","Wie ist die Konfidenz einer Assoziationsregel \(I \to j\) definiert?","conf(I -> j) = sup(I ∪ {j}) / sup(I).","Kapitel 5 - Folien.txt","week-5 ws-25-26 confidence association-rules data-mining source::kapitel-5---folien"
"20260206-1305-00-4","Welche beiden Schwellenwerte bestimmen, ob eine Regel \(I \to j\) ausgegeben wird?","Die Regel wird ausgegeben, wenn sup(I ∪ {j}) >= s und conf(I -> j) >= c.","Kapitel 5 - Folien.txt","week-5 ws-25-26 support confidence association-rules data-mining source::kapitel-5---folien"
"20260206-1305-00-5","Welche Monotonieeigenschaft nutzen häufige Elementmengen im A-Priori-Algorithmus?","Für J ⊆ I gilt sup(I) ≤ sup(J). Ist J nicht häufig, dann kann keine Obermenge von J häufig sein.","Kapitel 5 - Folien.txt","week-5 ws-25-26 apriori frequent-itemsets monotonicity data-mining source::kapitel-5---folien"
"20260206-1305-00-6","Wie findet A-Priori häufige Paare mit zwei Durchläufen?","1) Erster Durchlauf: Zähle alle einzelnen Elemente und bestimme die häufigen. 2) Zweiter Durchlauf: Zähle Paare nur, wenn beide Elemente häufig sind.","Kapitel 5 - Folien.txt","week-5 ws-25-26 apriori frequent-itemsets pairs data-mining source::kapitel-5---folien"
"20260206-1305-00-7","Wann ist eine k-Elementmenge ein Kandidat im A-Priori-Algorithmus?","Nur wenn alle (k-1)-elementigen Teilmengen häufig sind.","Kapitel 5 - Folien.txt","week-5 ws-25-26 apriori candidate-generation frequent-itemsets data-mining source::kapitel-5---folien"
"20260206-1305-00-8","Worin besteht die Besonderheit des PCY-Algorithmus im Vergleich zu A-Priori?","PCY nutzt im ersten Durchlauf zusätzlich Hash-Buckets für Paare und einen Bit-Vektor, um Kandidatenpaare für den zweiten Durchlauf stark zu reduzieren.","Kapitel 5 - Folien.txt","week-5 ws-25-26 pcy apriori comparison frequent-itemsets data-mining source::kapitel-5---folien"
"20260206-1305-00-9","Warum kann ein PCY-Bucket mit Anteil < s keine häufigen Paare enthalten?","Weil jedes Paar im Bucket höchstens so oft auftreten kann wie der Bucket selbst; ist der Bucket-Anteil < s, kann kein Paar darin Support >= s erreichen.","Kapitel 5 - Folien.txt","week-5 ws-25-26 pcy pruning support frequent-itemsets data-mining source::kapitel-5---folien"
"20260206-1305-00-10","Wann wird ein Paar {i, j} im 2. Durchlauf des PCY-Algorithmus gezählt?","Nur wenn beide Elemente häufig sind und das Paar in einen Bucket mit Bit-Wert 1 gehasht wird.","Kapitel 5 - Folien.txt","week-5 ws-25-26 pcy algorithm frequent-itemsets data-mining source::kapitel-5---folien"
"20260206-1305-00-11","Was sind die Hauptschritte im Algorithmus von Toivonen?","1) Ziehe eine Zufallsstichprobe und bestimme Kandidaten mit s’ = 0.9·s. 2) Baue die negative Grenze der Kandidaten. 3) Zähle Kandidaten und negative Grenze im Gesamtdatensatz. 4) Falls ein Element der negativen Grenze häufig ist, wiederhole mit neuer Stichprobe.","Kapitel 5 - Folien.txt","week-5 ws-25-26 toivonen sampling frequent-itemsets data-mining source::kapitel-5---folien"
"20260206-1305-00-12","Wie ist die negative Grenze (negative border) im Toivonen-Algorithmus definiert?","Eine Menge ist in der negativen Grenze, wenn sie kein Kandidat ist, aber alle direkten Untermengen Kandidaten (oder leer) sind.","Kapitel 5 - Folien.txt","week-5 ws-25-26 toivonen negative-border frequent-itemsets data-mining source::kapitel-5---folien"
"20260206-1305-00-13","Worin unterscheidet sich Toivonen grundlegend von A-Priori?","Toivonen nutzt eine Zufallsstichprobe mit kleinerem Schwellenwert und eine negative Grenze; er kann wiederholt werden, während A-Priori den gesamten Datensatz in festen Durchläufen verarbeitet.","Kapitel 5 - Folien.txt","week-5 ws-25-26 toivonen apriori comparison sampling data-mining source::kapitel-5---folien"
"20260207-2310-01-1","Was bedeutet One-Hot-Kodierung eines Textdokuments?","Darstellung als 0-1-Array, in dem jedes Element ein Merkmal repräsentiert; 1 bedeutet Merkmal vorhanden, 0 nicht vorhanden.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-2","Welche Alternative zur reinen Präsenz kann in einer One-Hot-Kodierung verwendet werden?","Statt binär kann die Anzahl/Vorkommenshäufigkeit eines Merkmals gezählt werden.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-3","Warum ist Bag-of-Words für Texte nur begrenzt geeignet?","Es berücksichtigt die Anordnung/Reihenfolge der Wörter nicht.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-4","Was ist ein N-Gramm in der Textverarbeitung?","Eine Folge von N aufeinanderfolgenden Wörtern.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-5","Was ist ein K-Shingle?","Ein Teil einer Zeichenkette der Länge k (Substring fester Länge).","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-6","Wie ist die Jaccarddistanz zwischen zwei Mengen \(X_1\) und \(X_2\) definiert?","d(X1,X2)=1 - |X1∩X2|/|X1∪X2|.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-7","Wie berechnet man die Jaccarddistanz aus One-Hot-Kodierungen?","Anzahl der Positionen mit genau einer 1 geteilt durch Anzahl der Positionen mit mindestens einer 1.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-8","Welches Ziel verfolgt Min-Hashing bei hochdimensionalen Dokumentrepräsentationen?","Es erzeugt kurze, ähnlichkeits-erhaltende Signaturen, damit Dokumente über Signaturen statt über volle Kodierungen verglichen werden.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-9","Wie ist eine Min-Hash-Funktion \(h_\pi(C)\) für eine Kodierung \(C\) definiert?","Wende eine Permutation \(\pi\) auf die Positionen an; \(h_\pi(C)\) ist der Index der ersten Position mit 1 in der permutierten Kodierung.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-10","Wie entsteht die Signatur einer Kodierung beim Min-Hashing?","Sie besteht aus den Min-Hash-Werten \(h_{\pi_1},\ldots,h_{\pi_n}\) mehrerer unabhängiger Permutationen.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-11","Welche zentrale Eigenschaft verbindet Min-Hashing und die Jaccarddistanz?","Für zwei Mengen \(C,D\) gilt \(P(h_\pi(C) \neq h_\pi(D)) = d(C,D)\); äquivalent \(P(h_\pi(C)=h_\pi(D))=1-d(C,D)\).","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-12","Wie kann man die Jaccarddistanz aus Min-Hash-Signaturen schätzen?","Durch den Anteil ungleicher Signaturpositionen: \(\hat d(C,D)=\#\{i: h_{\pi_i}(C) \neq h_{\pi_i}(D)\}/n\). Längere Signaturen liefern genauere Schätzungen.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-13","Wie kann man Permutationen beim Min-Hashing effizient simulieren?","Durch Zufallshashfunktionen, z.B. \(h(x)=(a x + b \bmod p) \bmod N\) mit Primzahl \(p>N\).","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-14","Was ist das Ziel von Locality Sensitive Hashing (LSH)?","Es beschränkt die Suche auf Paare von Signaturen, die mit hoher Wahrscheinlichkeit ähnlich sind, damit nur wenige Paare geprüft werden.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-15","Welcher Nachteil entsteht bei LSH trotz der Beschleunigung?","Es gibt False Negatives: ähnliche Paare, die nicht entdeckt werden.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-16","Wie funktioniert das Banding bei LSH?","Die Signaturmatrix wird in b Bänder mit je r Zeilen geteilt; jedes Band wird gehasht. Zwei Signaturen sind Kandidaten, wenn sie in mindestens einem Band im selben Bucket landen.","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-17","Wie lautet die LSH-Wahrscheinlichkeit, dass zwei Dokumente mit Ähnlichkeit \(t=1-d(C,D)\) mindestens ein Band teilen?","\(P=1-(1-t^r)^b\).","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2310-01-18","Wie beeinflussen \(b\) und \(r\) die False-Positive/False-Negative-Rate bei LSH?","Mehr Bänder (größeres b, kleineres r) erhöhen die Trefferwahrscheinlichkeit (weniger False Negatives, mehr False Positives); größere r reduziert Kandidaten (mehr False Negatives, weniger False Positives).","Kapitel 6 - Folien [updated 11.12.txt","week-6 ws-25-26 locality-sensitive-hashing min-hash one-hot-encoding jaccard-distance data-mining source::kapitel-6---folien-[updated-11.12"
"20260207-2315-39-1","Was ist die Entropie einer Zielvariablen \(Y\) in Entscheidungsbäumen?","H(Y) = -\sum_{y} p_y \log_2 p_y.","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-2","Wie interpretiert man hohe bzw. niedrige Entropie von \(Y\)?","Hohe Entropie bedeutet eine gleichmäßigere Verteilung von Y; niedrige Entropie eine ungleichmäßige Verteilung.","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-3","Wie ist die bedingte Entropie \(H(Y|X)\) definiert?","H(Y|X) = \sum_x P(X=x) H(Y|X=x).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-4","Nach welchem Kriterium wählt man in Entscheidungsbäumen den besten Split (nach Entropie)?","Wähle das Attribut/Split mit der kleinsten bedingten Entropie H(Y|X).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-5","Wie bestimmt man bei numerischen und kategorialen Attributen den Split im Entscheidungsbaum?","Numerisch: Wahl eines Schwellenwerts; kategorial: Auswahl einer Kategorie bzw. eines Kategorien-Sets.","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-6","Was ist das Ziel des Maximal-Margin-Classifiers (SVM) bei linear trennbaren Daten?","Finde eine Hyperebene mit maximalem Abstand (Margin) zu den Daten, sodass y(w0 + sum_i w_i x_i) >= 1 für alle Punkte gilt.","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-7","Wie hängt der Margin \(\gamma\) vom Gewichtsvektor \(w\) ab?","Für w′=(w1,…,wn) gilt \(\gamma = 1/||w′||\). Maximierung von \(\gamma\) entspricht Minimierung von \(\sum_i w_i^2\).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-8","Wie modelliert man bei nicht linear trennbaren Daten Strafterm/Fehlzuordnungen in SVMs?","Mit einer Zielfunktion \(\min \sum_i w_i^2 + C \cdot (\#\,\text{falsche Zuordnungen})\) bzw. mit Slacks \(\xi_j\).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-9","Wie beeinflusst der Parameter \(C\) den Trade-off bei nicht linear trennbaren Daten?","Großes C priorisiert korrekte Trennung (geringeres Fehlerrisiko); kleines C priorisiert großen Margin.","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-10","Wie ist die Hinge-Loss-Slack-Variable \(\xi_j\) definiert?","\(\xi_j = \max(0, 1 - y_j (w_0 + \sum_i w_i x_{ji}))\).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-11","Wann ist \(\xi_j = 0\), \(0<\xi_j\le 1\) und \(\xi_j>1\) bei der SVM?","\(\xi_j=0\) wenn der Punkt auf der richtigen Seite der Ränder liegt; \(0<\xi_j\le 1\) wenn korrekt klassifiziert aber innerhalb des Margin; \(\xi_j>1\) wenn auf der falschen Seite der Hyperebene.","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-12","Was sind Support-Vektoren im Support Vector Classifier?","Datenpunkte, die auf den Rändern oder auf der falschen Seite liegen; nur sie beeinflussen die Klassifikation neuer Punkte.","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-13","Wie lautet die Entscheidungsfunktion des Support Vector Classifiers in Form der Support-Vektoren?","f(x)=w0 + \sum_{s\in S} \alpha_s (x \cdot x_s).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-14","Wie erweitert man den linearen Support Vector Classifier zu einer SVM für nichtlineare Trennung?","Durch den Kernel-Trick: f(x)=w0 + \sum_{s\in S} \alpha_s K(x, x_s).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2315-39-15","Welche Kernel-Beispiele wurden genannt (polynomial, radial/RBF)?","Polynomial: K(x,x_s)=(1 + x\cdot x_s)^d; RBF: K(x,x_s)=exp(-r \sum_i (x_i - x_{si})^2).","Kapitel 7 - Folien [updated 19.12.txt","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"20260207-2323-38-1","Was ist ein Graph in der Form \(G=(V,E)\)?","Ein Graph besteht aus einer Knotenmenge V und einer Kantenmenge E, die Knotenpaare verbindet.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01"
"20260207-2323-38-2","Was ist das Ziel von Community Detection in Graphen?","Finde Mengen von Knoten mit hoher Kantendichte innerhalb der Menge und geringerer Dichte nach außen.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 community"
"20260207-2323-38-3","Wie ist Edge-Betweenness einer Kante definiert?","Anzahl aller kürzesten Pfade, die durch die Kante verlaufen; bei x kürzesten Pfaden zwischen zwei Knoten wird 1/x addiert.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 edge-betweenness"
"20260207-2323-38-4","Warum ist eine Kante mit hoher Edge-Betweenness oft eine Brücke zwischen Communities?","Weil viele kürzeste Pfade zwischen Gruppen über diese Kante laufen, was auf eine verbindende Rolle hinweist.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 edge-betweenness"
"20260207-2323-38-5","Welche Schritte hat der Girvan-Newman-Algorithmus?","Wiederhole: Edge-Betweenness aller Kanten berechnen, Kante mit maximaler Edge-Betweenness entfernen; verbundene Komponenten sind die Cluster.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 girvan-newman"
"20260207-2323-38-6","Warum ergibt der Girvan-Newman-Algorithmus ein hierarchisches Clustering?","Weil Kanten schrittweise entfernt werden und dadurch eine hierarchische Zerlegung in Komponenten entsteht.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 girvan-newman"
"20260207-2323-38-7","Wie wird Edge-Betweenness (BFS-Variante) berechnet?","1) BFS-Level vom Startknoten, 2) Anzahl kürzester Pfade pro Knoten bestimmen, 3) Flow bottom-up verteilen; 4) für alle Startknoten wiederholen, Kanten-Flows aufsummieren und durch 2 teilen.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 edge-betweenness"
"20260207-2323-38-8","Was misst die Modularität einer Graph-Partitionierung?","Sie vergleicht die tatsächliche Anzahl interner Kanten in Clustern mit dem erwarteten Anteil in einem Zufallsgraph und misst die Güte der Partition.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 modularity"
"20260207-2323-38-9","In welchem Bereich liegt die Modularität \(Q\) und was bedeutet \(Q>0\)?","Q liegt in [-1,1]. Q>0 bedeutet: Es gibt mehr Kanten innerhalb der Cluster als in einem Zufallsgraph erwartet.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 modularity"
"20260207-2323-38-10","Wie nutzt Girvan-Newman die Modularität?","Die Modularität wird verwendet, um eine optimale Partitionierung (Anzahl/Struktur der Cluster) zu wählen.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 modularity girvan-newman"
"20260207-2323-38-11","Welche Grundschritte hat Spectral Clustering (Graph → Clustering)?","1) Graph in Matrixform überführen (Adjazenz/Grad/Laplace), 2) Eigenwerte/-vektoren berechnen, 3) zweite/weitere Eigenvektoren für die Aufteilung nutzen.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 spectral-clustering"
"20260207-2323-38-12","Wie wird die Laplace-Matrix eines Graphen gebildet?","\(L = D - A\), wobei A die Adjazenzmatrix und D die Gradmatrix ist.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 laplacian"
"20260207-2323-38-13","Was ist die Gradmatrix \(D\) eines Graphen?","D ist diagonal; der Diagonaleintrag ist der Grad des jeweiligen Knotens.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 laplacian"
"20260207-2323-38-14","Welche Rolle spielt der zweitkleinste Eigenvektor der Laplace-Matrix im Spectral Clustering?","Der zu \(\lambda_2\) gehörige Eigenvektor (Fiedler-Vektor) wird zur Aufteilung der Knoten in zwei Cluster verwendet (z.B. nach Vorzeichen oder Schwellenwert).","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 spectral-clustering"
"20260207-2323-38-15","Wie erhält man mehr als zwei Cluster beim Spectral Clustering?","Durch zusätzliche Eigenvektoren (z.B. drittkleinster) und eine gemeinsame Partitionierung anhand mehrerer Eigenvektoren.","Kapitel 8 - Folien [updated 14.01.txt","week-8 ws-25-26 community-detection graph-data spectral-clustering data-mining source::kapitel-8---folien-[updated-14.01 spectral-clustering"
"20260207-2325-55-1","Was misst Degree Centrality eines Knotens?","Die Anzahl der ein- und ausgehenden Kanten (Verbundenheit) des Knotens.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 degree-centrality"
"20260207-2325-55-2","Wie ist Closeness Centrality \(CC(v)\) definiert?","CC(v) = (N-1) / \sum_u \delta(u,v), mit \(\delta(u,v)\) als kürzeste Distanz.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 closeness-centrality"
"20260207-2325-55-3","Was misst Betweenness Centrality eines Knotens?","Anteil der kürzesten Pfade zwischen allen Knotenpaaren, die über den Knoten laufen.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 betweenness-centrality"
"20260207-2325-55-4","Wie lautet die Formel der Betweenness Centrality \(CB(v)\)?","CB(v)=\sum_{i\ne j\ne v} \sigma_{ij}(v) / \sigma_{ij}.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 betweenness-centrality"
"20260207-2325-55-5","Was ist die Grundidee von PageRank?","PageRank ist die stationäre Besuchswahrscheinlichkeit eines Random Surfers; Seiten sind wichtig, wenn viele (wichtige) Seiten auf sie verlinken.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank"
"20260207-2325-55-6","Wie lautet die einfache PageRank-Gleichung ohne Dämpfungsfaktor?","r_i = \sum_{j \to i} r_j / d_j^+, wobei \(d_j^+\) der Outdegree von j ist.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank"
"20260207-2325-55-7","Wie wird die stochastische Adjazenzmatrix \(M\) für PageRank definiert?","M_{ij} = 1/d_j^+ falls j→i, sonst 0; Spalten summieren zu 1.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank"
"20260207-2325-55-8","Welches Gleichungssystem beschreibt PageRank in Matrixform?","\(\mathbf{r}=M\cdot\mathbf{r}\) mit \(M\) als stochastischer Adjazenzmatrix.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank"
"20260207-2325-55-9","Wie lautet die PageRank-Gleichung mit Dämpfungsfaktor \(\beta\)?","r_i = \beta \sum_{j\to i} r_j/d_j^+ + (1-\beta)/n.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank"
"20260207-2325-55-10","Wie ist die Google-Matrix \(A\) definiert?","A = \beta M + (1-\beta) * (1/n) * 1_{n\times n}.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank"
"20260207-2325-55-11","Was ist eine Sackgasse (dangling node) im PageRank?","Eine Seite ohne ausgehende Links; der Random Surfer kann sie nicht verlassen und Gewichtung „verschwindet“.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank-problems"
"20260207-2325-55-12","Was ist eine Spider Trap im PageRank?","Eine Gruppe von Seiten ohne ausgehende Links nach außen; Gewichtung konzentriert sich in der Gruppe.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank-problems"
"20260207-2325-55-13","Wie behebt der Dämpfungsfaktor \(\beta\) Sackgassen und Spider Traps?","Mit Wahrscheinlichkeit 1-\beta springt der Surfer zufällig zu einer Seite; dadurch bleiben alle Seiten erreichbar.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank-problems"
"20260207-2325-55-14","Warum führt die einfache PageRank-Gleichung auf ein lineares Gleichungssystem?","Es entstehen n Gleichungen mit n Unbekannten (PageRank je Knoten), beschrieben durch r = M r.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank"
"20260207-2325-55-15","Was ist die Idee der Blockmatrix-Repräsentation bei der PageRank-Berechnung?","Die spärliche Matrix M wird in Blöcke partitioniert, damit Matrix-Vektor-Produkte verteilt (z.B. MapReduce) berechnet werden können.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 pagerank blockmatrix"
"20260207-2325-55-16","Was ist the menspezifischer PageRank?","PageRank mit Teleportation nur zu Seiten einer Themenmenge S, um Relevanz nach Thema statt nur Popularität zu bewerten.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 topic-specific-pagerank"
"20260207-2325-55-17","Wie lautet die Formel für themenspezifischen PageRank für i in S?","Für i∈S: r_i^{(S)} = \beta \sum_{j\to i} r_j/d_j^+ + (1-\beta)/|S|.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 topic-specific-pagerank"
"20260207-2325-55-18","Wie lautet die Formel für themenspezifischen PageRank für i nicht in S?","Für i∉S: r_i^{(S)} = \beta \sum_{j\to i} r_j/d_j^+.","Kapitel 9 - Folien [updated 22.01.txt","week-9 ws-25-26 centrality pagerank graph-data data-mining source::kapitel-9---folien-[updated-22.01 topic-specific-pagerank"
"20260207-2329-04-1","Welches Problem löst die DGIM-Methode in Data Streams?","Sie schätzt die Anzahl der Einsen unter den letzten k Bits eines 0/1-Datenstroms (Sliding Window) mit wenig Speicher.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 dgim sliding-window"
"20260207-2329-04-2","Welche Informationen speichert ein DGIM-Bereich (Bucket)?","A: Zeitpunkt des neuesten 1-Bits im Bereich. B: Anzahl der Einsen im Bereich als Zweierpotenz (Größe).","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 dgim buckets"
"20260207-2329-04-3","Welche Invarianten gelten für DGIM-Bereiche?","Jedes 1-Bit liegt in genau einem Bereich; das rechte Ende eines Bereichs enthält eine 1; Bereichsgrößen sind Zweierpotenzen; von neu nach alt sind die Größen monoton nicht fallend; es gibt höchstens zwei Bereiche pro Größe; Bereiche außerhalb des Sliding Windows verschwinden.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 dgim buckets"
"20260207-2329-04-4","Wie werden DGIM-Bereiche aktualisiert, wenn ein neues Bit = 1 ankommt?","Erstelle einen neuen Bereich der Größe 1 mit aktuellem Zeitpunkt. Falls nun drei Bereiche derselben Größe existieren, kombiniere die zwei ältesten zu einem Bereich der doppelten Größe; wiederhole dies ggf. für größere Größen.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 dgim update"
"20260207-2329-04-5","Wie schätzt DGIM die Anzahl der Einsen unter den letzten k Bits?","Finde den ältesten Bereich B, dessen Zeitstempel noch in den letzten k Bits liegt. Schätzer = Summe der Größen aller neueren Bereiche + Hälfte der Größe von B.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 dgim query"
"20260207-2329-04-6","Wie groß ist der maximale relative Fehler der DGIM-Schätzung?","Maximal 50 % (es wird höchstens die Hälfte zu viel oder zu wenig gezählt).","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 dgim error"
"20260207-2329-04-7","Wofür wird ein Bloom-Filter in Data Streams verwendet?","Für speichereffizientes Set-Membership-Testing: Keine False Negatives, aber False Positives möglich.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 bloom-filter filter"
"20260207-2329-04-8","Wie funktioniert Einfügen und Abfragen in einem Bloom-Filter mit k Hash-Funktionen?","Einfügen: Für jedes Element s setze B[h_i(s)] = 1 für alle i=1..k. Abfrage: Falls alle B[h_i(a)] = 1, dann ""möglicherweise in S""; sonst sicher nicht in S.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 bloom-filter"
"20260207-2329-04-9","Wie lautet die approximative False-Positive-Wahrscheinlichkeit eines Bloom-Filters?","p_fp ≈ (1 − e^{−km/n})^k, wobei m die Anzahl der Elemente und n die Bit-Array-Größe ist.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 bloom-filter probability"
"20260207-2329-04-10","Wie wählt man die optimale Anzahl k der Hash-Funktionen im Bloom-Filter?","k_opt = (n/m) · ln 2.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 bloom-filter optimization"
"20260207-2329-04-11","Wie definiert Flajolet-Martin r(e) und R, und wie wird die Kardinalität geschätzt?","r(e) ist die Anzahl der hinteren Nullen von h(e). R ist das Maximum von r(e) über alle beobachteten Elemente. Schätzer: 2^R.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 flajolet-martin cardinality"
"20260207-2329-04-12","Warum tauchen bei Flajolet-Martin große r-Werte nur bei vielen verschiedenen Elementen auf?","Ein Hash-Wert hat mit Wahrscheinlichkeit 2^{−r} genau r hintere Nullen. Viele verschiedene Elemente erhöhen die Chance, mindestens einen Hash mit großem r zu sehen.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 flajolet-martin intuition"
"20260207-2329-04-13","Wie reduziert Flajolet-Martin die Überschätzung durch Extremwerte?","Durch mehrere Hash-Funktionen und robuste Aggregation: z.B. Median (oder Mittelwert der Mediane aus Gruppen).","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 flajolet-martin variance"
"20260207-2329-04-14","Welches Ziel hat der Alon-Matias-Szegedy (AMS) Algorithmus?","Er schätzt Momente der Häufigkeitsverteilung, insbesondere das 2. Moment, in einem Datenstrom mit begrenztem Speicher.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 alon-matias-szegedy moments"
"20260207-2329-04-15","Was ist die Grundidee des AMS-Algorithmus für das 2. Moment?","Wähle zufällige Zeitpunkte t. Für das Element i am Zeitpunkt t zähle c_t, wie oft i ab t noch vorkommt. Verwende diese Werte zur Schätzung des Moments und mittlere über mehrere Variablen.","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 alon-matias-szegedy idea"
"20260207-2329-04-16","Wie ist der AMS-Schätzer f(X) für das 2. Moment definiert?","Für eine Variable X mit Wert c (Anzahl der Vorkommen ab t) gilt f(X) = n(2c − 1); der Moment-Schätzer ist der Mittelwert der f(X) über viele Variablen (ggf. skaliert mit |A|).","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 alon-matias-szegedy estimator"
"20260207-2329-04-17","Wie lautet der AMS-Schätzer f(X) für das k-te Moment?","Für c = X.val gilt f(X) = n(c^k − (c−1)^k).","Kapitel 10 - Folien [updated 05.02.txt","week-10 ws-25-26 streams data-mining source::kapitel-10---folien-[updated-05.02 alon-matias-szegedy moments"
