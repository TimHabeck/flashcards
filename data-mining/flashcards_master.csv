"id","front","back","source_file","tag"
"20260206-1105-21-1","Was ist Data Mining?","Der Prozess, aus großen Datenmengen interessante Muster und nützliches Wissen zu entdecken.","Kapitel 1 - Folien.txt","week-1 wise-25-26 definition data-mining source::kapitel-1-folien"
"20260206-1105-21-2","Welches Ziel verfolgt Data Mining?","Versteckte Muster und Strukturen in Datenbeständen mit statistischen Methoden aufdecken, um neue Erkenntnisse, Querverbindungen und Trends zu erkennen.","Kapitel 1 - Folien.txt","week-1 wise-25-26 goal data-mining source::kapitel-1-folien"
"20260206-1105-21-3","Nenne typische Anwendungen von Data Mining.","Warenkorbanalysen, Kreditwürdigkeits- und Betrugserkennung in Banken/Versicherungen, personalisierte Medikamente und Nebenwirkungsanalyse in der Pharmaindustrie, Muster in Genaktivitäten/-mutationen (Medizin/Bioinformatik).","Kapitel 1 - Folien.txt","week-1 wise-25-26 applications data-mining source::kapitel-1-folien"
"20260206-1105-21-4","Aus welchen Bereichen speist sich Data Mining?","Datenbanken, Statistik/Datenanalyse und Maschinelles Lernen.","Kapitel 1 - Folien.txt","week-1 wise-25-26 roots data-mining source::kapitel-1-folien"
"20260206-1105-21-5","Welche Herausforderungen sind typisch für Data Mining?","Große Datenmenge, hohe Dimensionalität, Heterogenität, Komplexität, Verteiltheit.","Kapitel 1 - Folien.txt","week-1 wise-25-26 challenges data-mining source::kapitel-1-folien""20260206-1112-05-1","Was ist das Ziel von Clustering?","Datenpunkte so zu gruppieren, dass die paarweisen Distanzen innerhalb eines Clusters klein und zwischen verschiedenen Clustern groß sind.","Kapitel 2 - Folien.txt","week-2 wise-25-26 clustering objective data-mining source::kapitel-2-folien"
"20260206-1112-05-2","Wie ist die euklidische Distanz zwischen zwei Punkten definiert?","Als Wurzel der Summe der quadrierten Koordinatendifferenzen.","Kapitel 2 - Folien.txt","week-2 wise-25-26 distance-metrics clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-3","Wie ist die Manhattan-Distanz zwischen zwei Punkten definiert?","Als Summe der absoluten Koordinatendifferenzen.","Kapitel 2 - Folien.txt","week-2 wise-25-26 distance-metrics clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-4","Wann ist eine direkte Visualisierung von Clustering-Ergebnissen möglich?","Nur bei zwei Dimensionen.","Kapitel 2 - Folien.txt","week-2 wise-25-26 clustering visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-5","Was ist die Interquartilsabstand (IQR) in einem Boxplot?","Die Differenz zwischen dem dritten und dem ersten Quartil: IQR = Q3 - Q1.","Kapitel 2 - Folien.txt","week-2 wise-25-26 boxplot visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-6","Wie werden die Whiskers in einem Boxplot definiert?","Sie reichen von Q1 - 1,5*IQR bis Q3 + 1,5*IQR; der Whisker endet am letzten Datenpunkt innerhalb dieses Bereichs.","Kapitel 2 - Folien.txt","week-2 wise-25-26 boxplot visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-7","Wann gilt ein Datenpunkt im Boxplot als Ausreißer?","Wenn er außerhalb der Whiskers liegt.","Kapitel 2 - Folien.txt","week-2 wise-25-26 boxplot visualization data-mining source::kapitel-2-folien"
"20260206-1112-05-8","Warum ist Clustering für große Datenmengen rechnerisch anspruchsvoll?","Es gibt k^N mögliche Zuordnungen der N Punkte zu k Clustern, und paarweise Ähnlichkeitsberechnungen kosten O(N^2).","Kapitel 2 - Folien.txt","week-2 wise-25-26 clustering complexity data-mining source::kapitel-2-folien"
"20260206-1112-05-9","Was besagt der Fluch der hohen Dimensionen für Distanzen?","In sehr hohen Dimensionen haben fast alle Paare von Datenpunkten eine ähnliche Distanz.","Kapitel 2 - Folien.txt","week-2 wise-25-26 curse-of-dimensionality clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-10","Was ist der Unterschied zwischen agglomerativem und divisivem hierarchischem Clustering?","Agglomerativ (Bottom-up) startet mit Einzelpunkten und fusioniert Cluster; divisiv (Top-down) startet mit einem Cluster und teilt ihn schrittweise.","Kapitel 2 - Folien.txt","week-2 wise-25-26 hierarchical-clustering clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-11","Was zeigt ein Dendrogramm in der hierarchischen Clusteranalyse?","Die Blätter sind einzelne Datenpunkte; die Höhe einer Vereinigung entspricht der Distanz zwischen den Clustern.","Kapitel 2 - Folien.txt","week-2 wise-25-26 hierarchical-clustering dendrogram data-mining source::kapitel-2-folien"
"20260206-1112-05-12","Welche zwei grundlegenden Entscheidungen sind für hierarchische Clusteranalyse nötig?","Definition der Distanz zwischen Clustern und eine Stoppregel.","Kapitel 2 - Folien.txt","week-2 wise-25-26 hierarchical-clustering clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-13","Wie definieren die gängigen Linkage-Methoden die Distanz zwischen zwei Clustern?","Single: minimale Paar-Distanz; Complete: maximale Paar-Distanz; Average: durchschnittliche Paar-Distanz; Centroid: Distanz der Centroiden; Ward: Zunahme der Varianz beim Zusammenführen.","Kapitel 2 - Folien.txt","week-2 wise-25-26 linkage hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-14","Welche Linkage-Methoden sind direkt in nicht-euklidischen Räumen anwendbar?","Single, Complete und Average Linkage.","Kapitel 2 - Folien.txt","week-2 wise-25-26 linkage hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-15","Warum sind Centroid- und Ward-Linkage nicht immer direkt anwendbar?","Sie erfordern die Berechnung eines Centroiden, was z. B. bei String-Daten nicht direkt möglich ist.","Kapitel 2 - Folien.txt","week-2 wise-25-26 linkage hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-16","Was sind typische Stoppregeln bei agglomerativem Clustering?","Vorgegebene Anzahl von Clustern; maximale Distanz im neu entstandenen Cluster übersteigt Schwellenwert; durchschnittliche maximale Distanz steigt stark an; Cluster-Dichte fällt unter einen Schwellenwert.","Kapitel 2 - Folien.txt","week-2 wise-25-26 hierarchical-clustering stopping-criteria data-mining source::kapitel-2-folien"
"20260206-1112-05-17","Warum sollten Attribute vor dem Clustering standardisiert werden?","Damit unterschiedliche Skalen (z. B. Einkommen vs. Körpergröße) die Distanzberechnung nicht dominieren; Standardisierung auf Mittelwert 0 und Standardabweichung 1.","Kapitel 2 - Folien.txt","week-2 wise-25-26 preprocessing clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-18","Was ist ein Clustroid in nicht-euklidischen Räumen?","Ein Punkt aus dem Cluster mit minimaler Summe aller Distanzen zu den anderen Punkten oder minimaler maximaler Distanz zu den anderen Punkten.","Kapitel 2 - Folien.txt","week-2 wise-25-26 clustering non-euclidean data-mining source::kapitel-2-folien"
"20260206-1112-05-19","Wie ist die Zeitkomplexität von hierarchischem Clustering?","Single Linkage: O(N^2); andere Verfahren mit Priority Queue: O(N^2 log N); naiv können sie O(N^3) erreichen.","Kapitel 2 - Folien.txt","week-2 wise-25-26 complexity hierarchical-clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-20","Was ist die Zielsetzung von k-Means-Clustering?","Eine Partition in k Cluster mit möglichst kleiner durchschnittlicher Distanz innerhalb der Cluster.","Kapitel 2 - Folien.txt","week-2 wise-25-26 k-means clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-21","Wie läuft der k-Means-Algorithmus ab?","Initiale k Centroiden wählen, Punkte dem nächstgelegenen Centroid zuordnen, Centroiden neu berechnen, wiederholen bis Konvergenz.","Kapitel 2 - Folien.txt","week-2 wise-25-26 k-means algorithm data-mining source::kapitel-2-folien"
"20260206-1112-05-22","Welche Initialisierungsstrategien für k-Means sind üblich?","Zufällige Zuordnung und Centroid-Berechnung; Auswahl von k Punkten mit maximalen paarweisen Distanzen; hierarchisches Clustering auf Stichprobe und Wahl der Clustroiden.","Kapitel 2 - Folien.txt","week-2 wise-25-26 k-means initialization data-mining source::kapitel-2-folien"
"20260206-1112-05-23","Warum wird k-Means oft mit mehreren Initialisierungen ausgeführt?","Der Algorithmus konvergiert zwar, kann aber in einem lokalen Minimum landen; daher wählt man das beste Ergebnis aus mehreren Starts.","Kapitel 2 - Folien.txt","week-2 wise-25-26 k-means optimization data-mining source::kapitel-2-folien"
"20260206-1112-05-24","Wie kann man die Anzahl der Cluster k heuristisch wählen?","k schrittweise erhöhen (z. B. 2, 4, 8, 16, …) und dort verfeinern, wo sich relevante Änderungen zeigen; binäre Suche zur Reduktion des Aufwands.","Kapitel 2 - Folien.txt","week-2 wise-25-26 k-means model-selection data-mining source::kapitel-2-folien"
"20260206-1112-05-25","Was ist der BFR-Algorithmus?","Eine k-Means-Variante für sehr große Datensätze, die nicht in den Hauptspeicher passen; Cluster werden als multivariat normalverteilt mit unabhängigen Dimensionen angenommen.","Kapitel 2 - Folien.txt","week-2 wise-25-26 bfr clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-26","Wie werden Cluster im BFR-Algorithmus effizient repräsentiert?","Mit 2d + 1 Zahlen: n, SUM_1..SUM_d und SUMSQ_1..SUMSQ_d.","Kapitel 2 - Folien.txt","week-2 wise-25-26 bfr clustering statistics data-mining source::kapitel-2-folien"
"20260206-1112-05-27","Wie berechnet man Centroid und Varianz aus den BFR-Statistiken?","Centroid = SUM / n; Varianz = SUMSQ / n - (SUM / n)^2 (komponentenweise).","Kapitel 2 - Folien.txt","week-2 wise-25-26 bfr clustering statistics data-mining source::kapitel-2-folien"
"20260206-1112-05-28","Welche drei Mengen verwendet der BFR-Algorithmus?","Discard Set (DS): zugeordnete Punkte; Retained Set (RS): noch nicht zugeordnete Punkte; Compression Set (CS): zusammengefasste Mini-Cluster aus RS.","Kapitel 2 - Folien.txt","week-2 wise-25-26 bfr clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-29","Wie läuft der BFR-Algorithmus grob ab?","Initialisiere k Cluster; lade Daten in Chunks; ordne Punkte den DS-Clustern zu, wenn Distanz unter Schwellenwert; clustere verbleibende Punkte und bilde CS; merge CS ggf. mit DS; am Ende ordne CS und RS den nächstliegenden DS-Clustern zu.","Kapitel 2 - Folien.txt","week-2 wise-25-26 bfr algorithm data-mining source::kapitel-2-folien"
"20260206-1112-05-30","Nach welchem Kriterium wird ein Punkt im BFR-Algorithmus einem Cluster zugeordnet?","Minimaler Mahalanobis-Abstand zum Centroid und unter einem Schwellenwert.","Kapitel 2 - Folien.txt","week-2 wise-25-26 bfr clustering distance-metrics data-mining source::kapitel-2-folien"
"20260206-1112-05-31","Wie ist der Mahalanobis-Abstand im BFR-Kontext definiert und welche Schwellenwerte sind typisch?","M(x, c) = Σ_i ((x_i - c_i)/σ_i)^2; ca. 68% der Punkte haben M < d, ca. 95% haben M < 2d.","Kapitel 2 - Folien.txt","week-2 wise-25-26 bfr mahalanobis distance-metrics data-mining source::kapitel-2-folien"
"20260206-1112-05-32","Welche Einschränkungen von BFR adressiert der CURE-Algorithmus?","BFR setzt normalverteilte Cluster mit unabhängigen Dimensionen voraus; CURE erlaubt Cluster beliebiger Form.","Kapitel 2 - Folien.txt","week-2 wise-25-26 cure bfr clustering data-mining source::kapitel-2-folien"
"20260206-1112-05-33","Wie funktioniert der CURE-Algorithmus in Grundzügen?","Zufallsstichprobe ziehen, hierarchisch clustern (ohne Centroid bevorzugt), repräsentative Punkte pro Cluster wählen (weit auseinander), diese Richtung Centroid um einen Anteil verschieben, Cluster anhand geringer max. Distanz der Repräsentanten zusammenführen, alle Punkte dem nächsten Repräsentanten zuordnen.","Kapitel 2 - Folien.txt","week-2 wise-25-26 cure clustering algorithm data-mining source::kapitel-2-folien"
"20260206-1122-46-1","Was sind zentrale Ziele der Dimensionsreduktion?","Aufdeckung versteckter Korrelationen Entfernung redundanter und verrauschter Merkmale Leichtere Interpretation und Visualisierung Schnellere Speicherung und Verarbeitung ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 dimension-reduction goals data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-2","Was ist das Grundprinzip der Dimensionsreduktion über Projektion?","Liegen Datenpunkte nahe einem d-dimensionalen Unterraum, werden sie durch ihre Projektionen auf diesen Unterraum repräsentiert.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 dimension-reduction principle data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-3","Wie werden die Achsen des Unterraums bei der varianz-basierten Dimensionsreduktion gewählt?","1. Faktor: Richtung mit der größten Streuung (maximale Varianz). Weitere Faktoren: jeweils orthogonal zu den vorherigen und mit maximaler verbleibender Varianz. ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 dimension-reduction factors data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-4","Was ist die Hauptkomponentenanalyse (PCA)?","Ein Verfahren der multivariaten Statistik und Linearen Algebra, das Daten durch eine lineare Transformation auf Hauptkomponenten projiziert und so eine beste lineare Approximation liefert.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca definition linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-5","Welche drei Hauptschritte umfasst die PCA?","Translation (Zentrierung): Mittelwert der Spalten auf 0 setzen. Rotation: Kovarianzmatrix bilden und Eigenvektoren bestimmen. Projektion: Daten auf die (wichtigsten) Hauptachsen projizieren. ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca procedure linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-6","Welche Rolle spielen Eigenvektoren und Eigenwerte in der PCA?","Eigenvektoren (Ladungsvektoren) definieren die Hauptachsen und sind orthonormal. Eigenwerte geben die Varianz entlang der jeweiligen Hauptachse an. Die Komponenten werden nach absteigenden Eigenwerten sortiert. ","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca eigenvectors linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-7","Wie wählt man in der PCA die Anzahl der Hauptkomponenten?","Wähle so viele Komponenten, dass der kumulierte erklärte Varianzanteil hoch ist und eine weitere Komponente den Anteil nur noch gering erhöht.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 pca model-selection statistics data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-8","Was ist die Singulärwertzerlegung (SVD)?","Eine Zerlegung einer Matrix in U, Σ und Vᵀ: U und V haben orthonormale Spalten, Σ ist diagonal mit nicht-negativen Singulärwerten, r ist der Rang der Matrix.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd definition linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-9","Inwiefern ist SVD eine Verallgemeinerung der PCA?","SVD verallgemeinert PCA auf allgemeine (auch rechteckige) Matrizen; PCA lässt sich als spezielle Anwendung der SVD verstehen.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd pca relationship linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-10","Wie nutzt man SVD zur Dimensionsreduktion?","Man behält die größten Singulärwerte und setzt kleine Singulärwerte auf 0; so erhält man eine Rang-k-Approximation der Daten.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd dimension-reduction linear-algebra data-mining source::kapitel-3---folien-(neu)"
"20260206-1122-46-11","Welches Kriterium wird für die Wahl von k bei SVD genannt?","Behalte typischerweise 80–90&nbsp;% der Energie (Summe der quadrierten Singulärwerte), um einen guten Kompromiss zwischen Kompression und Genauigkeit zu erhalten.","Kapitel 3 - Folien (neu).txt","week-3 ws-25-26 svd model-selection linear-algebra data-mining source::kapitel-3---folien-(neu)"
