DATA MINING

Kapitel 7: Supervised Machine Learning
Dr. Christian Martin
Wintersemester 2025/26

Abteilung Datenbanken / ScaDS.AI
UniversitÃ¤t Leipzig

THEMENÃœBERSICHT
Hochdimensionale Daten

Graphdaten

DatenstrÃ¶me

Clustering

Dimensionsreduktion

Community
Detection

Windowing

Empfehlungssysteme

Assoziationsregeln

PageRank

Filtern

Locality Sensitive
Hashing

Supervised ML

Web Spam

Momente

7-2

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ EntscheidungsbÃ¤ume / Random Forests
âˆ’ Support Vector Machines
âˆ’ Auswertung binÃ¤re Klassifikation

7-3

SUPERVISED LEARNING
âˆ’ Daten zu einem Paar (ğ’™, ğ‘¦)
âˆ’ ğ’™ ist ein Vektor aus Merkmalen (Features):
âˆ’ verschiedene Datentypen mÃ¶glich
âˆ’ Beispiel: (Age, Male, Class)

Age

Male

Class Survived

Child

True

1

Yes

Adult

True

2

No

Adult

False

1

Yes

Child

True

3

No

â€¦

â€¦

â€¦

â€¦

âˆ’ ğ‘¦ ist eine Bezeichnung (Label),
âˆ’ eine reelle Zahl (â†’ Regression)
âˆ’ oder eine Kategorie/Klasse (â†’ Klassifikation)

X

Y

Xâ€™

Yâ€™

âˆ’ Beispiel: Survived

âˆ’ Ziel: Auffinden einer Funktion (Modell) ğ‘“ mit
ğ‘¦ = ğ‘“(ğ’™)
âˆ’ Vielzahl an Funktionen mÃ¶glich
âˆ’ Bewertung und Auswahl der Funktion Ã¼ber Daten

Trainingsdaten
Testdaten

âˆ’ Aufteilung in Trainings- und Testdaten (z.B. 80% vs. 20%)
âˆ’ SchÃ¤tzen der Funktion Ã¼ber Trainingsdaten: Fehler mÃ¶glichst gering
âˆ’ Bewerten der Funktion an Testdaten: Fehler mÃ¶glichst gering
7-4

SUPERVISED LEARNING
âˆ’ Cross-Validation
âˆ’ Aufteilung der Trainingsdaten in Trainings- und Validierungsdaten
âˆ’ Ziel: finde den besten ML-Algorithmus inkl. Hyperparametern
(Hyperparameter-Tuning)
âˆ’ Die finale Evaluation mit dem besten ML-Algorithmus findet (einmalig!)
auf den Testdaten statt
âˆ’ Die Testdaten dÃ¼rfen nicht zum HyperparameterXâ€™â€™
Yâ€™â€™
Tuning verwendet werden (data leakage)

âˆ’ Beispiel: 5-fold cross-validation:

Xâ€™â€™â€™
Yâ€™â€™â€™
âˆ’ Teile die Trainingsdaten in 5 gleich groÃŸe BlÃ¶cke
âˆ’ 5 DurchlÃ¤ufe (x = 1 to 5)
Xâ€™
Yâ€™
âˆ’ Training auf allen BlÃ¶cken auÃŸer Block x (Xâ€˜â€˜, Yâ€˜â€˜)
âˆ’ Validierung auf Block x (Xâ€˜â€˜â€˜, Yâ€˜â€˜â€˜)
Trainingsdaten
âˆ’ Durchschnitt ergibt Trainingsfehler
Validierungsdaten
âˆ’ Mehrere DurchlÃ¤ufe um Hyperparameter zu
Testdaten
optimieren
7-5

SUPERVISED LEARNING
âˆ’ Leave-one-out cross-validation:
âˆ’ Trainiere auf allen Datenpunkten bis auf einem (Xâ€˜â€˜, Yâ€˜â€˜)
âˆ’ Validiere auf diesem einen Datenpunkt (Xâ€˜â€˜â€˜, Yâ€˜â€˜â€˜)
âˆ’ N (Anzahl Datenpunkte) DurchlÃ¤ufe
âˆ’ nur bei DatensÃ¤tzen mit wenigen Datenpunkten
sinnvoll

âˆ’ Nested Cross-Validation

Xâ€™â€™

Yâ€™â€™

Xâ€™â€™â€™

Yâ€™â€™â€™

Xâ€™

Yâ€™

âˆ’ Cross-Validierung innerhalb einer Cross-Validierung
Trainingsdaten
âˆ’ Wie Cross-Validierung, nur dass jeder Teil der
Validierungsdaten
Daten einem zum Testdatensatz wird
Testdaten
âˆ’ Robusteres Ergebnis als einfache Cross-Validation,
aber auch zeitaufwendiger,
auch Berechnung von Standardabweichung mÃ¶glich
7-6

SUPERVISED LEARNING - METHODEN
BewÃ¤hrte Methoden (Ausschnitt)
Lineare Regression

Logistische Regression
Linear Discriminant Analysis
K-Nearest Neighbors
NaÃ¯ve Bayes
Random Forests
Support Vector Machines
Neuronale Netze

â€¢ Generelles Problem bei
hochdimensionalen Daten:
Overfitting (Funktion passt
sich den zufÃ¤lligen Fehlern an)

â€¢ LÃ¶sung z.B.
â€“ Dimensionsreduktion Ã¼ber
PCA/SVD oder Clustering
â€“ Regularisierung (Ridge,
Lasso Regression)
â€“ Informationskriterien (z.B.
Akaike Information Criterion
(AIC), BIC, DIC, WAIC, â€¦)
um QualitÃ¤t eines Modells zu
schÃ¤tzen
7-7

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ EntscheidungsbÃ¤ume / Random Forests
âˆ’ Support Vector Machines
âˆ’ Auswertung binÃ¤re Klassifikation

7-8

ENTSCHEIDUNGSBÃ„UME

ğ’™ğŸ < ğ’—
A

âˆ’ Ziel-Variable ğ‘Œ: numerisch
(Regression) oder kategorial
(Klassifikation)
âˆ’ Vorhersage von Y durch X Ã¼ber eine
Baumstruktur
âˆ’ Folge den Ã„sten des Baums
entsprechend den Werten von X
âˆ’ Vorhersagewert fÃ¼r Y
an den Blattknoten

âˆ’ i.d.R. binÃ¤re BÃ¤ume

C

Y=4

ğ’™ğŸ ïƒ{ğ’‚, ğ’ƒ, ğ’…}

ğ’™ğŸ‘ < ğ’˜

D

E
ğ’™ğŸ’ > ğ’–

F

Y=1

H

I

âˆ’ HÃ¶here Unterscheidungskraft
âˆ’ Jeder nicht-binÃ¤re Baum Ã¼berfÃ¼hrbar

7-9

ERSTELLEN EINES ENTSCHEIDUNGSBAUMS
(REGRESSION)
Ziel: Aufteilung der Variaben aus X in Regionen ğ‘…1 , ğ‘…2 , â€¦ , ğ‘…ğ½ , so dass
ğ½

à· à· ğ‘¦ğ‘– âˆ’ ğ‘¦à·
ğ‘…ğ‘—

2

ğ‘—=1 ğ‘–âˆˆğ‘…ğ‘—

minimiert wird (ğ‘¦à·
ğ‘…ğ‘— bezeichnet den Vorhersagewert der Region ğ‘…ğ‘— ).

âˆ’ Vorhersagewert ist der Mittelwert der Datenpunkte der Region
A

4,5
4

Berufserfahrung

3,5

ğ‘…2

ğ‘…1

3

ğ€ğ¥ğ­ğğ« < ğŸğŸ

2,5

B

ğ‘Œğ‘…1 = 4

2
1,5

ğğğ«ğ®ğŸğ¬ âˆ’
ğğ«ğŸğšğ¡ğ«ğ®ğ§ğ  < ğŸ, ğŸ’

ğ‘…3

1
0,5
0

16

21

26

Alter

31

ğ‘Œğ‘…3 = 1

ğ‘Œğ‘…2 = 2
7-10

ERSTELLEN EINES ENTSCHEIDUNGSBAUMS
(REGRESSION)
Ziel: Aufteilung der Variaben aus X in Regionen ğ‘…1 , ğ‘…2 , â€¦ , ğ‘…ğ½ , so dass
ğ½

à· à· ğ‘¦ğ‘– âˆ’ ğ‘¦à·
ğ‘…ğ‘—

2

ğ‘—=1 ğ‘–âˆˆğ‘…ğ‘—

minimiert wird (ğ‘¦à·
ğ‘…ğ‘— bezeichnet den Vorhersagewert der Region ğ‘…ğ‘— ).

âˆ’ Vorhersagewert ist der Mittelwert der Datenpunkte der Region
âˆ’ Problem: zu viele mÃ¶gliche Aufteilungen in ğ½ Regionen

7-11

ERSTELLEN EINES ENTSCHEIDUNGSBAUMS
(REGRESSION)
âˆ’ Problem: zu viele mÃ¶gliche Aufteilungen in ğ½ Regionen
âˆ’ Ausweg: Recursive Binary Splitting
1. Auswahl
âˆ’ einer Variable xj und
âˆ’ eines Schwellenwerts s,

so dass die beiden Regionen
R1 j, s = {x|xj < s} und
R 2 j, s = {x|xj â‰¥ s}
die folgende Summe minimieren:

2. Ïƒi:xiâˆˆR1 j,s yi âˆ’ yà·Ÿ
R1 (j,s)

2

+ Ïƒi:xi âˆˆR2 j,s yi âˆ’ yà·Ÿ
R2 (j,s)

2

3. Teile resultierende Regionen und gehe zu 1.

âˆ’ Stopp, falls z.B. in jede Region weniger als 5 Datenpunkte fallen
âˆ’ Jede Variable kann auch mehrfach ausgewÃ¤hlt werden
7-12

ENTSCHEIDUNGSBAUM FÃœR KLASSIFIKATION
âˆ’ Vorhersagewert ist der hÃ¤ufigste Wert der Datenpunkte einer Region
âˆ’ FehlermaÃŸ: z.B. Entropie
ğ‘¯ ğ‘Œ =âˆ’

à·

ğ‘ğ‘¦ log 2 ğ‘ğ‘¦

ğ‘¦âˆˆDomÃ¤ne von ğ‘Œ

âˆ’ Der Wert ğ‘ğ‘¦ gibt die Wahrscheinlichkeit der AusprÃ¤gung ğ‘Œ = ğ‘¦ an
âˆ’ Hohe Entropie: gleichverteiltes Y
âˆ’ Niedrige Entropie: ungleiche Verteilung

Niedrige Entropie

Hohe Entropie

7-13

X

ENTROPIE: BEISPIEL
âˆ’ ğ‘¿: Studium
âˆ’ ğ’€: mag den Film â€œCasablancaâ€
âˆ’ SchÃ¤tzen der Wahrscheinlichkeiten
Ã¼ber relative HÃ¤ufigkeiten
âˆ’

âˆ’

3
ğ‘ƒ ğ‘Œ = ğ½ğ‘ = 8 , ğ‘ƒ ğ‘Œ = ğ‘ğ‘’ğ‘–ğ‘›
1
ğ‘ƒ ğ‘Œ = ğ½ğ‘|ğ‘‹ = ğ‘€ğ‘ğ‘¡â„ğ‘’ =
4

5
=8

Y

Mathe

Ja

Geschichte

Nein

Informatik

Ja

Mathe

Nein

Mathe

Nein

Informatik

Ja

Mathe

Nein

Geschichte

Nein

3

âˆ’ ğ‘ƒ ğ‘Œ = ğ‘ğ‘’ğ‘–ğ‘›|ğ‘‹ = ğ‘€ğ‘ğ‘¡â„ğ‘’ = 4

âˆ’ Entropie:
3
3 5
5
ğ» ğ‘Œ = âˆ’ log 2 âˆ’ log 2 â‰ˆ 0.95
8
8 8
8
âˆ’ Spezifische bedingte Entropie:
1
1 3
3
ğ» ğ‘Œ ğ‘‹ = ğ‘€ğ‘ğ‘¡â„ğ‘’ = âˆ’ log âˆ’ log â‰ˆ 0.81
4
4 4
4
7-14

ENTROPIE: BEISPIEL

X

âˆ’ Bedingte Entropie:
ğ» ğ‘Œ ğ‘‹ = à· ğ‘ƒ ğ‘‹ = ğ‘¥ ğ»(ğ‘Œ|ğ‘‹ = ğ‘¥)
ğ‘¥âˆˆğ‘‚ğ‘‹

ğ‘¥
Mathe
Geschichte
Informatik

ğ‘ƒ(ğ‘‹ = ğ’™)
1
2
1
4
1
4

ğ»(ğ‘Œ|ğ‘‹ = ğ’™)
0.81
0

Y

Mathe

Ja

Geschichte

Nein

Informatik

Ja

Mathe

Nein

Mathe

Nein

Informatik

Ja

Mathe

Nein

Geschichte

Nein

0

1
1
1
ğ» ğ‘Œ ğ‘‹ = âˆ™ 0.81 + âˆ™ 0 + âˆ™ 0 = 0.4
2
4
4
âˆ’ Ziel: Auswahl des Attributs X mit niedrigstem ğ»(ğ‘Œ|ğ‘‹)

7-15

BESTE AUFTEILUNG EINES KNOTENS

A
ğ’™ğŸ < ğ’—

âˆ’ AnschlieÃŸend
âˆ’ X ist numerisch: Auswahl des Schwellenwertes
âˆ’ X ist kategorisch: Auswahl von Kategorien

C

Y=4

âˆ’ Beispiel: (hier Auswahl der besten Kategorie)
âˆ’ Aufteilung: ğ‘‹ = ğ‘€ğ‘ğ‘¡â„ğ‘’ vs. ğ‘‹ â‰  ğ‘€ğ‘ğ‘¡â„ğ‘’

ğ’™ğŸ ïƒ{ğ‘´ğ’‚ğ’•ğ’‰ğ’†}
D

F

âˆ’ ğ»(ğ‘Œ|ğ‘‹ = ğ‘€ğ‘ğ‘¡â„ğ‘’) = 0.81 und ğ»(ğ‘Œ|ğ‘‹ â‰  ğ‘€ğ‘ğ‘¡â„ğ‘’) = 1
âˆ’ Spezifische bedingte Entropie gewichtet nach der Anzahl der EintrÃ¤ge pro Kindsknoten:
1

1

2

2

âˆ’ ğ» ğ‘Œ ğ‘‹ = ğ»(ğ‘Œâ”‚ğ‘‹ = ğ‘€ğ‘ğ‘¡â„ğ‘’) +

ğ»(ğ‘Œ|ğ‘‹ â‰  ğ‘€ğ‘ğ‘¡â„ğ‘’) = 0.9

âˆ’ Aufteilung: ğ‘¿ = ğ‘°ğ’ğ’‡ğ’ğ’“ğ’ğ’‚ğ’•ğ’Šğ’Œ vs. ğ‘¿ â‰  ğ‘°ğ’ğ’‡ğ’ğ’“ğ’ğ’‚ğ’•ğ’Šğ’Œ
âˆ’ ğ» ğ‘Œ|ğ‘‹ = ğ¼ğ‘›ğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘–ğ‘˜ = 0 und ğ» ğ‘Œ|ğ‘‹ â‰  ğ¼ğ‘›ğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘–ğ‘˜ = 0.65
1

3

4

4

âˆ’ ğ» ğ‘Œ ğ‘‹ = ğ» ğ‘Œ ğ‘‹ = ğ¼ğ‘›ğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘–ğ‘˜ + ğ» ğ‘Œ|ğ‘‹ â‰  ğ¼ğ‘›ğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘¡ğ‘–ğ‘˜ = ğŸ. ğŸ’ğŸ–

âˆ’ Aufteilung: ğ‘‹ = ğºğ‘’ğ‘ ğ‘â„ğ‘–ğ‘â„ğ‘¡ğ‘’ vs. ğ‘‹ â‰  ğºğ‘’ğ‘ ğ‘â„ğ‘–ğ‘â„ğ‘¡ğ‘’
âˆ’ ğ» ğ‘Œ|ğ‘‹ = ğºğ‘’ğ‘ ğ‘â„ğ‘–ğ‘â„ğ‘¡ğ‘’ = 0 und ğ» ğ‘Œ|ğ‘‹ â‰  ğºğ‘’ğ‘ ğ‘â„ğ‘–ğ‘â„ğ‘¡ğ‘’ = 1
1

3

4

4

âˆ’ ğ» ğ‘Œ ğ‘‹ = ğ» ğ‘Œ ğ‘‹ = ğºğ‘’ğ‘ ğ‘â„ğ‘–ğ‘â„ğ‘¡ğ‘’ + ğ» ğ‘Œ|ğ‘‹ â‰  ğºğ‘’ğ‘ ğ‘â„ğ‘–ğ‘â„ğ‘¡ğ‘’ = 0.75

7-16

ENTSCHEIDUNGSBÃ„UME - KRITIK
âˆ’ Leicht zu verstehen, implementieren und interpretieren
âˆ’ Parallelisierbar:
âˆ’ B. Panda, J. S. Herbach, S. Basu, and R. J. Bayardo. PLANET:
Massively parallel learning of tree ensembles with MapReduce. In
Proc. VLDB 2009.
âˆ’ J. Ye, J.-H. Chow, J. Chen, Z. Zheng. Stochastic Gradient Boosted
Distributed Decision Trees. In Proc. CIKM 2009.
âˆ’ Sowohl fÃ¼r kategoriale als auch metrische Ergebnisvariable Y geeignet
âˆ’ Problem: Overfitting (Ãœberanpassung des Modells an die Daten)
âˆ’ Overfitting bei zu vielen Ebenen
âˆ’ Doch bei wenigen Ebenen kÃ¶nnen nur wenige Attribute verwendet
werden

7-17

BAGGING UND RANDOM FORESTS
âˆ’ Ausweg: Kombination mehrerer EntscheidungsbÃ¤ume geringer Tiefe
âˆ’ z.B. Ã¼ber Bagging
âˆ’ Ziehen mehrerer Zufallsstichproben aus den Daten (mit ZurÃ¼cklegen)
âˆ’ Ein Entscheidungsbaum geringer Tiefe pro Stichprobe
âˆ’ Mittelwert/hÃ¤ufigster Wert Ã¼ber alle BÃ¤ume ergibt Vorhersage

âˆ’ Random Forest:
ZusÃ¤tzlich zum Bagging
wird beim Lernen der
BÃ¤ume an jedem Knoten
nur eine kleine (zufÃ¤llige)
Auswahl der Attribute
betrachtet

7-18

GINI INDEX, FEATURE IMPORTANCE
âˆ’ Gini Index: Alterative Messung der QualitÃ¤t eines Splits
(an einem Knoten zu einem Parameter)
âˆ’ Datensatz D mit C Klassen
âˆ’ Relative HÃ¤ufigkeit der Klasse c im Datensatz D: ğ‘ğ‘
âˆ’ Gini Index des Datensatzes D:
ğ¶

ğºğ‘–ğ‘›ğ‘– ğ· = 1 âˆ’ à· ğ‘ğ‘2
ğ‘=1

âˆ’ Split S: D wird geteilt in ğ·1 und ğ·2
âˆ’ Gini Index des Splits:
ğ·1
ğ·2
ğºğ‘–ğ‘›ğ‘–ğ‘† ğ· =
ğºğ‘–ğ‘›ğ‘– ğ·1 +
ğºğ‘–ğ‘›ğ‘– ğ·2
ğ·
ğ·
âˆ’ Reduktion der Unreinheit:
âˆ†ğºğ‘–ğ‘›ğ‘– ğ· = ğºğ‘–ğ‘›ğ‘– ğ· âˆ’ ğºğ‘–ğ‘›ğ‘–ğ‘† ğ·
7-19

GINI INDEX, FEATURE IMPORTANCE
âˆ’ Anwendung:
Ãœber Gini
Index kann
die Feature
Importance
fÃ¼r jedes
Feature
berechnen
werden.

Walther et al., Machine Learning for Rupture Risk Prediction of Intracranial Aneurysms, Symmetry 2022, 14(5)

7-20

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ EntscheidungsbÃ¤ume / Random Forests
âˆ’ Support Vector Machines
âˆ’ Auswertung binÃ¤re Klassifikation

7-21

KLASSIFIZIERUNG ÃœBER HYPEREBENE
âˆ’ Numerischer Merkmalsvektor ğ’™ = (ğ‘¥1 , â€¦ ğ‘¥ğ‘› )
âˆ’ BinÃ¤re Variable ğ‘¦ âˆˆ âˆ’1, +1
+1

- - -- -1
- - -7-22

KLASSIFIZIERUNG ÃœBER HYPEREBENE
âˆ’ Numerischer Merkmalsvektor ğ’™ = (ğ‘¥1 , â€¦ ğ‘¥ğ‘› )
âˆ’ BinÃ¤re Variable ğ‘¦ âˆˆ âˆ’1, +1
âˆ’ Eine Hyperebene des â„ğ‘› teilt diesen Raum in 2 Bereiche
âˆ’ Die Gewichte ğ’˜ = ğ‘¤0 , ğ‘¤1 , â€¦ , ğ‘¤ğ‘› âˆˆ â„ğ‘›+1 beschreiben
eine Hyperebene H Ã¼ber
ğ‘›

ğ» = ğ‘¥ âˆˆ â„ğ‘› |ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘– = 0
ğ‘–=1

âˆ’ Klassifizierung Ã¼ber Hyperebene:
ğ‘¦à·œ = +1, falls ğ‘¤0 + Ïƒğ‘– ğ‘¤ğ‘– ğ‘¥ğ‘– > 0
ğ‘¦à·œ = âˆ’1, falls ğ‘¤0 + Ïƒğ‘– ğ‘¤ğ‘– ğ‘¥ğ‘– < 0

7-23

KLASSIFIZIERUNG ÃœBER HYPEREBENE
Ziel: Finden der Parameter ğ’˜, so dass der Raum der
Merkmalsvektoren in zwei Teile aufgespalten wird und Punkte mit dem
gleichen Label auf der gleichen Seite sind
ğ‘›

ğ‘¥+

ğ‘›

ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘– = 0
ğ‘–=1

ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘–+ > 0
ğ‘–=1

ğ‘¥

- - -- - âˆ’ - ğ‘¥ -

ğ‘›

ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘–âˆ’ < 0
ğ‘–=1

7-24

AUSWAHL DER HYPEREBENE

-

-

-

- -

Welche Hyperebene ist die beste?
7-25

MAXIMAL MARGIN CLASSIFIER
Verwendung der Hyperebene mit maximalen Abstand ğœ¸ zu
den Daten

ğœ¸

7-26

MAXIMAL MARGIN CLASSIFIER
Ziel: Suche nach Gewichten ğ’˜ = ğ‘¤0 , ğ‘¤1 , â€¦ , ğ‘¤ğ‘› , so dass 1. der Rand
(Margin) ğœ¸ maximal ist und 2. fÃ¼r alle Daten (ğ’™,ğ‘¦), gilt:
ğ‘›

ğ‘¦ ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘– â‰¥ 1
ğ‘–=1
ğ‘›

ğ‘›

ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘– = 0
ğ‘–=1

ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘– = âˆ’ğŸ
ğ‘–=1

ğ‘›

ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘– = +ğŸ
ğ‘–=1

Sei ğ°

â€²

1
= (ğ‘¤1 , â€¦ , ğ‘¤ğ‘› ). FÃ¼r den Rand gilt: ğ›¾ = â€²
|ğ’˜ |

ğ›¾ ist maximal, wenn Ïƒğ§ğ¢=ğŸ ğ°ğ¢ğŸ minimal ist.

7-27

MAXIMAL MARGIN CLASSIFIER
Sei ğ° â€² = (ğ‘¤1 , â€¦ , ğ‘¤ğ‘› ). FÃ¼r ğ’™+ gilt:

ğ‘¥+

ğ‘›

1 = ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘–+
ğ‘–=1
+ ğ° â€² âˆ™ ğ’™+

= w0
= ğ‘¤0 + ğ° â€² ğ’—â€²
= w0 + ğ° â€² ğ’— + ğ›¾
= w0 + ğ’˜â€² ğ’— + ğ’˜â€² ğ›¾
= w0 + ğ’˜â€² âˆ™ ğ’™ + ğ’˜ â€² ğ›¾
= 0 + ğ’˜â€² ğ›¾

ğ’—â€²

Orthogonale Projektion von ğ‘¥ + auf ğ‘¤ â€²

ğ‘¥

Also: 1 = |ğ’˜â€² |ğ›¾

ğœ¸

Rand ğ›¾ ist maximal, wenn

ğ’˜â€²

=

Ïƒğ§ğ¢=ğŸ ğ°ğ¢ğŸ minimal

ğ’—

ğ’—â€²
7-28

LINEAR NICHT TRENNBARE DATEN
âˆ’ Falls Daten nicht linear trennbar, EinfÃ¼hrung einer
Bestrafung fÃ¼r falsche Zuordnungen:
ğ§

min à· ğ°ğ¢ğŸ + ğ‘ª âˆ™ (# ğŸğšğ¥ğ¬ğœğ¡ğ ğ™ğ®ğ¨ğ«ğğ§ğ®ğ§ğ ğğ§)
ğ’˜

ğ¢=ğŸ

âˆ’ Optimaler Wert fÃ¼r den Parameter C
kann Ã¼ber Testdaten ermittelt werden
âˆ’ ğ‘ª groÃŸ: wichtig ist die Trennung der
Daten (soweit mÃ¶glich)
âˆ’ ğ‘ª klein: wichtig ist ein groÃŸer Rand

ğ‘ª groÃŸ

+
+

+ +
+

-

-

+

-

ğ‘ª klein

-

7-29

+

LINEAR NICHT TRENNBARE DATEN

+ +
âˆ’ Nicht alle falschen Zuordnungen sind +
+
gleich wichtig
+
âˆ’ Bestrafung ğœ‰ğ‘–
+
+
ğ§
min à· ğ°ğ¢ğŸ + ğ‘ª âˆ™ à· ğœ‰ğ‘—
ğ’˜

ğ¢=ğŸ

ğ‘—

âˆ’ Hinge Loss fÃ¼r Datenpunkt (ğ’™ğ‘– , ğ‘¦ğ‘– )

-

-

ï¸j

ï¸i

-

+
- -

ğ‘›

ğœ‰ğ‘— â‰” max 0, 1 âˆ’ ğ‘¦ğ‘— ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘—ğ‘–
ğ‘–=1

âˆ’ ğœ‰ğ‘— = 0 falls ğ‘¥ğ‘— auf â€œrichtiger Seiteâ€ der RÃ¤nder
âˆ’ ğœ‰ğ‘— â‰¤ 1 falls ğ‘¥ğ‘— auf â€œrichtiger Seiteâ€ der Hyperebene

âˆ’ ğœ‰ğ‘— > 1 falls ğ‘¥ğ‘— auf â€œfalschen Seiteâ€ der Hyperebene

7-30

SUPPORT VECTOR CLASSIFIER
âˆ’ Zielfunktion:
ğ§

ğ‘›

min à· ğ°ğ¢ğŸ + ğ‘ª âˆ™ à· max 0, 1 âˆ’ ğ‘¦ğ‘— ğ‘¤0 + à· ğ‘¤ğ‘– ğ‘¥ğ‘—ğ‘–
ğ’˜

ğ¢=ğŸ

ğ‘—

ğ‘–=1

âˆ’ SchÃ¤tzung der Parameter: (Stochastic) Gradient Descent
âˆ’ Als Support-Vektoren bezeichnet man die Punkte des Datensatzes,
welche direkt auf den RÃ¤ndern oder auf der falschen Seite des
zugehÃ¶rigen Randes liegen â†’ nur Support-Vektoren beeinflussen
die Klassifizierung neuer Datenpunkte
âˆ’ Sei ğ‘† die Menge der Support-Vektoren und ğ›¼ğ‘  âˆˆ â„, ğ‘  âˆˆ ğ‘†,
dazugehÃ¶rige Parameter. Der Klassifizierer lÃ¤sst sich in folgender
Form schreiben:
ğ‘“ ğ‘¥ = ğ‘¤0 + à· ğ›¼ğ‘  ğ‘¥ âˆ™ ğ‘¥ğ‘ 
ğ‘ âˆˆğ‘†
7-31

SUPPORT VECTOR MACHINE
âˆ’ Erweiterung des (linearen) Support Vector Classifier Ã¼ber Kernel:

ğ‘“ ğ‘¥ = ğ‘¤0 + à· ğ›¼ğ‘  ğ¾ ğ‘¥, ğ‘¥ğ‘ 
ğ‘ âˆˆğ‘†
Polynomial Kernel
Radial Kernel
ğ¾ ğ‘¥, ğ‘¥ğ‘  = 1 + ğ‘¥ âˆ™ ğ‘¥ğ‘ 

ğ‘‘

ğ‘›

ğ¾ ğ‘¥, ğ‘¥ğ‘  = exp âˆ’ğ‘Ÿ à· ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘ ğ‘– 2
ğ‘–=1

7-32

VERGLEICH
EntscheidungsbÃ¤ume

Support Vector Machines

Regression & Klassifikation
(mehrere (~10) Klassen)

Klassifikation: gewÃ¶hnlich 2
Klassen (erweiterbar auf mehrere
Klassen)
Support Vector Regression

Attribute sind numerisch oder
kategorial

Attribute sind numerisch bzw. binÃ¤r

BeschrÃ¤nkung auf wenige, dicht
besetzte Attribute

Verwendung tausender, spÃ¤rlich
besetzter Attribute

Gut interpretierbar

Schwer interpretierbar

7-33

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ EntscheidungsbÃ¤ume / Random Forests
âˆ’ Support Vector Machines
âˆ’ Auswertung binÃ¤re Klassifikation

7-34

BINÃ„RE KLASSIFIKATION - KONTINGENZTAFEL
âˆ’ Bewertung eines binÃ¤ren Klassifikators
â†’ 2x2 Kontingenztabelle (contingency table / confusion matrix)

Reality
Positive

Negative

Positive

True Positives
(TP)

False Positives
(FP)

Negative

False Negatives
(FN)

True Negatives
(TN)

Classifier

7-35

BINÃ„RE KLASSIFIKATION - BEWERTUNGSMAÃŸE
âˆ’ Abgeleitete BewertungsmaÃŸe

Reality

Positive

Negative

Positive

True Positives (TP)

False Positives (FP)

Negative

False Negatives (FN)

True Negatives (TN)

Classifier

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ + ğ¹ğ‘

ğ¹ğ‘
ğ‘‡ğ‘ƒ + ğ¹ğ‘

ğ¹ğ‘ƒ
ğ¹ğ‘ƒ + ğ‘‡ğ‘

ğ‘‡ğ‘ƒ
[Precision]
ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ [Positive Predictive Value]
ğ¹ğ‘ƒ
[False Discovery Rate]
ğ‘‡ğ‘ƒ + ğ¹ğ‘ƒ
ğ¹ğ‘
[False Omission Rate]
ğ¹ğ‘ + ğ‘‡ğ‘
ğ‘‡ğ‘
[Negative Predictive
ğ¹ğ‘ + ğ‘‡ğ‘ Value]

ğ‘‡ğ‘
ğ¹ğ‘ƒ + ğ‘‡ğ‘

[Recall]
[False
[Specificity]
[Fall-out]
[Sensitivity]
Negative [False Positive [True Negative
[True Positive Rate]
Rate]
Rate]
Rate]

7-36

PRECISION VS. RECALL
âˆ’ Einsatzgebiete: Suche, Information Retrieval

âˆ’ Auswertung der (positiven) Ergebnisliste
Reality
Positive

Negative

Positive

True Positives (TP)

False Positives (FP)

Negative

False Negatives (FN)

True Negatives (TN)

Classifier

Recall =

Precision =

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘

âˆ’ Precision: Wie rein ist die Ergebnisliste?
âˆ’ Recall: Wie komplett ist die Ergebnisliste?
7-37

PRECISION VS. RECALL
âˆ’ Beispiel: Erkenne alle Katzen in einem (Bild-)Datensatz
âˆ’ Datensatz mit 800 Katzen und vielen anderen Bildern
âˆ’ Suche liefert 750 Treffer, von denen aber nur 600 Katzen sind
Reality

Positive

Positive

Negative

TP = 600

FP = 150

Precision =

Classifier
Negative

FN = 200

TN = x

600
600+150

= 80%
(Reinheit)

Recall =

600
= 75%
600+200

(VollstÃ¤ndigkeit)

âˆ’ Die TN werden nicht berÃ¼cksichtigt.
7-38

F1 SCORE
âˆ’ alias: F-score, F-measure

âˆ’ Harmonisches Mittel von Precision und Recall

ğ¹1 =

2
1
1
+
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘‘ğ‘–ğ‘œğ‘›

=2

ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ âˆ— ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›
ğ‘‡ğ‘ƒ
=
1
ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ + ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›
ğ‘‡ğ‘ƒ + 2 (ğ¹ğ‘ƒ + ğ¹ğ‘)

âˆ’ Vorteil: Ein Wert, um die Performanz des Klassifikators zu messen
âˆ’ Nachteile:
âˆ’ Precision (FPs) und Recall (FNs) werden gleich gewichtet
âˆ’ Die TN werden auch beim F1 Score nicht berÃ¼cksichtigt
7-39

SENSITIVITY VS. SPECIFICITY
âˆ’ Einsatzgebiete: z.B. Medizin, Psychologie
âˆ’ Auswertung eines Tests (in positive und negative Richtung)
Reality (Gold Standard)
Positive

Negative

Positive

True Positives (TP)

False Positives (FP)

Negative

False Negatives (FN)

True Negatives (TN)

Classifier

Sensitivity =

ğ‘‡ğ‘ƒ
ğ‘‡ğ‘ƒ+ğ¹ğ‘

SpezifitÃ¤t =

ğ‘‡ğ‘
ğ¹ğ‘ƒ+ğ‘‡ğ‘

âˆ’ Sensitivity: Wieviele der positiven FÃ¤lle (z.B. erkrankte Patienten)
wurden gefunden?
âˆ’ Specificity: Wieviele der negativen FÃ¤lle (z.B. gesunde Patienten)
wurden auch als gesund identifziert?

7-40

SENSITIVITY VS. SPECIFICITY: BEISPIEL
âˆ’ Aufgabe: Erkenne alle Patienten mit Covid-19

Reality (Gold Standard)
Positive

Negative

Positive

TP = 500

FP = 100

Negative

FN = 50

TN = 400

Classifier

Sensitivity =

500
â‰ˆ 91%
500+50

SpezifitÃ¤t =

400
= 80%
400+100

7-41

SENSITIVITY VS. SPECIFICITY: SPECIFICITY
âˆ’ Hohe Sensitivity: Wie viele der erkrankten Personen werden erkannt?
âˆ’ Was ist eine gute Specificity?
âˆ’ HÃ¤ngt vom Einsatzgebiet ab und wie der Test verwendet wird

âˆ’ HÃ¤ngt von der PrÃ¤valenz ab (wie hÃ¤ufig ist eine Erkrankung)
âˆ’ Eine kleine PrÃ¤valenz erfordert eine hohe SpezifitÃ¤t

âˆ’ Beispiel:
âˆ’ Corona-Test mit Sensitivity = 99% und Specificity = 90%

âˆ’ Alle Personen in einer Stadt mit 100.000 Einwohnern werden getestet
âˆ’ PrÃ¤valenz = 0,1% (100 von 100.000 Personen)

âˆ’ Ist dies ein guter Test?

7-42

SENSITIVITY VS. SPECIFICITY: SPECIFICITY
âˆ’ Sensitivity = 99%, Specificity = 90%, Prevalence = 0.1%

Reality (Gold Standard)
Positive

Negative

Positive

TP = 99

FP = 9900

Negative

FN = 1

TN = 89100

99
= 99%
99+1

SpezifitÃ¤t =

Classifier

Sensitivity =

9999
positive
Tests!

89100
= 90%
89100+9900

âˆ’ 9999 Personen werden Covid-19 positiv getestet â†’ QuarantÃ¤ne
7-43

DIAGNOSTIC ODDS RATIO
âˆ’ Alternative Interpretation: Test wird als Risikofaktor interpretiert
âˆ’ Misst die EffektivitÃ¤t eines Tests
âˆ’ ğ·ğ‘‚ğ‘… =

ğ‘‡ğ‘ƒ ğ¹ğ‘
àµ—
ğ¹ğ‘ƒ ğ‘‡ğ‘

âˆ’ Beispiel: Test fÃ¼r Covid-19

Reality (Gold Standard)

Positive

Negative

Positive

TP = 200

FP = 100

Negative

FN = 50

TN = 150

Classifier

200 50
àµ—
=6
100 150

â†’ Wenn der Test positiv ist, dann ist die
Chance/das Risiko Covid-19 zu haben, 6x
hÃ¶her als wenn der Test negativ wÃ¤re.
âˆ’ ğ·ğ‘‚ğ‘… = 1 â†’ unbrauchbarer Test

âˆ’ ğ·ğ‘‚ğ‘… =

âˆ’ Je hÃ¶her ğ·ğ‘‚ğ‘…, desto besser der Test

7-44

RECEIVER OPERATOR CHARACTERICTICS (ROC)
âˆ’ Ursprung
âˆ’ Entwickelt wÃ¤hrend des 2. Weltkriegs zur Analyse von Radarsignalen
âˆ’ Detektion von Flugzeugen anhand von Radarsignalen
âˆ’ Dann: Verwendung in der Psychologie (Erkennung von Reizen) und Medizin
âˆ’ Heute: Standard-Ansatz zur Evaluation von Verfahren des maschinellen
Lernens

âˆ’ Ansatz:
âˆ’ Verwende die Regressions-Version statt der Klassifikations-Version
âˆ’ Setze und variiere Schwellwert, um Klassifikation zu erhalten
âˆ’ Trage BewertungsmaÃŸe gegeneinander auf, typische ROC-Plots:
âˆ’ x-Achse: 1 â€“ Specificity, y-Achse: Sensitivity

âˆ’ x-Achse: Recall, y-Achse: Precision
âˆ’ Berechne: Area under the curve (AUC)
7-45

RECEIVER OPERATOR CHARACTERICTICS (ROC)

class 0

t = 0 (SE=85%, SP=80%)

ğ’˜âˆ—ğ’™ âˆ’ğ‘ =ğ‘¡

class 1

7-46

RECEIVER OPERATOR CHARACTERICTICS (ROC)

class 0

t = 2 (SE=0%, SP=100%)
t = 1 (SE=35%, SP= 90%)
t = 0 (SE=85%, SP=80%)
t = -1
(SE=95%, SP= 20%)

t = -2
(SE=100%, SP=0%)

ğ’˜âˆ—ğ’™ âˆ’ğ‘ =ğ‘¡

class 1

Threshold ğ‘¡ âˆˆ [âˆ’2,2]

7-47

ROC KURVEN UND AUC
t = -2 (SE=100%, SP=0%)

1

t = -1 (SE=95%, SP=20%)

Sensitivity
(True
Positive
Rate)

t = 0 (SE=85%, SP=80%)

Area Under the Curve
ğ´ğ‘ˆğ¶ âˆˆ [0,1]

t = 1 (SE=35%, SP=80%)

t = 2 (SE=0%, SP=100%)

0
Specificity = 1

1
Specificity = 0

1 â€“ Specificity
(False Positive Rate)

âˆ’ Je grÃ¶ÃŸer die AUC, desto besser das Modell
7-48

ROC KURVEN

1

1

exzellent

exzellent
gut

Precision

Sensitivity

gut
Zufall

schlechter als Zufall

0

1-Specificity

Zufall

schlechter als Zufall

1

0

Recall

1

7-49

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ EntscheidungsbÃ¤ume / Random Forests
âˆ’ Support Vector Machines
âˆ’ Auswertung binÃ¤re Klassifikation

7-50

