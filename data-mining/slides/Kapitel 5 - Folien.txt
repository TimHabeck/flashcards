DATA MINING

Kapitel 5: Assoziationsregeln
Dr. Christian Martin
Wintersemester 2025/26

Abteilung Datenbanken / ScaDS.AI
UniversitÃ¤t Leipzig

THEMENÃœBERSICHT
Hochdimensionale Daten

Graphdaten

DatenstrÃ¶me

Clustering

Dimensionsreduktion

Community
Detection

Windowing

Empfehlungssysteme

Assoziationsregeln

PageRank

Filtern

Locality Sensitive
Hashing

Supervised ML

Web Spam

Momente

INHALTSVERZEICHNIS
âˆ’ Einleitung
âˆ’ A-Priori Algorithmus
âˆ’ PCY Algorithmus
âˆ’ Algorithmen mit Stichproben
âˆ’ Prinzip Zufallsstichprobe
âˆ’ SON Algorithmus
âˆ’ Algorithmus von Toivonen

Data Mining | WiSe 2025/2026

5-3

MOTIVATION
âˆ’ Warenkorbanalyse (Market Basket Analysis): Analyse des
Kaufverhaltens
âˆ’ Datengrundlage:
âˆ’ GroÃŸe Menge an Elementen und groÃŸe Menge an WarenkÃ¶rben
âˆ’ Zuordnungen von kleinen Mengen an Elementen zu WarenkÃ¶rben

âˆ’ Beispiel: Kaufverhalten im Supermarkt
âˆ’ Welche Produkte werden von hinreichend vielen Leuten (nicht)
zusammen gekauft?
âˆ’ Klassische (Assoziations-)Regeln:
âˆ’ Wenn jemand BratwÃ¼rste kauft, dann auch Senf.
âˆ’ Wenn jemand Coca Cola kauft, dann nicht Pepsi.
âˆ’ Wenn jemand Windeln kauft, dann auch Bier.
WarenkÃ¶rbe
Brot, Cola, Milch
Milch, Windeln
Bier, Cola, Windeln, Milch
Bier, Brot, Windeln, Milch
Cola, Windeln, Milch, Bier

Assoziationsregeln:
{Milch} â†’ {Cola}
{Windeln, Milch} â†’ {Bier}
Data Mining | WiSe 2025/2026

5-4

ANWENDUNGEN
âˆ’ Kaufverhalten fÃ¼r Werbung, Treueprogramme, Store-Design,
RabattplÃ¤ne oder den â€Querverkaufâ€œ von Produkten
âˆ’ Produktempfehlungen:
âˆ’ â€Kunden, die diesen Artikel gekauft haben, kauften auch â€¦â€œ
âˆ’ â€Kunden, die diesen Film angesehen haben, haben auch angesehen â€¦â€œ

âˆ’ Thematisch verwandte Konzepte
âˆ’ Elemente: WÃ¶rter
âˆ’ WarenkÃ¶rbe: Dokumente

âˆ’ Plagiatserkennung
âˆ’ Elemente: SÃ¤tze
âˆ’ WarenkÃ¶rbe: Dokumente

?
=

âˆ’ Nebenwirkungen von bestimmten Kombination von Medikamenten
âˆ’ Elemente: Nebenwirkung und Medikament
âˆ’ WarenkÃ¶rbe: Patienten

Data Mining | WiSe 2025/2026

5-5

HÃ„UFIGE ELEMENTMENGEN
âˆ’ Elementmenge ğ¼ (Itemset) = Teilmenge der Elemente
âˆ’ Support einer Elementmenge ğ¼ (sup(ğ¼)): Anteil der WarenkÃ¶rbe,
welche alle Elemente aus ğ¼ enthalten
âˆ’ Gegeben eines Schwellenwerts ğ‘ , eine Elementmenge ğ¼ wird als
HÃ¤ufige Elementmenge (Frequent Itemset) bezeichnet, falls
sup(ğ¼) â‰¥ ğ‘ 
WarenkÃ¶rbe
HÃ¤ufige Mengen (ğ‘  = 0.6)
sup
Brot, Cola, Milch

{Milch}

1.0

{Windeln}, {Milch, Windeln}

0.8

{Cola}, {Bier}, {Bier, Milch},
{Cola, Milch}, {Windeln, Bier},
{Bier, Windeln, Milch}

0.6

Milch, Windeln

Bier, Cola, Windeln, Milch
Bier, Brot, Windeln, Milch
Cola, Windeln, Milch, Bier

Data Mining | WiSe 2025/2026

5-6

ASSOZIATIONSREGELN
âˆ’ Assoziationsregel: Wenn-Dann-Regel zu den Inhalten
der WarenkÃ¶rbe
âˆ’ Form: ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ â†’ ğ‘—
âˆ’ Interpretation: Falls ein Korb die ğ‘˜ Elemente ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ enthÃ¤lt,
dann enthÃ¤lt er mit hoher Wahrscheinlichkeit das Element ğ‘—

âˆ’ Auswahl einer Regel Ã¼ber deren Confidence:

sup( ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ , ğ‘— )
conf {ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ } â†’ ğ‘— =
sup( ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ )
âˆ’ Beispiel: falls ğ‘ğ‘œğ‘›ğ‘“ ğ‘–1 , â€¦, ğ‘–ğ‘˜ â†’ ğ‘— > 0.8, dann kommt in
Ã¼ber 80% aller WarenkÃ¶rbe mit den Elementen ğ‘–1 , â€¦, ğ‘–ğ‘˜
auch das Element ğ‘— vor
Data Mining | WiSe 2025/2026

5-7

BEISPIEL
WarenkÃ¶rbe

HÃ¤ufige Mengen (ğ‘  = 0.6)

sup

Brot, Cola, Milch

{Milch}

1.0

{Windeln}, {Milch, Windeln}

0.8

Milch, Windeln
Bier, Cola, Windeln, Milch
Bier, Brot, Windeln, Milch
Cola, Windeln, Milch, Bier

{Cola}, {Bier}, {Bier, Milch}, {Cola,
Milch}, {Windeln, Bier}, {Bier,
0.6
Windeln, Milch}

sup( ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ , ğ‘— )
conf {ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ } â†’ ğ‘— =
sup( ğ‘–1 , ğ‘–2 , â€¦, ğ‘–ğ‘˜ )
sup({ğ¶ğ‘œğ‘™ğ‘, ğ‘€ğ‘–ğ‘™ğ‘â„}) 0.6
ğ‘ğ‘œğ‘›ğ‘“ ğ¶ğ‘œğ‘™ğ‘ â†’ ğ‘€ğ‘–ğ‘™ğ‘â„ =
=
=1
sup({ğ¶ğ‘œğ‘™ğ‘})
0.6
sup({ğ‘€ğ‘–ğ‘™ğ‘â„, ğµğ‘–ğ‘’ğ‘Ÿ}) 0.6
ğ‘ğ‘œğ‘›ğ‘“ ğ‘€ğ‘–ğ‘™ğ‘â„ â†’ ğµğ‘–ğ‘’ğ‘Ÿ =
=
= 0.6
sup({ğ‘€ğ‘–ğ‘™ğ‘â„})
1

Data Mining | WiSe 2025/2026

5-8

BEISPIEL
WarenkÃ¶rbe

HÃ¤ufige Mengen (ğ‘  = 0.6)

sup

Brot, Cola, Milch

{Milch}

1.0

{Windeln}, {Milch, Windeln}

0.8

Milch, Windeln
Bier, Cola, Windeln, Milch
Bier, Brot, Windeln, Milch
Cola, Windeln, Milch, Bier

{Cola}, {Bier}, {Bier, Milch}, {Cola,
Milch}, {Windeln, Bier}, {Bier,
0.6
Windeln, Milch}

Regeln
conf
{Cola} â†’ {Milch}, {Bier} â†’ {Windeln}, {Bier} â†’ {Milch}, {Windeln}
1.0
â†’ {Milch}, {Bier,Windeln} â†’ {Milch}, {Bier, Milch} â†’ {Windeln}
{Milch} â†’ {Windeln}
0.8
{Windeln} â†’ {Bier}, {Windeln, Milch} â†’ {Bier}

0.75

{Milch} â†’ {Cola}, {Milch} â†’ {Bier}

0.6

Data Mining | WiSe 2025/2026

5-9

INTEREST EINER REGEL
âˆ’ Nicht alle Regeln mit hoher Konfidenz sind wichtig,
z.B. Milch kommt oft vor, weil sie einfach fast immer
gekauft wird â†’ ist in jedem Korb vorhanden
âˆ’ Interest einer Regel
ğ‘–ğ‘›ğ‘¡ I â†’ ğ‘— = ğ‘ğ‘œğ‘›ğ‘“ I â†’ j âˆ’ P[j]
âˆ’ P[j] ist relative HÃ¤ufigkeit des Elementes j
âˆ’ Interessant sind Regeln mit
âˆ’ ğ‘–ğ‘›ğ‘¡ I â†’ ğ‘— > 0.5 oder
âˆ’ ğ‘–ğ‘›ğ‘¡ I â†’ ğ‘— < âˆ’0.5

âˆ’ Beispiel:
âˆ’ ğ‘–ğ‘›ğ‘¡ Cola â†’ {ğ‘€ğ‘–ğ‘™ğ‘â„} = 1.0 âˆ’ 1.0 = 0
âˆ’ ğ‘–ğ‘›ğ‘¡ Windeln â†’ {ğµğ‘–ğ‘’ğ‘Ÿ} = 0.75 âˆ’ 0.6 = 0.15
Data Mining | WiSe 2025/2026

5-10

DIE SUCHE NACH ASSOZIATIONSREGELN
âˆ’ Problem: Finde alle Assoziationsregeln ğ¼ â†’ ğ‘—
mit sup(ğ¼ âˆª ğ‘— ) â‰¥ ğ‘  und conf ğ¼ â†’ ğ‘— â‰¥ ğ‘
âˆ’ Schwierigkeit: Suche nach hÃ¤ufigen Elementmengen
mit ğ‘ ğ‘¢ğ‘(ğ¼) â‰¥ ğ‘  bzw. die Berechnung des Supports aller
mÃ¶glichen Elementmengen
âˆ’ Konstruktion der Assoziationsregeln aus hÃ¤ufigen
Elementmengen
âˆ’ FÃ¼r jede hÃ¤ufige Elementmenge ğ¼ und jedes Element ğ‘— aus ğ¼
âˆ’ Erstelle die Regel ğ¼\ ğ‘— â†’ ğ‘—
ğ‘ ğ‘¢ğ‘ ğ¼

âˆ’ Ausgabe der Regel, falls conf ğ¼\ ğ‘— â†’ ğ‘— = ğ‘ ğ‘¢ğ‘ ğ¼\ ğ‘— â‰¥ ğ‘
âˆ’ (da ğ¼ hÃ¤ufig ist, ist auch ğ¼\{ğ‘—} hÃ¤ufig; somit wurde sowohl sup(ğ¼) als
auch sup(ğ¼\{ğ‘—}) schon berechnet)

Data Mining | WiSe 2025/2026

5-11

SUCHE NACH HÃ„UFIGEN ELEMENTMENGEN
âˆ’ Annahme:
Daten sind in Dateisystem auf Festplatte gespeichert
âˆ’ Sortiert nach WarenkÃ¶rben
âˆ’ Wenige Elemente pro Korb: Aufwand der Generierung
aller Teilmengen eines Korbs ist relativ gering
âˆ’ Entscheidend: Anzahl der WarenkÃ¶rbe, insb. wenn
diese nicht vollstÃ¤ndig in Hauptspeicher passen
âˆ’ 1. Problem: Speichern der Zwischenergebnisse
(HÃ¤ufigkeiten der Teilmengen) in Hauptspeicher
âˆ’ 2. Problem: Evtl. mehrere FestplattendurchlÃ¤ufe
nÃ¶tig (Lesen aller WarenkÃ¶rbe von Festplatte)

Data Mining | WiSe 2025/2026

Item
Item
Item
Item
Item
Item
Item
Item
Item
Item
Item
Item

etc.

5-12

PROBLEM: SPEICHERN DER ZWISCHENERGEBNISSE
âˆ’ Beispiel: Bestimmen der hÃ¤ufigen Paare
âˆ’ ZÃ¤hlen im Hauptspeicher ist nicht fÃ¼r groÃŸe Dimensionen geeignet
âˆ’ Angenommen ğ‘› verschiedene Elemente und Speichern der ZÃ¤hler
(Integer: 4 Byte) als Dreiecksmatrix = Array der Form {1,2}, {1,3},â€¦,
{1,n}, {2,3}, {2,4},â€¦,{2,n}, {3,4},â€¦
ğ‘›
ğ‘›(ğ‘›âˆ’1)
ğ‘›2
âˆ’ Speicherplatz fÃ¼r
=
â‰ˆ ZÃ¤hler Ã¡ 4 Byte: 2ğ‘›2 Byte
2
2
2
âˆ’ Bei 32 GB Hauptspeicher: ğ‘› < 217 â‰ˆ 130 000 Elemente
âˆ’ Damit wÃ¼rde weder Walmart (17 Millionen Produkte) noch Amazon (500
Millionen Produkte) auskommen

âˆ’ Weniger Speicherplatz notwendig, falls nicht alle Paare auftreten
âˆ’ Speichern als spÃ¤rlich besetzte Matrix: Liste von Tripeln [ğ‘–, ğ‘—, ğ‘] wobei ğ‘–
und ğ‘— die Elemente bezeichnen und ğ‘ den ZÃ¤hler: 12 Byte pro Paar
âˆ’ Weniger Speicherplatz, falls maximal 1/3 der Paare auftreten

Data Mining | WiSe 2025/2026

5-13

INHALTSVERZEICHNIS
âˆ’ Einleitung
âˆ’ A-Priori Algorithmus
âˆ’ PCY Algorithmus
âˆ’ Algorithmen mit Stichproben
âˆ’ Prinzip Zufallsstichprobe
âˆ’ SON Algorithmus
âˆ’ Algorithmus von Toivonen

Data Mining | WiSe 2025/2026

5-14

A-PRIORI ALGORITHMUS

ğ½ = ğ‘, ğ‘ mit sup ğ½ = â‹¯
ğ¼ = ğ‘, ğ‘, ğ‘ dann sup(ğ¼) â‰¤ sup(ğ½)

âˆ’ Wichtige Eigenschaft der hÃ¤ufigen Elementmengen: Monotonie
ğ½ âŠ† ğ¼ â‡’ sup(ğ¼) â‰¤ sup(ğ½)
âˆ’ Gegenrichtung: Falls eine Menge ğ½ keine hÃ¤ufige Elementmenge ist,
dann ist keine Menge ğ¼, welche ğ½ enthÃ¤lt, eine hÃ¤ufige
Elementmenge
âˆ’ A-Priori Algorithmus: Begrenzung des benÃ¶tigten (Haupt-)
Speicherplatzes Ã¼ber zweifaches Einlesen aller Daten
âˆ’ 1. Durchlauf: ZÃ¤hlen aller Elemente (einelementige Mengen) und
Auswahl der hÃ¤ufigen Elemente Ã¼ber Schwellenwert ğ‘ 
âˆ’ 2. Durchlauf: ZÃ¤hlen eines vorkommenden Paares nur dann, wenn
beide Elemente hÃ¤ufig sind
Data Mining | WiSe 2025/2026

5-15

A-PRIORI ALGORITHMUS: DETAILS
âˆ’ Tabelle: Durchnummerierung der Elemente von 1, â€¦, n
âˆ’ 1. Durchlauf: ZÃ¤hler fÃ¼r Elemente ist einfaches Array der LÃ¤nge n
âˆ’ Danach: Neues Array der gleichen LÃ¤nge, aber Zuordnung der
hÃ¤ufigen Element zu neuer Nummerierung 1, â€¦, m und seltene
Elemente auf 0
Elemente â†’
{1, â€¦, n}

ZÃ¤hler fÃ¼r
Elemente

HÃ¤ufige
Elemente

RAM

âˆ’ 2. Durchlauf: Speicherung der
Paare hÃ¤ufiger Elemente Ã¼ber
Dreiecksmatrix
(da spÃ¤rlich besetzte Matrix)

Elemente â†’
{1, â€¦, m}

ZÃ¤hler fÃ¼r
Paar hÃ¤ufiger
Elemente

1. Durchlauf

2. Durchlauf

Data Mining | WiSe 2025/2026

5-16

HÃ„UFIGE TRIPEL ETC.
âˆ’ Weiterer Durchlauf fÃ¼r jede Menge der GrÃ¶ÃŸe k
âˆ’ Menge ğ¶ğ‘˜ von Kandidaten
âˆ’ Kandidat = Menge der GrÃ¶ÃŸe k, die aufgrund der Informationen Ã¼ber
Mengen der GrÃ¶ÃŸe ğ‘˜ âˆ’ 1 hÃ¤ufig sein kÃ¶nnten
âˆ’ FÃ¼r einen Kandidat gilt: alle (k-1)-elementigen Teilmengen mÃ¼ssen
hÃ¤ufig sein

âˆ’ Menge ğ¿ğ‘˜ der hÃ¤ufigen Mengen der GrÃ¶ÃŸe k
ZÃ¤hlen und
Aussortieren

C1

Alle Elemente

Filter

Alle Elementpaare von ğ¿1

L1

Erzeuge

HÃ¤ufige Elemente

Paare ZÃ¤hlen und
Aussortieren

Alle ElementTripel von ğ¿2

Filter

Erzeuge

C2
Alle Paare

L2

HÃ¤ufige Paare

1. Durchlauf

...

C3

Alle Tripel

2. Durchlauf
Data Mining | WiSe 2025/2026

5-17

BEISPIEL
ğ¶1 = {{b}, {c}, {j}, {m}, {n}, {p}}
Neue
Elementmengen
generieren

â†“
ğ¿1 = {{b}, {c}, {j}, {m}}

1. Support berechnen
2. Bestimmung
hÃ¤ufiger Mengen
sup(ci ) â‰¥ ğ‘ 

â†“
ğ¶2 = {{b, c}, {b, j}, {b, m}, {c, j}, {c, m}, {j, m}}

â†“
Neue
Elementmengen
generieren

ğ¿2 = {{b, c}, {b, m}, {c, j}, {c, m}}

â†“

1. Support berechnen
2. Bestimmung
hÃ¤ufiger Mengen
sup(ci ) â‰¥ ğ‘ 

ğ¶3 = {{b, c, m}}

â†“
ğ¿3 = {}
Data Mining | WiSe 2025/2026

Warum z.B. {b,c,j}
nicht?

5-18

WEITERES BEISPIEL

Achtung: andere
Definition von
Support.

Dol Aher, Sunita & J, Lobo. (2012). A Comparative Study of Association Rule Algorithms for Course Recommender System in E-learning. International Journal of Computer
Applications. 39. 48-52. 10.5120/4788-7021.

Data Mining | WiSe 2025/2026

5-19

INHALTSVERZEICHNIS
âˆ’ Einleitung
âˆ’ A-Priori Algorithmus
âˆ’ PCY Algorithmus
âˆ’ Algorithmen mit Stichproben
âˆ’ Prinzip Zufallsstichprobe
âˆ’ SON Algorithmus
âˆ’ Algorithmus von Toivonen

Data Mining | WiSe 2025/2026

5-20

PARK-CHEN-YU (PCY) ALGORITHMUS
âˆ’ Ziel: Finde hÃ¤ufige Elementmengen in groÃŸen DatensÃ¤tzen
âˆ’ In einigen FÃ¤llen kÃ¶nnte Hauptspeicher fÃ¼r den 2. Durchlauf des
A-Priori Algorithmus nicht ausreichen
âˆ’ Idee: Verwende den restlichen Speicherplatz im 1. Durchlauf, um die
Kandidatenmenge weiter zu reduzieren
âˆ’ PCY-Algorithmus:
âˆ’ Verwendung einer Hash-Funktion (z.B. â„ ğ‘–, ğ‘— = ğ‘– + ğ‘— ğ‘šğ‘œğ‘‘ 3) mit
mÃ¶glichst vielen Buckets und einem Array aus ZÃ¤hlern (ein ZÃ¤hler
pro Bucket)
âˆ’ FÃ¼r jeden Warenkorb:
âˆ’ FÃ¼r jedes Element des Warenkorbs: Addiere 1 zu dem ZÃ¤hler des
Elements
c:2 c:4 c:4
âˆ’ FÃ¼r jedes Paar von Elementen:
âˆ’ Anwendung der Hash-Funktion auf Paar
âˆ’ Addiere 1 zu dem ZÃ¤hler des Bucket
Data Mining | WiSe 2025/2026

0

1

2

{1,2} {1,3} {1,4}
...
{2,7} ...

5-21

PCY ALGORITHMUS
âˆ’ Beobachtungen:
âˆ’ Falls ein Bucket ein hÃ¤ufiges Paar enthÃ¤lt (Support grÃ¶ÃŸer als ein Schwellenğ‘Ã¤â„ğ‘™ğ‘’ğ‘Ÿ
wert ğ‘ ), dann ist der Anteil
dieses Bucket grÃ¶ÃŸer als ğ‘ 
ğ´ğ‘›ğ‘§ğ‘â„ğ‘™ ğ‘‘ğ‘’ğ‘Ÿ ğ‘Šğ‘ğ‘Ÿğ‘’ğ‘›ğ‘˜Ã¶ğ‘Ÿğ‘ğ‘’

âˆ’ Auch seltene Paare (sup < ğ‘ ) kÃ¶nnen in Buckets mit einem Anteil > ğ‘ 
vorkommen
âˆ’ Buckets mit einem Anteil < ğ‘  kÃ¶nnen keine hÃ¤ufigen Paare (sup > ğ‘ ) enthalten

âˆ’ Folgerung: Alle Paare aus Buckets mit
Anteil < ğ‘  mÃ¼ssen im 2. Durchlauf
nicht betrachtet werden
âˆ’ FÃ¼r 2. Durchlauf:

Elemente â†’
{1, â€¦, n}

Elemente â†’
{1, â€¦, n}

ZÃ¤hler fÃ¼r
Elemente

HÃ¤ufige
Elemente

âˆ’ Ersetze Hash-Tabelle durch Bit-Vektor:
1 bedeutet, dass Anteil des Bucket grÃ¶ÃŸer
als ğ‘  ist

Bit-Vektor
HashTabelle

ZÃ¤hler fÃ¼r
Paare hÃ¤ufiger
Elemente

1. Durchlauf

2. Durchlauf

1

âˆ’ Bit-Vektor benÃ¶tigt nur des Speichers
32
der Hash-Tabelle (Integer: 4 Byte)

Data Mining | WiSe 2025/2026

5-22

PCY ALGORITHMUS
âˆ’ 2. Durchlauf: ZÃ¤hle ein Paar {ğ‘–, ğ‘—} genau dann, wenn
âˆ’ beide Element ğ‘– und ğ‘— hÃ¤ufig sind und
âˆ’ {ğ‘–, ğ‘—} Ã¼ber die Hash-Funktion auf ein Bucket mit Wert 1 im Bit-Vektor abgebildet
wird

âˆ’ Anmerkungen:
âˆ’ GroÃŸe Anzahl an Buckets notwendig
âˆ’ Die Buckets der Hash-Tabelle benÃ¶tigen nur wenige Bytes: man muss nur
bis ğ‘  âˆ™ Anzahl der WarenkÃ¶rbe zÃ¤hlen â†’ Je nach Hauptspeicher: groÃŸe Anzahl
an Buckets mÃ¶glich
âˆ’ PCY sollte 2/3 der Kandidaten eliminieren, damit Liste von Tripeln (spÃ¤rlich
besetzte Matrix) im 2. Durchlauf verwendbar und PCY tatsÃ¤chlich effizienter als
A-Priori

âˆ’ Erweiterungen:
âˆ’ Falls Hauptspeicher fÃ¼r 2. Durchlauf noch nicht ausreichend: weitere
EinschrÃ¤nkung der Kandidatenmenge Ã¼ber erneuten Durchlauf mÃ¶glich
(Multistage PCY)
âˆ’ In manchen FÃ¤llen ist es effizient, zwei verschiedene Hash-Funktionen
anzuwenden und daraus zwei kleinere Bit-Vektoren zu erzeugen (Multihash PCY)
Data Mining | WiSe 2025/2026

5-23

INHALTSVERZEICHNIS
âˆ’ Einleitung
âˆ’ A-Priori Algorithmus
âˆ’ PCY Algorithmus
âˆ’ Algorithmen mit Stichproben
âˆ’ Prinzip Zufallsstichprobe
âˆ’ SON Algorithmus
âˆ’ Algorithmus von Toivonen

Data Mining | WiSe 2025/2026

5-24

PRINZIP ZUFALLSSTICHPROBE

âˆ’ Vermeidung von False Positives: Verifiziere die Ã¼ber
die Stichprobe ausgewÃ¤hlten hÃ¤ufigen Elementmengen
durch einen weiteren Durchlauf (benÃ¶tigt weniger
Speicher, da weniger Kandidaten)
âˆ’ Reduzierung der False Negatives: Kleineren
Schwellenwert, z.B. 0.9 âˆ™ ğ‘  (benÃ¶tigt mehr Speicherplatz)
Data Mining | WiSe 2025/2026

Hauptspeicher

âˆ’ Problem: Datenmenge ist so groÃŸ, dass immer nur Teile/Stichproben
betrachtet werden kÃ¶nnen
â†’ ziehe Zufallsstichprobe, so dass alle WarenkÃ¶rbe dieser Stichprobe
und die benÃ¶tigten ZÃ¤hler in den Hauptspeicher passen
âˆ’ Anwendung von A-Priori oder PCY im Hauptspeicher
âˆ’ Nur ein Festplattendurchlauf fÃ¼r die Suche nach den
hÃ¤ufigen Elementmengen aller GrÃ¶ÃŸen
Stichprobe

Platz fÃ¼r
ZÃ¤hler

5-25

SON Algorithmus
âˆ’ Savasere-Omiecinski-Navathe (SON) Algorithmus:
âˆ’ Lade wiederholt eine feste Anzahl an WarenkÃ¶rben (Chunk) in
Hauptspeicher
âˆ’ Finde hÃ¤ufige Elementmengen aller GrÃ¶ÃŸen
âˆ’ Gesamter Datensatz wird in Teilen durchlaufen und hÃ¤ufige
Elementmengen der Chunks werden gespeichert
âˆ’ Keine False Negatives: Eine Elementmenge kann nicht im
gesamten Datensatz mit Schwellenwert ğ‘  hÃ¤ufig vorkommen, ohne in
mindestens einem Chunk mit Schwellenwert ğ‘  hÃ¤ufig vorzukommen
âˆ’ Aussortieren der False Positives Ã¼ber weiteren Durchlauf

Data Mining | WiSe 2025/2026

5-26

Algorithmus von Toivonen
âˆ’ 1. Durchlauf:
âˆ’ Ziehen einer Zufallsstichprobe, die Arbeit in Hauptspeicher
erlaubt
âˆ’ Nimm die Elementmengen aller GrÃ¶ÃŸen, die mit
Schwellenwert s â€² = 0.9 âˆ™ ğ‘  hÃ¤ufig sind, als Kandidaten
âˆ’ Erstellen der Negativen Grenze zu Kandidaten:

Eine Menge ist in der Negativen Grenze, falls
âˆ’ sie kein Kandidat ist (nicht hÃ¤ufig, also sup(ğ¼) < ğ‘ â€²), aber
âˆ’ alle direkten Untermengen (Untermengen haben genau ein
Element weniger) Kandidaten oder die leere Menge sind:
alle ğ¼â€² âŠ‚ ğ¼ mit |ğ¼â€²| = |ğ¼| âˆ’ 1 sind hÃ¤ufig, d.h., sup(ğ¼â€²) â‰¥ ğ‘ â€²

Data Mining | WiSe 2025/2026

5-27

Algorithmus von Toivonen - Beispiel
âˆ’ Erstellen der Negativen Grenze zu Kandidaten:
Eine Menge ist in der Negativen Grenze, falls
âˆ’ sie kein Kandidat ist, aber
âˆ’ Alle direkten Untermengen (Untermengen haben genau ein
Element weniger) Kandidaten oder die leere Menge sind.
Beispiel: Annahmen:
âˆ’ Grundmenge {A,B,C,D,E}
âˆ’ {A}, {B}, {C}, {D}, {B,C}, {C,D} kommen hÃ¤ufig in Stichprobe vor
Dann:
âˆ’ Negative Grenze: {E}, {A,B}, {A,C}, {A,D}, {B,D}
âˆ’ {E}, da {} immer hÃ¤ufig ist
âˆ’ Die anderen Paare sind nicht in Negativer Grenze, da sie entweder E
enthalten oder hÃ¤ufig vorkommen
âˆ’ kein Tripel ist in Negativer Grenze
âˆ’ z.B. {B,C,D} nicht, da {B,D} nicht hÃ¤ufig
Data Mining | WiSe 2025/2026

5-28

Algorithmus von Toivonen
âˆ’ 2. Durchlauf: ZÃ¤hlen der Kandidaten und der Elemente der Negativen
Grenze im gesamten Datensatz
âˆ’ Aussortieren der False Positives
âˆ’ falls kein Element der Negativen Grenze hÃ¤ufig vorkommt, wurden alle
hÃ¤ufigen Elementmengen gefunden (keine False Negatives)
âˆ’ falls ein Element der Negativen Grenze hÃ¤ufig vorkommt, muss der
Algorithmus mit neuer Zufallsstichprobe wiederholt werden (evtl. mit
geringerem Schwellenwert)

Data Mining | WiSe 2025/2026

5-29

ZUSAMMENFASSUNG
âˆ’ Assoziationsregeln
âˆ’ Support von I: Anteil der WarenkÃ¶rbe, welche alle Elemente aus I
enthalten
âˆ’ HÃ¤ufige Elementmengen wenn sup(ğ¼) â‰¥ ğ‘ 
âˆ’ Auswahl einer Regel Ã¼ber deren Confidence

âˆ’ A-Priori Algorithmus
âˆ’ 2 DurchlÃ¤ufe, Ausnutzen von Monotonie

âˆ’ PCY Algorithmus
âˆ’ Verwenden von Hash-Funktion

âˆ’ Algorithmen mit Stichproben
âˆ’ Prinzip Zufallsstichprobe
âˆ’ SON Algorithmus (Datensatz wird in Chunks durchlaufen)
âˆ’ Algorithmus von Toivonen (Negative Grenze)
Data Mining | WiSe 2025/2026

5-30

