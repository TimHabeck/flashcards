DATA MINING

Kapitel 2: Clustering
Dr. Christian Martin
Wintersemester 2025/26

Abteilung Datenbanken / ScaDS.AI
UniversitÃ¤t Leipzig

THEMENÃœBERSICHT
Hochdimensionale Daten

Graphdaten

DatenstrÃ¶me

Clustering

Dimensionsreduktion

Community
Detection

Windowing

Empfehlungssysteme

Assoziationsregeln

PageRank

Filtern

Locality Sensitive
Hashing

Supervised ML

Web Spam

Momente

2-2

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hierarchische Clusteranalyse
âˆ’ Hierarchisch agglomeratives Clustern

âˆ’ Partitionierende Clusteranalyse
âˆ’ k-means-Algorithmus
âˆ’ BFR-Algorithmus
âˆ’ CURE-Algorithmus

2-3

LONDON, 1854
âˆ’ Cholera Epidemie
âˆ’ 578 Tote
âˆ’ John Snow
(1813 â€“ 1858)

Quelle: Wikipedia

âˆ’ kann als Anfang von
Data Mining / Science
gesehen werden

Quelle: Wikipedia
2-4

CLUSTERING
âˆ’ Gegeben einer Menge von ğ‘µ Datenpunkten im â„ğ‘‘
âˆ’ ğ’™ğŸ = x11 , x12 , â€¦ , x1d ,
âˆ’ ğ’™ğŸ = x21 , x22 , â€¦ , x2d ,
âˆ’ â€¦,
âˆ’ ğ’™ğ‘ = xğ‘1 , xğ‘2 , â€¦ , xNd
âˆ’ Distanzfunktion ğ’… ğ’™i , ğ’™ğ‘— :
âˆ’ Euklidisch: ğ‘‘ ğ‘¥ğ‘– , ğ‘¥ğ‘— =

2
ğ‘‘
Ïƒğ‘˜=1 ğ‘¥ğ‘–ğ‘˜ âˆ’ ğ‘¥ğ‘—ğ‘˜

âˆ’ Manhattan-Norm: ğ‘‘ ğ‘¥ğ‘– , ğ‘¥ğ‘— = Ïƒğ‘‘ğ‘˜=1 ğ‘¥ğ‘–ğ‘˜ âˆ’ ğ‘¥ğ‘—ğ‘˜
âˆ’ Ziel: Gruppierung der Datenpunkte in Cluster, so daÃŸ
âˆ’ Mitglieder eines Cluster eine geringe paarweise Distanz aufweisen
âˆ’ Mitglieder verschiedener Cluster eine hohe paarweise Distanz aufweisen
2-5

WIE CLUSTER BILDEN?

Wieviele Cluster?

6 Cluster

2 Cluster

4 Cluster

1-6

VISUALISIERUNG DES ERGEBNISSES
âˆ’ Visualisierung mÃ¶glich bei nur 2 Dimensionen:

x
xx x
x x
x x x
x
xx x
x

x
x
x x
x x
x xx x
x x x
x x
Cluster

Outlier
x
x x
x x x x
x x x
x

2-7

VISUALISIERUNGEN VON CLUSTERDETAILS (1)
âˆ’ Box (and -Whisker-Plot)
âˆ’ Median, Q1, Q3
âˆ’ Interquartile range IRQ
= Q3 - Q1
âˆ’ Whiskers:
âˆ’ Q1 â€“ 1,5 * IQR
âˆ’ Q3 + 1,5 * IQR
âˆ’ Whisker ends at the last
real data point within the
range

âˆ’ Outlier: data points
outside of the whiskers
https://en.wikipedia.org/wiki/Box_plot

2-8

VISUALISIERUNGEN VON CLUSTERDETAILS (2)

https://towardsdatascience.com/best-practices-for-visualizing-your-cluster-results-20a3baac7426

âˆ’ Visualisierung der einzelnen Cluster und Variablen mit
Box-and-Whisker Plots
2-9

VISUALISIERUNGEN VON CLUSTERDETAILS (3)

https://towardsdatascience.com/best-practices-for-visualizing-your-cluster-results-20a3baac7426

âˆ’ Visualisierung der einzelnen Cluster und Variablen mit
Box-and-Whisker Plots
2-10

VISUALISIERUNGEN VON CLUSTERDETAILS (4)
âˆ’ Mittelwerte der Parameter und Cluster im Vergleich zu
Gesamt-Mittelwert

2-11
https://towardsdatascience.com/best-practices-for-visualizing-your-cluster-results-20a3baac7426

VISUALISIERUNGEN VON CLUSTERDETAILS (5)
âˆ’ Mittelwerte der Parameter und Cluster im Vergleich zu
Gesamt-Mittelwert

https://towardsdatascience.com/best-practices-for-visualizing-your-cluster-results-20a3baac7426

2-12

VISUALISIERUNGEN VON CLUSTERDETAILS (6)
âˆ’ Visualisierung
mit StarPlot

2-13
https://towardsdatascience.com/best-practices-for-visualizing-your-cluster-results-20a3baac7426

ANWENDUNG: SEGMENTIERUNG VON BILDERN
âˆ’ Clustering der Farben
â†’ Regionen mit Ã¤hnlichen Farben ergeben ein Cluster

https://www.researchgate.net/publication/285926394_Piecewise_Flat_Embedding_for_Image_Segmentation/figures?lo=1

1-14

ANWENDUNG: DNA SEQUENCE CLUSTERING

doi: https://doi.org/10.1371/journal.pcbi.1002958.g003

1-15

ANWENDUNG: CLUSTERING VON DOKUMENTEN

1-16
https://www.kaggle.com/code/maksimeren/covid-19-literature-clustering

ANWENDUNG: KUNDENSEGMENTIERUNG

https://www.business-wissen.de/hb/kundensegmentierung-mit-clusteranalyse/

1-17

FLUCH DER HOHEN DIMENSIONEN
âˆ’ Clustering ist anspruchsvoll im Fall groÃŸer Datenmengen
âˆ’ gegebene Anzahl an Clustern ğ‘˜
âˆ’ ğ’Œğ‘µ MÃ¶glichkeiten die N Punkte in ğ‘˜ Cluster zu ordnen
ğ‘
âˆ’ Paarweiser Vergleich erfordert Berechnung von
Ã„hnlichkeiten
2

âˆ’ Clustering ist anspruchsvoll bei hoher Dimension der Datenpunkte
âˆ’ Oft: 10-10.000 Dimensionen
âˆ’ Datenpunkte liegen im Wesentlichen auf den Kanten des Datenraumes
âˆ’ The Curse of Dimensionality: Im Falle einer sehr hohen Dimension
haben fast alle Paare von Datenpunkten eine Ã¤hnliche Distanz

2-18

ÃœBERSICHT: CLUSTERVERFAHREN
âˆ’ Hierarchisch:
âˆ’ Agglomerativ (Bottom-up):
âˆ’ Zu Beginn bildet jeder Punkt ein Cluster
âˆ’ Wiederholtes Kombinieren von zwei
Ã¤hnlichen Clustern zu einem neuen Cluster

âˆ’ Divisiv (Top-down):
âˆ’ Ein groÃŸes Cluster zu Beginn
âˆ’ Wiederholtes Aufteilen eines groÃŸen Clusters
in zwei unÃ¤hnliche kleinere Cluster

âˆ’ Partitionierend:
âˆ’ Feste Anzahl an Cluster
âˆ’ Zuordnen der Punkte zu den Clustern
2-19

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hierarchische Clusteranalyse
âˆ’ Hierarchisch agglomeratives Clustern

âˆ’ Partitionierende Clusteranalyse
âˆ’ k-Means-Algorithmus
âˆ’ BFR-Algorithmus
âˆ’ CURE-Algorithmus

2-20

HIERARCHISCHE CLUSTERANALYSE
âˆ’ Agglomerativ: Wiederholtes Kombinieren der beiden
Cluster mit geringster Distanz zu einem neuen Cluster
âˆ’ Dendrogramm:
âˆ’ BlÃ¤tter (unterste Ebene)
reprÃ¤sentieren die
Datenpunkte
âˆ’ Jeder Punkt ist ein Cluster
âˆ’ HÃ¶he der Vereinigung gibt die
Distanz zwischen den beiden
Clustern an

âˆ’ Zwei Kriterien:
âˆ’ Definition der Distanz zwischen zwei Clustern
âˆ’ Stoppregel

2-21

HIERARCHISCHE CLUSTERANALYSE
(AGGLOMERATIV)
âˆ’ MÃ¶gliche Definitionen der Distanz zwischen zwei Clustern:
âˆ’ Single linkage: Minimale paarweise Distanz zwischen allen Mitgliedern der
beiden Cluster
âˆ’ Complete linkage: Maximale paarweise Distanz zwischen allen Mitgliedern der
beiden Cluster
âˆ’ Average linkage: Durchschnittliche paarweise Distanz zwischen allen Mitgliedern
der beiden Cluster
âˆ’ Centroid linkage: Distanz zwischen den beiden Centroiden der Cluster (Centroid
= Arithmetisches Mittel aller Punkte des Clusters)
âˆ’ Ward linkage: Zunahme der Varianz, wenn die Cluster zusammengefÃ¼hrt werden

âˆ’ Anmerkungen:
âˆ’ Single linkage bildet oft entartete BÃ¤ume
âˆ’ Complete/Average linkage bilden ausgeglichene BÃ¤ume
âˆ’ Single/Complete/Average linkage auch in nicht euklidischen RÃ¤umen direkt
anwendbar
âˆ’ Centroid/Ward linkage: Erfordern Berechnung eines Centroiden, dies ist jedoch
nicht immer direkt mÃ¶glich (Beispiel: Vergleich von Strings)
2-22

HIERARCHISCHE CLUSTERANALYSE
(AGGLOMERATIV)
âˆ’ MÃ¶gliche Stoppregeln:
âˆ’ Anzahl der Cluster
âˆ’ Maximale Distanz innerhalb des neu entstandenen Clusters Ã¼bersteigt
Schwellenwert
âˆ’ Durchschnittliche maximale Distanz steigt stark an
âˆ’ â€Dichteâ€œ der Cluster liegt unter einem Schwellenwert

âˆ’ Auch die Skalierung der Dimensionen hat Einfluss auf das Clustering
âˆ’ Bei Daten mit verschiedenen Attributen (z.B. Einkommen und KÃ¶rpergrÃ¶ÃŸe)
âˆ’ Daten sollten standardisiert werden, so dass alle Attribute einen Mittelwert von 0
und eine Standardabweichung von Eins aufweisen

2-23

BEISPIEL: VERWENDUNG DER CENTROIDEN
o (5,3)

(1,2)
o
x (1.5,1.5)
(1,1) x

o (2,1)

o (0,0)

o â€¦ Datenpunkt
x â€¦ Centroid

(4.7,1.3) x

o (4,1)
x
(4.5,0.5) o (5,0)

Dendrogram
2-24

HIERARCHISCHE CLUSTERANALYSE
âˆ’ Auch in Nicht-Euklidischen RÃ¤umen mÃ¶glich
âˆ’ z.B. Ã¼ber Jaccard-Metrik
âˆ’ Anstatt Centroid: Clustroid = Punkt aus dem Cluster mit
âˆ’ Minimaler Summe aller Distanzen zu den anderen Punkten des
Clusters, oder
âˆ’ Minimaler maximale Distanz zu den anderen Punkten des Clusters

âˆ’ Oder Anwendung eines Verfahrens ohne Centroid

âˆ’ Bestes Verfahren zur Auswahl zweier Cluster hÃ¤ngt von
der Form der tatsÃ¤chlichen Cluster ab (welche man nicht
kennt)

2-25

KOMPLEXITÃ„T (BEARBEITUNGSZEIT)
âˆ’ Single Linkage (Ã¼ber minimale Distanz): ğ‘‚(ğ‘ 2 )
âˆ’ Einmal: Berechnung aller paarweiser Distanzen und Sortierung
der Paare aufsteigend nach Distanz
âˆ’ ZusammenfÃ¼gen der Cluster in dieser Reihenfolge

âˆ’ Alle anderen Verfahren: ğ‘‚(ğ‘ 2 logğ‘)
âˆ’ Start: Berechnung aller paarweiser Distanzen und Sortierung der
Paare aufsteigend nach Distanz
âˆ’ Naiv: In jedem Schritt mÃ¼ssen DistanzmaÃŸe zwischen allen
Clustern neu berechnet werden: ğ‘‚(ğ‘ 2 ), ğ‘‚((ğ‘ âˆ’ 1)2 ), ğ‘‚àµ«(ğ‘ âˆ’
2)2 àµ¯, â€¦,
âˆ’ Da N Schritte, also ğ‘‚(ğ‘ 3 ) insgesamt
âˆ’ Bei Verwendung einer Priority Queue: ğ‘‚(ğ‘ 2 log ğ‘)
2-26

KOMPLEXITÃ„T â€“ PRIORITY QUEUE
âˆ’ Abstrakter Datentyp (wie Stack, Heap, Queue)
âˆ’ Verschiedene Implementierungen mÃ¶glich
âˆ’ z.B. Implementierung als Heap mit binÃ¤rem Baum
âˆ’ Aufwand fÃ¼r Operationen Edit / Delete
âˆ’ Naiver Ansatz: ğ‘‚ ğ‘
âˆ’ Mit binÃ¤rer Suche: ğ‘‚ log ğ‘

2-27

KOMPLEXITÃ„T (BEARBEITUNGSZEIT)
âˆ’ Priority Queue: VerÃ¤nderungen in ğ‘‚ log ğ‘
âˆ’ Vorgehen:
âˆ’ Berechnung aller Distanzen und Sortierung der Paare nach
Distanz in Priority Queue P: ğ‘‚ ğ‘ 2

C,D C,E A,B
1.2 2.1 2.3

D,A D,B A,E
2.8 3.3 3.5

C,B â€¦
4.0 â€¦

âˆ’ Wiederhole:
âˆ’ Finden des Minimums (in ğ‘‚ 1 ), hier Paar (C, D)
âˆ’ Entfernen aller Elemente aus P, welche sich auf C oder D beziehen,
z.B. (C,D), (C,E), (D,A), â€¦: max. 2ğ‘ VerÃ¤nderungen, also
ğ‘‚ ğ‘ log ğ‘
âˆ’ Berechnung aller Distanzen zwischen neuem Cluster X = (C,D) und
anderen Clustern, sowie HinzufÃ¼gen dieser Paare zu P: ğ‘‚ ğ‘ log ğ‘

âˆ’ Maximal ğ‘ Wiederholungen, also ğ‘‚(ğ‘ 2 log ğ‘) insgesamt
2-28

ANWENDUNG: CLUSTERN VON GENEXPRESSIONEN

Unsupervised hierarchical
clustering analysis of miRNA
gene expression of 41
mammary tumors
Min Zhi et al., Genome Biology,
volume 12, R77 (2011)

2-29

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hierarchische Clusteranalyse
âˆ’ Hierarchisch agglomeratives Clustern

âˆ’ Partitionierende Clusteranalyse
âˆ’ k-means-Algorithmus
âˆ’ BFR-Algorithmus
âˆ’ CURE-Algorithmus

2-30

K-MEANS
âˆ’ Anzahl der Cluster k ist vorgegeben
âˆ’ Seien ğ¶1 , ğ¶2 , â€¦ , ğ¶ğ‘˜ Mengen von Datenpunkten mit
âˆ’ ğ¶1 âˆª ğ¶2 âˆª â‹¯ âˆª ğ¶ğ‘˜ = Menge aller Datenpunkte
âˆ’ ğ¶ğ‘– âˆ© ğ¶ğ‘— = âˆ… fÃ¼r alle ğ‘– â‰  ğ‘—.

âˆ’ k-Means-Clustering versucht ein Clustering ğ¶1 , ğ¶2 , â€¦ , ğ¶ğ‘˜ mit mÃ¶glichst
geringer durchschnittlicher Distanz innerhalb der Cluster zu finden:
âˆ’ minimiere Ïƒğ‘˜ğ‘—=1
ğ¶1 ,ğ¶2 ,â€¦,ğ¶ğ‘˜

1
Ïƒ â€² ğ‘‘(ğ‘¥ğ‘– , ğ‘¥ğ‘– â€² )
|ğ¶ğ‘— | ğ‘–,ğ‘– âˆˆğ¶ğ‘—

âˆ’ Sehr schwieriges Optimierungsproblem fÃ¼r groÃŸe Datenmengen
âˆ’ Der k-Means-Algorithmus approximiert dieses Ziel ziemlich gut
âˆ’ KomplexitÃ¤t: O(N)
2-31

K-MEANS-ALGORITHMUS
âˆ’ Gegeben einer initialen Wahl von k Centroiden
âˆ’ HinzufÃ¼gen aller Punkte zum Cluster mit
nÃ¤chstgelegenem Centroiden
âˆ’ Wiederholung bis Konvergenz (keine Ã„nderungen):
âˆ’ Neuberechnung der Centroiden
âˆ’ Zuordnung aller Punkte zum Cluster mit nÃ¤chstgelegenem
Centroiden

x

x
x

x
x
x

x

x

x x x

x â€¦ Datenpunkte
â€¦ Centroid

Cluster nach 1. Runde

2-32

K-MEANS-ALGORITHMUS
âˆ’ Gegeben einer Initialen Wahl von k Centroiden
âˆ’ HinzufÃ¼gen aller Punkte zum Cluster mit
nÃ¤chstgelegenem Centroiden
âˆ’ Wiederholung bis Konvergenz (keine Ã„nderungen):
âˆ’ Neuberechnung der Centroiden
âˆ’ Zuordnung aller Punkte zum Cluster mit nÃ¤chstgelegenem
Centroiden
x

x

x
x
x
x

x

x

x x x

x â€¦ Datenpunkte
â€¦ Centroid

Cluster nach 2. Runde

2-33

K-MEANS-ALGORITHMUS
âˆ’ Gegeben einer Initialen Wahl von k Centroiden
âˆ’ HinzufÃ¼gen aller Punkte zum Cluster mit
nÃ¤chstgelegenem Centroiden
âˆ’ Wiederholung bis Konvergenz (keine Ã„nderungen):
âˆ’ Neuberechnung der Centroiden
âˆ’ Zuordnung aller Punkte zum Cluster mit nÃ¤chstgelegenem
Centroiden
x

x
x
x
x
x

x

x

x â€¦ Datenpunkte
â€¦ Centroid

x x x

Cluster nach 3. Runde

2-34

INITIALISIERUNG: WAHL VON K CENTROIDEN
âˆ’ 1. MÃ¶glichkeit: Jeder Punkt wird zufÃ¤llig einem von k Clustern
zugeordnet und Berechnung der dazugehÃ¶rigen Centroiden
âˆ’ 2. MÃ¶glichkeit: Auswahl von k Punkten mit grÃ¶ÃŸtmÃ¶glichen
paarweisen Entfernungen
âˆ’ WÃ¤hle ersten Punkt zufÃ¤llig
âˆ’ Wiederhole: WÃ¤hle den Punkt mit der maximalen minimalen Distanz zu
allen schon gewÃ¤hlten Punkten

âˆ’ 3. MÃ¶glichkeit: Hierarchische Clusteranalyse auf Stichprobe (so dass
k Cluster entstehen) und Auswahl der jeweiligen Clustroiden

âˆ’ Ergebnis des Algorithmus hÃ¤ngt von der Initialisierung ab
âˆ’ Die durchschnittliche Distanz ist zwar minimal (Konvergenz), aber nur
lokales Minimum
âˆ’ Wiederholung mit verschiedenen Initialisierungen und Wahl des besten
Clustering
2-35

WAHL VON K
âˆ’ Ausprobieren verschiedener Werte k = 2, 4, 8, 16, 32, â€¦
âˆ’ Schrittweises ErhÃ¶hen von k solange â€sich etwas Relevantes Ã¤ndertâ€œ
âˆ’ Beispiel: durchschnittliche Distanz fÃ¤llt signifikant

Bester Wert fÃ¼r k
durchschnittliche
Distanz

k

âˆ’ BinÃ¤re Suche um Rechenaufwand gering zu halten
âˆ’ Annahme: Signifikante Ã„nderung von k = 8 zu k = 16, aber keine
signifikante Ã„nderungen von k = 16 zu k = 32 â†’ Setze k = 12
âˆ’ Falls signifikante Ã„nderungen von k = 12 zu k = 16, setze k = 14 , â€¦
âˆ’ Falls keine signifikante Ã„nderungen von k = 12 zu k = 16, setze k = 10 ,
â€¦
2-36

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hierarchische Clusteranalyse
âˆ’ Hierarchisch agglomeratives Clustern

âˆ’ Partitionierende Clusteranalyse
âˆ’ k-means-Algorithmus
âˆ’ BFR-Algorithmus
âˆ’ CURE-Algorithmus

2-37

BFR-ALGORITHMUS
âˆ’ BFR [Bradley-Fayyad-Reina] ist eine Variante
des k-Means-Algorithmus, welche die
Verarbeitung sehr umfangreicher DatensÃ¤tze
(die nicht in den Hauptspeicher passen) erlaubt
âˆ’ Annahme: Cluster sind (multivariat) normal
verteilt
âˆ’ um den Centroiden
âˆ’ mit stochastisch unabhÃ¤ngigen Dimensionen

âˆ’ Idee:
âˆ’ Da die Cluster jeweils einer Normalverteilung
folgen, mÃ¼ssen nicht mehr alle Punkte eines
Clusters explizit gespeichert werden
âˆ’ Es genÃ¼gen die Parameter der
Normalverteilungen:
Erwartungswert und Varianz

âˆ’ Ziel: Bestimmung der k Centroiden und
dazugehÃ¶rigen Varianzen

2-38

REPRÃ„SENTATION DER CLUSTER
âˆ’ Anzahl der Dimensionen: ğ‘‘
âˆ’ Effiziente Zusammenfassung eines Clusters mittels 2ğ‘‘ + 1 Zahlen
âˆ’ n: Anzahl der im Cluster enthaltenen Punkte
âˆ’ ğ‘†ğ‘ˆğ‘€1 , ğ‘†ğ‘ˆğ‘€2 , â€¦ , ğ‘†ğ‘ˆğ‘€ğ‘‘ die Summen der Komponenten dieser Punkte
âˆ’ ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„1 , ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„2 , â€¦ , ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„ğ‘‘ die Summen der Quadrate der Komponenten
dieser Punkte

âˆ’ HinzufÃ¼gen eines Punktes (ğ‘¥1 , ğ‘¥2 , â€¦, ğ‘¥ğ‘‘ )
âˆ’ n+1
âˆ’ ğ‘†ğ‘ˆğ‘€1 + ğ‘¥1 , ğ‘†ğ‘ˆğ‘€2 +ğ‘¥2 , â€¦ , ğ‘†ğ‘ˆğ‘€ğ‘‘ + ğ‘¥ğ‘‘
âˆ’ ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„1 + ğ‘¥12 , ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„2 + ğ‘¥22 , â€¦ , ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„ğ‘‘ + ğ‘¥ğ‘‘2

âˆ’ Centroid:

ğ‘†ğ‘ˆğ‘€1 ğ‘†ğ‘ˆğ‘€2
ğ‘†ğ‘ˆğ‘€ğ‘‘
,
,
â€¦
,
ğ‘›
ğ‘›
ğ‘›

âˆ’ Varianz:

ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„1 ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„2
ğ‘†ğ‘ˆğ‘€ğ‘†ğ‘„ğ‘‘
,
,â€¦,
ğ‘›
ğ‘›
ğ‘›

âˆ’

ğ‘†ğ‘ˆğ‘€1 ğ‘†ğ‘ˆğ‘€2
ğ‘†ğ‘ˆğ‘€ğ‘‘
,
,â€¦,
ğ‘›
ğ‘›
ğ‘›

2

2-39

DREI MENGEN
âˆ’ Discard set (DS): Punkte, die einem Cluster zugeordnet wurden
âˆ’ Retained set (RS): Punkte, die bisher keinem Cluster zugeordet wurden
âˆ’ Compression set (CS): Punkte aus RS, die nahe genug beieinander liegen,
um sie zusammenzufassen (Mini-Cluster)

Punkte
aus RS

Cluster (DS)

Komprimierte
Mengen (CS)

2-40

BFR-ALGORITHMUS
âˆ’ Initialisiere k Cluster, z.B. Clusteranalyse auf Stichprobe
âˆ’ Wiederhole:
âˆ’ Lade einen Chunk mit Punkten (FÃ¼lle Hauptspeicher mit Daten von
Festplatte)
âˆ’ HinzufÃ¼gen der Punkte zu den k vorhandenen Clustern (DS), falls deren
Distanz innerhalb eines Schwellenwerts liegen
âˆ’ Clusteranalyse auf Ã¼brigen Punkten, inkl. der Punkte aus RS
âˆ’ ZusammenfÃ¼hren der entstandenen â€Mini-Clusterâ€œ mit CS
âˆ’ z.B. ZusammenfÃ¼hren zweier Cluster, falls Varianz deren Kombination unter
einem Schwellenwert liegt
âˆ’ Manche Punkte bleiben einzeln und somit in RS

âˆ’ Evtl. ZusammenfÃ¼hren einiger â€Mini-Clusterâ€œ aus CS mit Clustern aus
DS.

âˆ’ Am Ende: HinzufÃ¼gen der Cluster aus CS und Punkte aus RS zu
nÃ¤chstliegenden Clustern aus DS
2-41

BFR-CLUSTER
âˆ’ Nach welchem Kriterium wird ein Punkt einem vorhandenen Cluster
hinzugefÃ¼gt?
âˆ’ Kriterium: Mahalanobis-Abstand zwischen Punkt und Centroid eines
Clusters ist minimal und liegt unter einem Schwellenwert
âˆ’ Punkt (ğ‘¥1 , ğ‘¥2 , â€¦, ğ‘¥ğ‘‘ )
âˆ’ Centroid (ğ‘1 , ğ‘2 , â€¦, ğ‘ğ‘‘ )
âˆ’ Standardabweichungen (ğœ1 , ğœ2 , â€¦, ğœğ‘‘ )
ğ‘‘

ğ‘€ ğ‘¥, ğ‘ =

ğ‘¥ğ‘– âˆ’ ğ‘ğ‘–
à·
ğœğ‘–

2

ğ‘–=1

âˆ’ FÃ¼r 68% der Punkte gilt: ğ‘€ ğ‘¥, ğ‘ < ğ‘‘
âˆ’ FÃ¼r 95% der Punkte gilt: ğ‘€ ğ‘¥, ğ‘ < 2 ğ‘‘
âˆ’ Wahl des Schwellenwerts, so dass Punkt mit hoher Wahrscheinlichkeit zu
Cluster gehÃ¶rt

2-42

VERGLEICH: EUKLIDISCH VS MAHALANOBIS
Konturlinien der Punkte mit gleichem Abstand zum Ursprung

Gleichverteilung,
Euklidische Distanz

Normalverteilung,
Euklidische Distanz

Normalverteilung,
Mahalanobis-Distanz

2-43

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hierarchische Clusteranalyse
âˆ’ Hierarchisch agglomeratives Clustern

âˆ’ Partitionierende Clusteranalyse
âˆ’ k-means-Algorithmus
âˆ’ BFR-Algorithmus
âˆ’ CURE-Algorithmus

2-44

CURE-ALGORITHMUS
âˆ’ Erweiterung von k-Means zu Clustern beliebiger Form
âˆ’ Probleme bei BFR:
âˆ’ Annahme der Normalverteilung
und unabhÃ¤ngige Dimensionen
âˆ’ Ellipsen sind mÃ¶glich, doch
keine rotierten Ellipsen

âˆ’ CURE (Clustering Using Representatives):
âˆ’ Cluster beliebiger Form mÃ¶glich
âˆ’ Cluster werden Ã¼ber eine
Menge reprÃ¤sentativer Punkte
beschrieben

2-45

CURE-ALGORITHMUS
âˆ’ Ziehe eine Zufallsstichprobe, die in den Hauptspeicher passt
âˆ’ Initialisierung: Hierarchische Clusteranalyse, wobei eine
agglomerative Methode ohne Centroid bevorzugt werden sollte
âˆ’ Auswahl von reprÃ¤sentativen Punkten fÃ¼r jedes Cluster: Innerhalb
eines Clusters sollten die Punkte mÃ¶glichst weit auseinander liegen
âˆ’ Verschiebung der reprÃ¤sentativen Punkte um einen bestimmten
Anteil (z.B. 20%) hin zu den jeweiligen Centroiden der Cluster
âˆ’ ZusammenfÃ¼hrung zweier Cluster, deren reprÃ¤sentative Punkte eine
geringe maximale paarweise Distanz aufweisen (Schwellenwert)
âˆ’ Durchlauf aller Punkte und Zuordnung zu Cluster mit geringstem
Abstand zu einem reprÃ¤sentativen Punkt

2-46

BEISPIEL
âˆ’ Ziehen einer Zufallsstichprobe & hierarchische
Clusteranalyse

2-47

BEISPIEL
âˆ’ Auswahl reprÃ¤sentativer Punkte & Verschiebung zu
Centroiden

2-48

BEISPIEL
âˆ’ ZusammenfÃ¼hrung zweier Cluster, deren reprÃ¤sentative
Punkte eine geringe Distanz aufweisen

2-49

ZUSAMMENFASSUNG
âˆ’ EinfÃ¼hrung
âˆ’ Hierarchische Clusteranalyse
âˆ’ Hierarchisch agglomeratives Clustern

âˆ’ Partitionierende Clusteranalyse
âˆ’ k-means-Algorithmus
âˆ’ BFR-Algorithmus
âˆ’ CURE-Algorithmus

2-50

