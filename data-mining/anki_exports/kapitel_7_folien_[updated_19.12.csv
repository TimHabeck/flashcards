"Was ist die Entropie einer Zielvariablen \(Y\) in Entscheidungsbäumen?","<blockquote>\(H(Y) = -\sum_{y} p_y \log_2 p_y\).</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie interpretiert man hohe bzw. niedrige Entropie von \(Y\)?","<ul><li><strong>Hohe Entropie</strong>: Y ist (nahezu) gleichverteilt.</li><li><strong>Niedrige Entropie</strong>: Y ist ungleich verteilt.</li></ul>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie ist die bedingte Entropie \(H(Y|X)\) definiert?","<blockquote>\(H(Y|X)=\sum_{x} P(X=x)\,H(Y|X=x)\).</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Nach welchem Kriterium wählt man in Entscheidungsbäumen den besten Split (nach Entropie)?","<blockquote>Man wählt den Split mit der <strong>kleinsten bedingten Entropie</strong> \(H(Y|X)\).</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie bestimmt man bei numerischen und kategorialen Attributen den Split im Entscheidungsbaum?","<ul><li><strong>Numerisch</strong>: Schwellenwert wählen.</li><li><strong>Kategorial</strong>: Kategorie(n) wählen.</li></ul>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Was ist das Ziel des Maximal-Margin-Classifiers (SVM) bei linear trennbaren Daten?","<ul><li>Maximiere den <strong>Margin</strong> zur Hyperebene.</li><li>Nebenbedingung: \(y\,(w_0 + \sum_i w_i x_i) \ge 1\) für alle Datenpunkte.</li></ul>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie hängt der Margin \(\gamma\) vom Gewichtsvektor \(w\) ab?","<ul><li>\(\gamma = \frac{1}{\lvert w' \rvert}\) mit \(w'=(w_1,\dots,w_n)\).</li><li>Maximiere \(\gamma\) \(\Leftrightarrow\) minimiere \(\sum_i w_i^2\).</li></ul>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie modelliert man bei nicht linear trennbaren Daten Strafterm/Fehlzuordnungen in SVMs?","<blockquote>Man minimiert \(\sum_i w_i^2 + C\cdot(\#\,\text{Fehlzuordnungen})\), oft mit Slacks \(\xi_j\).</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie beeinflusst der Parameter \(C\) den Trade-off bei nicht linear trennbaren Daten?","<ul><li><strong>Großes \(C\)</strong>: Trennung ist wichtiger (weniger Fehler).</li><li><strong>Kleines \(C\)</strong>: größerer Margin ist wichtiger.</li></ul>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie ist die Hinge-Loss-Slack-Variable \(\xi_j\) definiert?","<blockquote>\(\xi_j = \max\left(0, 1 - y_j\,(w_0 + \sum_i w_i x_{ji})\right)\).</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wann ist \(\xi_j = 0\), \(0<\xi_j\le 1\) und \(\xi_j>1\) bei der SVM?","<ul><li>\(\xi_j=0\): richtige Seite der Ränder.</li><li>\(0<\xi_j\le 1\): richtige Seite der Hyperebene, aber innerhalb des Margin.</li><li>\(\xi_j>1\): falsche Seite der Hyperebene.</li></ul>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Was sind Support-Vektoren im Support Vector Classifier?","<blockquote>Datenpunkte auf den <strong>Rändern</strong> oder auf der falschen Seite; nur sie beeinflussen die Klassifikation neuer Punkte.</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie lautet die Entscheidungsfunktion des Support Vector Classifiers in Form der Support-Vektoren?","<blockquote>\(f(x)=w_0 + \sum_{s\in S} \alpha_s\,(x \cdot x_s)\).</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Wie erweitert man den linearen Support Vector Classifier zu einer SVM für nichtlineare Trennung?","<blockquote>Kernel-Trick: \(f(x)=w_0 + \sum_{s\in S} \alpha_s\,K(x,x_s)\).</blockquote>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
"Welche Kernel-Beispiele wurden genannt (polynomial, radial/RBF)?","<ul><li><strong>Polynomial</strong>: \(K(x,x_s)=(1 + x\cdot x_s)^d\)</li><li><strong>RBF</strong>: \(K(x,x_s)=\exp\left(-r\sum_i (x_i-x_{si})^2\right)\)</li></ul>","week-7 ws-25-26 decision-trees svm supervised-learning data-mining source::kapitel-7---folien-[updated-19.12"
