DATA WAREHOUSING

Kapitel 6: Ãœberblick Data Mining/ML-Verfahren
Victor Christen
christen@informatik.uni-leipzig.de
UniversitÃ¤t Leipzig
Institut fÃ¼r Informatik
Abteilung Datenbanken

AGENDA
âˆ’ EinfÃ¼hrung Data Mining / maschinelles Lernen
âˆ’ KDD-Prozess
âˆ’ Anwendungsbeispiele

âˆ’ Assoziationsregeln / Warenkorbanalyse
âˆ’ Support und Konfidenz
âˆ’ A Priori-Algorithmus
âˆ’ Frequent Pattern (FP)-Trees

âˆ’ Clusteranalyse
âˆ’ k-Means-Algorithmus
âˆ’ Canopy Clustering

âˆ’ Klassifikation
âˆ’ Klassifikationsprozess
âˆ’ EntscheidungsbÃ¤ume
âˆ’ Neuronale Netze

Abteilung Datenbanken

DATA WAREHOUSING

6-2

KNOWLEDGE DISCOVERY IN DATABASES (KDD)
âˆ’ (semi-)automatische Extraktion von Wissen aus Datenbanken, das
âˆ’ gÃ¼ltig (im statistischen Sinn)
âˆ’ bisher unbekannt
âˆ’ und potenziell nÃ¼tzlich ist

âˆ’ Kombination von Verfahren zu Datenbanken, Statistik und KI
(maschinelles Lernen)
Selektion
Vorverarbeitung/
Transformation

Rohdaten

Data Mining

Daten
Interpretation

Daten
Muster
Abteilung Datenbanken

DATA WAREHOUSING

Wissen
6-3

DATA MINING / MACHINE LEARNING
âˆ’ Data Mining: Anwendung effizienter Algorithmen zur Erkennung von
Mustern in groÃŸen Datenmengen
âˆ’ Machine Learning: lernbasierte (KI-) AnsÃ¤tze fÃ¼r Vorhersagen /
Wissensgenerierung
âˆ’ Data Mining/ML fÃ¼r Big Data / Datenbanken bzw. Data Warehouses
âˆ’ Skalierbarkeit auf groÃŸe Datenmengen (nicht nur im Hauptspeicher)
âˆ’ parallele Realisierungen

âˆ’ umfassende Datenaufbereitung und Vorverarbeitung erforderlich
âˆ’ Diskretisierung numerischer Attribute (Aufteilung von Wertebereichen in
Intervalle, z.B. Altersgruppen)
âˆ’ Erzeugen abgeleiteter Attribute (z.B. Aggregationen fÃ¼r bestimmte
Dimensionen, UmsatzÃ¤nderungen)
âˆ’ Extraktion von analyserelevanten Merkmalen / Features aus
unstrukturierten Daten

Abteilung Datenbanken

DATA WAREHOUSING

6-4

ANALYSETECHNIKEN
âˆ’ Assoziationsregeln
âˆ’ Warenkorbanalyse (z.B. Kunde kauft A und B => Kunde kauft C)
âˆ’ Sonderformen zur BerÃ¼cksichtigung von Dimensionshierarchien (z.B.
Produktgruppen), quantitativen Attributen, zeitlichen Beziehungen (sequence
mining)

âˆ’ Clusteranalyse
âˆ’ Objekte werden aufgrund von Ã„hnlichkeiten in Klassen eingeteilt â€¢ â€¢
â€¢â€¢
(Segmentierung)

âˆ’ Klassifikation

â€¢

â€¢ â€¢ â€¢â€¢
â€¢ â€¢

â€¢

â€¢

âˆ’ Zuordnung von Objekten zu Gruppen/Klassen mit gemeinsamen Eigenschaften
bzw. Vorhersage von Attributwerten
âˆ’ Verwendung von Stichproben (Trainingsdaten)
âˆ’ AnsÃ¤tze: Entscheidungsbaum-Verfahren, neuronale Netze, statistische
Auswertungen (z.B. Maximum Likelihood-SchÃ¤tzung / Bayes-SchÃ¤tzer)

âˆ’ weitere AnsÃ¤tze:
âˆ’ genetische Algorithmen (multivariate Optimierungsprobleme, z.B. Identifikation
der besten Bankkunden)
âˆ’ Regressionsanalyse zur Vorhersage numerischer Attribute . . .
Abteilung Datenbanken

DATA WAREHOUSING

6-5

DATA MINING/ML: ANWENDUNGSBEISPIELE
âˆ’ Warenkorbanalyse: Produkt-Platzierung im Supermarkt,
Preisoptimierung, ...
âˆ’ Kundensegmentierung fÃ¼r Marketing
âˆ’ Gruppierung von Kunden mit Ã¤hnlichem Kaufverhalten / Ã¤hnlichen
Interessen
âˆ’ Nutzung fÃ¼r gruppenspezifische Empfehlungen, Product Bundling, ...

âˆ’ Bestimmung der KreditwÃ¼rdigkeit von Kunden
âˆ’ elektronische Vergabe von Kreditkarten
âˆ’ schnelle Entscheidung Ã¼ber VersicherungsantrÃ¤ge, ...

âˆ’ Entdeckung wechselbereiter Kunden
âˆ’ Entdeckung von Kreditkarten-Missbrauch
âˆ’ Erkennung von Spam-Emails
âˆ’ Auswahl personalisierter Behandlungsstrategien fÃ¼r Krebspatienten
â€¦
Abteilung Datenbanken

DATA WAREHOUSING

6-6

EVALUATION/INTERPRETATION
âˆ’ Ablauf
âˆ’ PrÃ¤sentation der gefundenen Muster, z.B. Ã¼ber Visualisierungen
âˆ’ Bewertung der Muster durch den Benutzer
âˆ’ falls schlechte Bewertung: erneute Analyse mit anderen
Parametern, anderem Verfahren oder anderen Daten
âˆ’ falls gute Bewertung: Integration des gefundenen Wissens in die
Wissensbasis / Metadaten und Nutzung fÃ¼r zukÃ¼nftige KDDProzesse

âˆ’ Bewertung der gefundenen Muster: Interessantheit,
Vorhersagekraft
âˆ’ sind Muster schon bekannt oder Ã¼berraschend?
âˆ’ wie gut lassen sich mit â€Trainingsdatenâ€œ (Stichproben) gefundene
Muster auf zukÃ¼nftige Daten verallgemeinern?
âˆ’ Vorhersagekraft wÃ¤chst mit GrÃ¶ÃŸe und ReprÃ¤sentativitÃ¤t der
Stichprobe
Abteilung Datenbanken

DATA WAREHOUSING

6-7

ASSOZIATIONSREGELN
âˆ’ Warenkorbanalyse auf Transaktions-Datenbank
âˆ’ Transaktion umfasst alle gemeinsam getÃ¤tigten EinkÃ¤ufe, innerhalb
eines Dokuments vorkommenden Worte, innerhalb einer Web-Sitzung
referenzierten Seiten, ...

âˆ’ Regeln der Form â€œRumpf â†’ Kopf [support, confidence]â€
âˆ’ Beispiele
âˆ’ kauft(â€œPapierâ€) â†’ kauft(â€œTonerâ€) [0.5%, 60%]
âˆ’ 80% aller Kunden, die Reifen und AutozubehÃ¶r kaufen,
bringen ihr Auto auch zum Service

âˆ’ relevante GrÃ¶ÃŸen
âˆ’ Support einer Regel X â†’ Y: Anteil der Transaktionen, in denen alle
Objekte X und Y vorkommen
âˆ’ Konfidenz einer Regel X â†’ Y: Anteil der Transaktionen
mit Rumpf-Objekten X, fÃ¼r die Regel erfÃ¼llt ist (d.h. fÃ¼r
die auch Objekte Y vorliegen)
âˆ’ Interessantheit: hoher Wahrscheinlichkeitsunterschied fÃ¼r
Y gegenÃ¼ber zufÃ¤lliger Verteilung
Abteilung Datenbanken

DATA WAREHOUSING

6-8

ASSOZIATIONSREGELN (2)
âˆ’ Aufgabe: Bestimmung aller Assoziationsregeln, deren
Support und Konfidenz Ã¼ber bestimmten Grenzwerten
liegen
âˆ’ Gegeben:
âˆ’ R eine Menge von Items/Objekten (z.B. Produkte, Webseiten)
âˆ’ t eine Transaktion, t ïƒ R
âˆ’ r eine Menge von Transaktionen
âˆ’ smin ïƒ [0,1] die minimale UnterstÃ¼tzung,
âˆ’ confmin ïƒ [0,1] die minimale Konfidenz

âˆ’ Aufgabe: Finde alle Regeln c:
X â†’ Y mit X ïƒ R, Y ïƒ R, X ïƒ‡ Y = {}
supp( r , c) =

ï» t ïƒ r X ïƒˆ Y ïƒ tï½
r

Abteilung Datenbanken

ï‚³ smin

ï» t ïƒ r X ïƒˆ Y ïƒ tï½
conf (r , c) =
ï‚³ conf
ï» t ïƒ r X ïƒ rt ï½

DATA WAREHOUSING

min

6-9

BEISPIEL: WARENKORBANALYSE

EinkaufsID

Aftershave

Bier

Chips

1

0

1

1

2

1

1

0

3

0

1

1

4

1

0

1

5

1

1

1

{Aftershave}
{Bier}
{Aftershave}
{Chips} â†’
{Aftershave}

â†’ {Bier}
â†’ {Chips}
â†’ {Chips}
{Aftershave}
â†’ {Bier,Chips}

Abteilung Datenbanken

s = 2/5, conf = 2/3
s= 3/5, conf = 3/4

s=

DATA WAREHOUSING

conf=

10

WEITERE BEISPIELE
âˆ’ Assoziationsregeln:
âˆ’ Aâ†’C
âˆ’

âˆ’ C â†’A

s=2/4=0,5
c=2/3

s= 0,5
c=

âˆ’ 5 WarenkÃ¶rbe:

TransaktionsID
2000
1000
4000
5000

Items
A,B,C
A,C
A,D
B,E,F

smin
= 0,5
minsup
= 50%,

minconf
= 50%
conf
min= 0,5

Assoziationsregel: PC â†’ Drucker
s=
c=

âˆ’ Drucker, Papier, PC, Toner
âˆ’ PC, Scanner
âˆ’ Drucker, Papier, Toner
âˆ’ Drucker, PC
âˆ’ Drucker, Papier, PC, Scanner, Toner
âˆ’ Abteilung Datenbanken
DATA WAREHOUSING

6-11

FREQUENT ITEMSETS
âˆ’ Frequent Item-Set (hÃ¤ufige Mengen):
âˆ’ Menge von Items/Objekten, deren Support Schranke smin Ã¼bersteigt
âˆ’ Bestimmung der Frequent-Itemsets wesentlicher Schritt zur
Bestimmung von Assoziationsregeln

âˆ’ effiziente Realisierung Ã¼ber A-Priori-Algorithmus
âˆ’ Nutzung der sog. A-Priori-Eigenschaft:
âˆ’ wenn eine Menge hÃ¤ufig ist, so auch all ihre Teilmengen (Anti-Monotonie)
âˆ’ wenn eine Menge selten ist, so auch all ihre Obermengen (Monotonie)

âˆ’ Support jeder Teilmenge und damit jedes einzelnen Items muss Ã¼ber
Schranke smin liegen
âˆ’ iterative Realisierung beginnend mit 1-elementigen Itemsets
âˆ’ schrittweise Auswertung von k-Itemsets (k Elemente, k >=1),
âˆ’ Ausklammern von Kombinationen mit Teilmengen, die Support smin nicht
erreichen (â€Pruningâ€œ)
âˆ’ wird â€A Prioriâ€œ getestet, bevor Support bestimmt wird

Abteilung Datenbanken

DATA WAREHOUSING

6-12

{A, B, C, D}
Wenn hÃ¤ufig

dann hÃ¤ufig

{A,B,C} {A,B,D} {B,C,D} {A,C,D}

k+1=3

{A,B} {A,C} {B,C} {B,D} {C,D} {A,D}

k=2

{A}

{B}

{C}
{}

Abteilung Datenbanken

DATA WAREHOUSING

{D}

HÃ¤ufige Mengen L k
ergeben Kandidaten
Ck+1

A-PRIORI-ALGORITHMUS
Apriori(R, r, smin, confmin)
L:= HÃ¤ufige-Mengen(R, r, smin)
c:= Regeln (L, confmin)
Return c
HÃ¤ufige-Mengen(R, r, smin)
k=1
L1:= {i ïƒ R, supp(i)>= smin}//hÃ¤ufige Items
while Lk ï‚¹ {}
Ck+1 := Erzeuge-Kandidaten(Lk)
Lk+1 := Prune(Ck+1, r) // eliminiere Itemsets, die smin nicht
erreichen
k:= k+1

k

Return ï• L
j
j =2

Abteilung Datenbanken

DATA WAREHOUSING

6-14

A-PRIORI-ALGORITHMUS (2)
Erzeuge-Kandidaten(Lk)
Ck+1 := {}
Forall l1, l2 in Lk,
so dass l1 = {i1,...,ik-1 , ik}
l2 = {i1,...,ik-1,iâ€˜k} ik < iâ€˜k
l := {i1, ..., ik-1 , ik, iâ€˜k}// sortierte Items pro
Itemset
if (alle k-elementigen Teilmengen von l in Lk) then
Ck+1 := Ck+1 ïƒˆ {l}

Return Ck+1
Beispiel:
L3 ={ABC, ABD, ACD, BCD, CDE}
L4 ={ABCD, BCDE, ACDE}
Abteilung Datenbanken

DATA WAREHOUSING

6-15

A-PRIORI-ALGORITHMUS: BEISPIEL
âˆ’ 5 PC-WarenkÃ¶rbe (Forderung: minimaler Support smin 60%)
âˆ’ Drucker, Papier, PC, Toner
âˆ’ PC, Scanner
âˆ’ Drucker, Papier, Toner
âˆ’ Drucker, PC
âˆ’ Drucker, Papier, PC, Scanner, Toner

âˆ’ Algorithmus-AusfÃ¼hrung
âˆ’ k=1: Drucker 4, Papier 3, PC 4, Toner 3, Scanner 2
âˆ’
L1= C1 = {Drucker, Papier, PC, Toner}
âˆ’ k=2: Drucker-Papier: 3, Drucker-PC: 3, Drucker-Toner: 3,
Papier-PC: 2, Papier-Toner: 3, PC-Toner: 2
âˆ’
L2= {Drucker-Papier, Drucker-PC, Drucker-Toner, Papier-Toner}
âˆ’ k=3: Drucker-Papier-PC? Drucker-Papier-Toner? Drucker-PC-Toner?
âˆ’ k=4:
Abteilung Datenbanken

DATA WAREHOUSING

6-16

REGELGENERIERUNG
âˆ’ Itemset I = {i1, ..., ik-1 , ik} erlaubt viele Regeln der
Form X â†’ Y, mit X ïƒˆ Y=I, X ïƒ‡ Y = { }
âˆ’ Konfidenz: conf (X â†’ Y)= supp (I) / supp (X)
âˆ’ Generierung der Assoziationsregeln aus Itemset I
âˆ’ wenn Konklusion (rechte Seite) lÃ¤nger und somit die linke Seite
kÃ¼rzer wird, kann Konfidenz sinken
âˆ’ die Ordnung der Attribute kann ausgenutzt werden
âˆ’ c1 = {i1, ..., ik-1 } â†’ { ik } conf 1
âˆ’ c2 = {i1, ... } â†’ {ik-1, ik } conf 2 ...
âˆ’ ck = {i1} â†’ {..., ik-1 , ik } conf k

âˆ’ Es gilt dann: conf 1 ï‚³ conf 2 ï‚³... ï‚³ conf k
âˆ’ Elimination aller Kombinationen, deren Konfidenz Minimalwert
unterschreitet
Abteilung Datenbanken

DATA WAREHOUSING

6-17

REGELGENERIERUNG: BEISPIEL
âˆ’ Regeln fÃ¼r ({Drucker, Papier, Toner}, minimaler Support
smin 60%
âˆ’ Bestimmung aller Regen mit minimaler Konfidenz conf min
75%
âˆ’ conf ({Drucker, Papier} â†’{Toner} ) =
supp(Dr.,Pap.,Toner)/supp(Dr.,Pap.)= 3/3=1
âˆ’ conf ({Drucker, Toner} â†’{Papier} ) = 3/3 = 1
âˆ’ conf ({Papier, Toner} â†’{Drucker} ) = 3/3= 1
âˆ’ conf ({Drucker} â†’{Papier, Toner} ) =
âˆ’ conf ({Papier} â†’{Drucker, Toner} ) =
âˆ’ conf ({Toner} â†’{Drucker, Papier} ) =

Abteilung Datenbanken

DATA WAREHOUSING

6-18

ASSOZIATIONSREGELN: WEITERE ASPEKTE
âˆ’ Nutzbarkeit u.a. fÃ¼r Cross-Selling, Produkt-Platzierung ...
âˆ’ Amazon: Kunden die dieses Buch gekauft haben, kauften auch ...

âˆ’ Sonderfall: Sequenzanalyse (Erkennung sequenzieller Muster)
âˆ’ BerÃ¼cksichtigung der Zeit-Dimension
âˆ’ Bsp. 1: in 30% der FÃ¤lle, wenn Produkt A gekauft wurde, wird bei einem
spÃ¤teren Besuch Produkt B gekauft
âˆ’ Bsp. 2: in 20% aller FÃ¤lle, in denen ein Nutzer Ã¼ber Werbung auf die
Web-Site gelangt und die Site vorher schon besucht hat, kauft er einen
Artikel (betrifft 10% der Sessions)

âˆ’ Probleme
âˆ’ sehr viele Produkte / Web-Seiten / Werbeaktionen / Besucher etc.
erfordern Bildung grÃ¶berer Bezugseinheiten
âˆ’ es kÃ¶nnen sinnlose Korrelationen ohne kausalen Zusammenhang
ermittelt werden
âˆ’ z.B. Schmutzeffekte aufgrund transitiver AbhÃ¤ngigkeiten (Bsp.: HaarlÃ¤nge
korreliert negativ mit KÃ¶rpergrÃ¶ÃŸe)
Abteilung Datenbanken

DATA WAREHOUSING

6-19

Abteilung Datenbanken

DATA WAREHOUSING

FREQUENT PATTERN-BAUM (FP-BAUM)
âˆ’ Probleme des A-Priori-Algorithmus
âˆ’ exponenziell wachsende Anzahl zu prÃ¼fender Kandidaten
âˆ’ Bsp.: 104 hÃ¤ufige Objekte;
> 107 2-Itemsets; ca. 1030 Kandidaten fÃ¼r 100-Itemsets

âˆ’ FP-Tree: Berechnung von Assoziationsregeln ohne
Kandidatengenerierung
âˆ’ komprimierte ReprÃ¤sentation aller Transaktionen durch
Frequent-Pattern Tree (FP-Baum)
âˆ’ effiziente Erkennung von Frequent Itemsets (Pattern Mining) mit
Divide-and-Conquer-Suche auf TeilbÃ¤umen
âˆ’ oft eine GrÃ¶ÃŸenordnung schneller als A-Priori

Abteilung Datenbanken

DATA WAREHOUSING

6-21

FP-BAUM: GENERIERUNG (1)
âˆ’ Input: Transaktionsdatenbank D, Schwellwert min-support smin
âˆ’ Erstellung Objekttabelle / FP-Baum
âˆ’ Scan von D, Erstellen der Menge F hÃ¤ufiger Objekte und ihrer
HÃ¤ufigkeiten, Ordnen von F in absteigender HÃ¤ufigkeit (Objekttabelle)

âˆ’ Wurzel des FP-Baums ist leere Menge
âˆ’ pro Transaktion in D: ordne Objekte gemÃ¤ÃŸ F (absteigende HÃ¤ufigkeit);
fÃ¼ge Pfad in FP-Baum ein (Knotenformat: Objekt-Id, ZÃ¤hler fÃ¼r
VerwendungshÃ¤ufigkeit in Transaktionen mit gleichem PrÃ¤fix)
âˆ’ Knoten mit gleicher Objekt-ID werden untereinander verkettet
(ausgehend von Objekttabelle F)
min_support = 0.5

TID
Objekte
e
10 Drucker, Papier, PC, Toner
20 PC, Scanner
30 Drucker, Papier, Toner
40 Drucker, PC
50 Drucker, Papier, PC,Scanner, Toner
Abteilung Datenbanken

DATA WAREHOUSING

Objekttabelle F

Objekt
Drucker
PC
Papier
Toner

count
4
4
3
3
6-22

FP-BAUM: GENERIERUNG (2)
gemÃ¤ÃŸ F-Ordnung
Drucker, PC, Papier, Toner
PC
Drucker, Papier, Toner
Drucker, PC
Drucker,

TID
Objekte
e
10 Drucker, Papier, PC, Toner
20 PC, Scanner
30 Drucker, Papier, Toner
40 Drucker, PC
50 Drucker, Papier, PC, Scanner, Toner

{}

min_support = 0.5
ObjektTabelle F

Objekt count Head
Drucker
4
PC
4
Papier
3
Toner
3

Drucker:1
PC:1
Papier:1
Toner:1

pro Transaktion gibt es Pfad, wobei gemeinsame PrÃ¤fixe nur einmal reprÃ¤sentiert
werden (Komprimierungseffekt)
Abteilung Datenbanken

DATA WAREHOUSING

6-23

FP-BAUM: ERKENNUNG HÃ„UFIGER MENGEN
âˆ’ Erkennung von Frequent Itemsets durch rekursive Suche
auf TeilbÃ¤umen (Divide and Conquer)
âˆ’ fÃ¼r jeden Knoten im FP-Baum erzeuge Musterbasis
(conditional pattern base)
âˆ’ gehe Objekt-Tabelle von unten (selten) nach oben durch. Die
Verweise fÃ¼hren zu den Pfaden, in denen das Objekt vorkommt.
âˆ’ das Objekt wird als Suffix betrachtet und alle PrÃ¤fixe davon als
Bedingungen fÃ¼r dieses Suffix. Die PrÃ¤fixpfade eines Suffixes
bilden seine Musterbasis

Abteilung Datenbanken

DATA WAREHOUSING

6-24

FP-BAUM: ERKENNUNG HÃ„UFIGER MENGEN
âˆ’ fÃ¼r jede Musterbasis erzeuge reduzierten FP-Baum
(conditional FP tree)
âˆ’ HÃ¤ufigkeiten der PrÃ¤fixe (fÃ¼r das betrachtete Suffix) werden von
unten nach oben propagiert.
âˆ’ Gleiche PrÃ¤fixpfade zu einem Suffix (vom Anfang bis zu einer
bestimmten Stelle) werden zusammengelegt und die
ursprÃ¼nglichen HÃ¤ufigkeiten addiert.
âˆ’ nur PrÃ¤fixpfade, die min_support erfÃ¼llen, verbleiben im
reduzierten FP-Baum

âˆ’ rekursives Ableiten von Frequent Itemsets aus
reduzierten FP-Trees
âˆ’ bei einzigem Pfad im Baum: AufzÃ¤hlen aller KnotenKombinationen

Abteilung Datenbanken

DATA WAREHOUSING

6-25

REDUZIERTE FP-BÃ„UME MIT EINEM PFAD
âˆ’ bei nur einem Pfad T kann weitere (rekursive)
Auswertung entfallen
âˆ’ Bestimmung der Frequent Itemsets durch AufzÃ¤hlung
aller Kombinationen von Knotern / Teil-Pfaden
{}
a:3
ïƒš

b:3
c:3

Frequent Itemsets
bezÃ¼glich d
d,
ad, bd, cd,
abd, acd, bcd,
abcd

reduzierter FP-Baum
fÃ¼r Suffix d (d-reduzierter FP-Baum)

Abteilung Datenbanken

DATA WAREHOUSING

6-26

ALGORITHMUS FP-GROWTH (TREE, I)
âˆ’Finden hÃ¤ufiger Muster durch sukzessives Wachstum von PatternFragmenten initialer Aufruf mit FP-Growth (FP-Baum, {})

Input: FP-Baum tree, Frequent Itemsets I
Output: vollstÃ¤ndige Menge hÃ¤ufiger Itemsets
if tree hat nur einen Pfad P then
foreach Kombination K von Knoten in P do
return IïƒˆK mit support = minimaler Support
der Items in K
else
foreach Item i in Tabelle F (in umgekehrter
HÃ¤ufigkeitsreihenfolge) do
K = Iïƒˆi mit support = Support von i;
Erstelle Musterbasis von K und reduzierten FPBaum FPK ;
if FPK nicht leer then FP-Growth (FPK ,K)end
Abteilung Datenbanken

DATA WAREHOUSING

6-27

SCHRITT 1: MUSTERBASIS-ERSTELLUNG
âˆ’ Gehe Objekt-Tabelle von unten (selten) nach oben durch. Die
Verweise fÃ¼hren zu den Pfaden, in denen das Objekt vorkommt.
âˆ’ Objekt wird als Suffix betrachtet und alle PrÃ¤fixe davon als
Bedingungen fÃ¼r dieses Suffix. Die transformierten PrÃ¤fixpfade eines
Suffixes bilden seine Musterbasis
{}
Objekt count Head
Drucker
4
PC
4
Papier
3
Toner
3
Objekt-Tabelle F

Drucker:4
PC:3
Papier:2

PC:1
Papier:1
Toner:1

Toner:2

Objekt

Musterbasis

Toner

Drucker,PC,Papier:2,
Drucker, Papier:1

Papier

Drucker, PC: 2
Drucker: 1

PC
Drucker

Abteilung Datenbanken

DATA WAREHOUSING

6-28

SCHRITT 2: ERZEUGE REDUZIERTE FP-BÃ„UME
âˆ’ propagiere Counts pro Objekt der Musterbasis von unten nach oben.
âˆ’ nur Knoten/PrÃ¤fix-Pfade, die min_support erfÃ¼llen, verbleiben im
reduzierten FP-Baum
Musterbasis fÃ¼r Toner

reduzierter FP-Baum fÃ¼r Toner

{}

{}

Drucker:2

Drucker:1

PC:2

Papier:1

Drucker:3

Frequent Itemsets:
Toner
Papier, Toner
Drucker, Toner
Drucker, Papier, Toner

Papier:3

Papier:2

reduzierter FP-Baum fÃ¼r Papier

{}
Musterbasis
fÃ¼r Papier

Drucker:2

Drucker:1

Papier
Drucker, Papier

Drucker:3

PC:2
Abteilung Datenbanken

{}

Frequent Itemsets:

DATA WAREHOUSING

6-29

ERGEBNISSE IM BEISPIEL
Objekt

Musterbasis

reduzierter FP-Baum

Toner

{(Drucker,PC,Papier:2,
Drucker, Papier:1}

{ (Drucker:3, Papier:3)} | Toner Toner
Papier, Toner
Drucker, Toner
Drucker, Papier, Toner

Papier

{(Drucker,PC:2,
Drucker:1}

{ (Drucker:3)} | Papier

Papier
Drucker, Papier

PC

{(Drucker:3) }

{ (Drucker:3)} | PC

PC
Drucker, PC

{}

{}

Drucker

Drucker

Abteilung Datenbanken

DATA WAREHOUSING

Frequent
Itemsets

6-30

ALTERNATIVBEISPIEL ZUR REKURSIVEN
AUSWERTUNG
âˆ’ reduzierter FP-Baum bezÃ¼glich Suffixobjekt e, minsupport=2
e-reduzierter FP-Baum

{}
a:8

{}

b:2

c:1

c:2
d:1

d:1
e:1

a:2
c:1

c:1
e:1

e:1

{}
a:2

d:1

d:1

Nutzung dieses Baums zur
Bestimmung der Frequent Itemsets fÃ¼r
Suffixe de, ce und ae

{}

c:1

a:2
d:1

d:1

-> Frequent Itemsets: de, ade

de-reduzierter FP-Baum

Pfade mit Suffix de
Abteilung Datenbanken

Quelle: Han/Kamber

DATA WAREHOUSING

6-31

EinfÃ¼hrung Data Mining / maschinelles Lernen
KDD-Prozess
Anwendungsbeispiele

Assoziationsregeln / Warenkorbanalyse
Support und Konfidenz
A Priori-Algorithmus
Frequent Pattern (FP)-Trees

Clusteranalyse
k-Means-Algorithmus
Canopy Clustering

Klassifikation
Klassifikationsprozess
EntscheidungsbÃ¤ume
Neuronale Netze
Abteilung Datenbanken

DATA WAREHOUSING

CLUSTERANALYSE
âˆ’ Ziele
âˆ’ automatische Identifikation einer endlichen Menge von
Kategorien, Klassen oder Gruppen (Cluster) in den
Daten
âˆ’ Objekte im gleichen Cluster sollen mÃ¶glichst Ã¤hnlich sein
dist ( x , y ) = ïƒ¥ ( x âˆ’ y )
âˆ’ Objekte aus verschiedenen Clustern sollen
mÃ¶glichst
unÃ¤hnlich zueinander sein
d

i

i

2

i =1

Abteilung Datenbanken

DATA WAREHOUSING

6-33

CLUSTERANALYSE
âˆ’ Ã„hnlichkeitsbestimmung
âˆ’ meist: Distanzfunktion dist(o1,o2) fÃ¼r Paare von Objekten
o1 und o2
d

( xi âˆ’ yi ) 2
âˆ’ z.B. Euklidische Distanz fÃ¼r numerische Attribute: dist ( x, y) = ïƒ¥
i =1

âˆ’ spezielle Funktionen fÃ¼r kategorische Attribute oder
Textdokumente

âˆ’ Clustering-AnsÃ¤tze: hierarchisch, partitionierend,
dichtebasiert, ...

Abteilung Datenbanken

DATA WAREHOUSING

6-34

CLUSTERANALYSE (2)
âˆ’ oft nicht 2, sondern 10 oder 10.000 Dimensionen
âˆ’ fast alle Objektpaare sind Ã¤hnlich weit voneinander entfernt
âˆ’ Methoden zur Dimensionsreduzierung erforderlich

âˆ’ Beispiel: Ã¤hnliche Musik-Alben
âˆ’ einige wenige Kategorien vs. kÃ¤uferspezifische PrÃ¤ferenzen
âˆ’ jeder Musikkunde bildet potenziell eigene Dimension
âˆ’ Ã¤hnliche Alben haben Ã¤hnliche Kunden und umgekehrt

âˆ’ Ã¤hnliche Text-Dokumente / News-Meldungen
âˆ’ Ã¤hnliche Mengen von Keywords

Abteilung Datenbanken

DATA WAREHOUSING

6-35

HIERARCHISCHES CLUSTERING
Top-Down (aufteilend)
âˆ’ beginne mit 1 Cluster
âˆ’ rekursives Splitten in Teil-Cluster
Agglomerativ (Bottom-Up-Ansatz)
âˆ’ zunÃ¤chst ist jeder Punkt eigenes Cluster
âˆ’ wiederholte Vereinigung der â€œÃ¤hnlichstenâ€ Cluster
âˆ’ Kernproblem: Bestimmung der Merge-Kandidaten
âˆ’ naiv: kubische KomplexitÃ¤t

Abteilung Datenbanken

DATA WAREHOUSING

6-36

HIERARCHISCHES CLUSTERING
âˆ’ ReprÃ¤sentation durch Dendogramme

1 2

3

4 5

Bestimmung von Cluster-ReprÃ¤sentanten
âˆ’ z.B. Cendroid (bei Euklidischer Distanz)
âˆ’ â€Clustroidâ€œ: ex. Punkt, der am nÃ¤chsten zu allen anderen
im Cluster liegt

Abteilung Datenbanken

DATA WAREHOUSING

6-37

K-MEANS ALGORITHMUS
Ausgangssituation
âˆ’ Objekte besitzen Distanzfunktion (meist Euklidische Distanz)
âˆ’ fÃ¼r jedes Cluster wird damit Clusterzentrum bestimmt (â€Mittelwertâ€œ)
âˆ’ Anzahl k der Cluster wird vorgegeben
Basis-Algorithmus
Schritt 1 (Initialisierung): k Clusterzentren werden (zufÃ¤llig) gewÃ¤hlt
Schritt 2 (Zuordnung): Jedes Objekt wird dem nÃ¤chst gelegenen
Clusterzentrum zugeordnet
Schritt 3 (Clusterzentren): FÃ¼r jedes Cluster wird Clusterzentrum neu
berechnet
Schritt 4 (Wiederholung): Abbruch, wenn sich Zuordnung nicht mehr
Ã¤ndert, sonst zu Schritt 2
Probleme: Konvergenz zu lokalem Minimum, d.h. Clustering muss nicht
optimal sein; relativ hoher Aufwand fÃ¼r Abstandsberechnungen
Abteilung Datenbanken

DATA WAREHOUSING

6-38

K-MEANS ALGORITHMUS: BEISPIEL
âˆ’ Clustering der Zahlen 1, 3, 6, 14, 17, 24, 26, 31 in 3
Cluster (k=3)
(1) Zentren: 10, 21, 29 (zufÃ¤llig gewÃ¤hlt, mÃ¼ssen nicht existieren)
(2) Cluster: C1={1, 3, 6, 14}, C2={17, 24}, C3={26, 31}
(3) Zentren (arithmetisches Mittel): C1: 6
C2: 20,5
C3: 28,5

(2) Cluster: C1={1, 3, 6}, C2={14, 17, 24}, C3={26, 31}
(3) Zentren: C1: 3,3
C2: 18,3
C3: 28,5
(2) Cluster: C1={1, 3, 6}, C2={14, 17}, C3={24, 26, 31}
(3) Zentren: C1: 3,3
C2: 15,5
C3: 27
(2) Cluster: C1={1, 3, 6}, C2={14, 17}, C3={24, 26, 31}
Abbruch, da sich das Clustering nicht mehr geÃ¤ndert hat.
Abteilung Datenbanken

DATA WAREHOUSING

6-39

K-MEANS: 2-D-BEISPIEL (K=3)

Abteilung Datenbanken

DATA WAREHOUSING

6-40

CANOPY CLUSTERING*
Bildung von Ã¼berlappenden Clustern (Canopies)
âˆ’ oft Nutzung als erster Schritt in mehrstufigem
Analyseverfahren (Pre-Clustering)
âˆ’ skalierbar auf sehr groÃŸe Datenmengen
âˆ’ auf String-Daten einsetzbar (mit StringÃ„hnlichkeits/Distanzfunktionen)
âˆ’ Clusteranzahl nicht vorzugeben
Eingabe
âˆ’ Punktmenge
âˆ’ Distanzfunktion
âˆ’ 2 Schwellwerte T1 und T2 (T1 > T2 )
T1

T2

* McCallum, A. et al.:Efficient clustering of high-dimensional data sets with application to reference matching Proc. ACM KDD, 2000
Abteilung Datenbanken

DATA WAREHOUSING

6-41

CANOPY CLUSTERING (2)
T2

Algorithmus
Schritt 1 (Initialisierung): Kandidatenliste fÃ¼r Wahl der
Canopy-Zentren wird mit allen Objekten initialisiert
Schritt 2 (Canopy-Zentrum): Zentrum Z wird (zufÃ¤llig)
aus Kandidatenliste gewÃ¤hlt
Schritt 3 (Zuordnung): alle Objekte, deren Abstand zu Z T1
geringer ist als Schwellwert T1, werden (Canopy) Z zugeordnet
Schritt 4 (Kandidatenliste): alle Objekte, die innerhalb des
Schwellwertes T2 zu Z liegen, werden aus Kandidatenliste gelÃ¶scht
Schritt 5 (Ende/Wiederholung): Abbruch, wenn Kandidatenliste leer ist,
sonst zu Schritt 2

Abteilung Datenbanken

DATA WAREHOUSING

6-42

CANOPY CLUSTERING ALGORITHMUS: BEISPIEL
Clustering der Zahlen 1, 2, 3, 8, 9, 15
Abstandsfunktion absolute Differenz, T1= 6, T2= 4
(1) Kandidatenliste = {1, 2, 3, 8, 9, 15}
(2) Canopyzentrum sei 1
(3) Canopy bilden
(4) Entferne
aus Kandidatenliste
(5) Kandidatenliste = {
(2) Canopyzentrum:
(3) Canopy bilden
(4) Entferne

}

aus Kandidatenliste

(5) Kandidatenliste = {
(2) Canopyzentrum:
(3) Canopy bilden
(4) Entferne
aus Kandidatenliste

}

(5) Abbruch, da Kandidatenliste leer
Abteilung Datenbanken

DATA WAREHOUSING

6-43

KLASSIFIKATION
Klassifikationsproblem
âˆ’ gegeben sei Stichprobe (Trainingsmenge) O von Objekten des Formats (a1,
. . ., ad) mit Attributen Ai, 1 ï‚£ i ï‚£ d, und KlassenzugehÃ¶rigkeit ci, ci ïƒ C = {c1 ,
. . ., ck}
âˆ’ gesucht: die KlassenzugehÃ¶rigkeit fÃ¼r Objekte aus D \ O
d.h. ein Klassifikator K : D â†’ C
weiteres Ziel:
âˆ’ Generierung (Lernen) des expliziten Klassifikationswissens
(Klassifikationsmodell, z.B. Klassifikationsregeln oder Entscheidungsbaum)
Abgrenzung zum Clustering
âˆ’ Klassifikation: Klassen vorab bekannt (Ã¼berwachte Verfahren, â€supervisedâ€œ)
âˆ’ Clustering: Klassen werden erst gesucht (â€unsupervisedâ€œ)
KlassifikationsansÃ¤tze
âˆ’ Entscheidungsbaum-Klassifikatoren, Bayes-Klassifikatoren (Auswertung der
bedingten Wahrscheinlichkeiten von Attributwerten)
âˆ’ neuronale Netze
Abteilung Datenbanken

DATA WAREHOUSING

6-44

KLASSIFIKATIONSPROZESS
Klassifikationsalgorithmus

1. Konstruktion des Klassifikationsmodells

Rank

Years

Tenured

Assistant Prof

3

No

Assistant Prof

7

Yes

Professor

2

Yes

Associate Prof

7

Yes

Assistant Prof

6

No

Associate Prof

3

No

2. Anwendung des Modells
zur Vorhersage (Prediction)

Unbekannte Daten

Klassifikator
Trainingsdaten

if rank = â€˜professorâ€™
or years > 6
then tenured = â€˜yesâ€™

Klassifikator

Tenured?

(Professor, 4)
Abteilung Datenbanken

DATA WAREHOUSING

6-45

ENTSCHEIDUNGSBÃ„UME
âˆ’ explizite, leicht verstÃ¤ndliche ReprÃ¤sentation des Klassifikationswissens
Autotyp
ID
1
2
3
4
5

â—¼

Alter
23
17
43
68
32

Autotyp
Familie
Sport
Sport
Familie
LKW

40

Familie

Risiko
hoch
hoch
hoch
niedrig
niedrig

= LKW

ï‚¹ LKW

Risikoklasse = niedrig

Alter
> 60

Risikoklasse = niedrig

<=60

Risikoklasse = hoch

Entscheidungsbaum ist Baum mit folgenden Eigenschaften:
â€“ ein innerer Knoten reprÃ¤sentiert ein Attribut
â€“ eine Kante reprÃ¤sentiert einen Test auf dem Attribut des VorgÃ¤ngerknotens
â€“ ein Blatt reprÃ¤sentiert eine der Klassen

â—¼

jeder Pfad von Wurzel zu Blatt entspricht einer Klassifikationsregel
â€“ Bsp: Autotyp ï‚¹ LKW AND Alter > 60 -> Risiko=niedrig

â—¼

Anwendung zur Vorhersage :

â€“ Top-Down-Durchlauf des Entscheidungsbaums von der Wurzel zu einem der BlÃ¤tter
â€“ eindeutige Zuordnung des Objekts zur Klasse des erreichten Blatts
Abteilung Datenbanken

DATA WAREHOUSING

6-46

KONSTRUKTION EINES ENTSCHEIDUNGSBAUMS
âˆ’ Basis-Algorithmus (divide and conquer)
âˆ’ Anfangs gehÃ¶ren alle TrainingsdatensÃ¤tze zur Wurzel
âˆ’ Auswahl des nÃ¤chsten Attributs (Split-Strategie): Maximierung des
Informationsgewinns (meÃŸbar Ã¼ber Entropie o.Ã¤.)
âˆ’ Partitionierung der TrainingsdatensÃ¤tze mit Split-Attribut
âˆ’ Verfahren wird rekursiv fÃ¼r die Partitionen fortgesetzt

âˆ’ Abbruchbedingungen
âˆ’ keine weiteren Split-Attribute
âˆ’ alle TrainingsdatensÃ¤tze eines Knotens gehÃ¶ren zur selben Klasse

âˆ’ Typen von Splits
âˆ’ kategorische Attribute: Split-Bedingungen der Form â€attribut = aâ€œ oder
â€attribut ïƒ setâ€œ (viele mÃ¶gliche Teilmengen)
âˆ’ numerische Attribute: Split-Bedingungen der Form â€attribut < aâ€œ (viele
mÃ¶gliche Split-Punkte)

Abteilung Datenbanken

DATA WAREHOUSING

6-47

NEURONALE NETZE
âˆ’ neuronales Netz (NN) besteht aus mehreren Schichten
âˆ’ Eingabe-/Ausgabeschicht
âˆ’ mind. einer verdeckten (hidden) Schicht

âˆ’ jede Schicht besteht aus Neuronen, die mit anderen
Neuronen verbunden sind
âˆ’ Verbindungen / Kanten verwenden Zahlen, z.B. Gewichte (ğ’˜iâˆˆâ„)

âˆ’ Deep Learning: mehrere hidden layers

Abteilung Datenbanken

DATA WAREHOUSING

6-48

DEEP LEARNING
âˆ’ Lernen einer DatenreprÃ¤sentation(Embeddings) auf groÃŸen Mengen
an Trainingsdaten
âˆ’ Nutzen des gelernten Wissens fÃ¼r Klassifikation, Vorhersagen etc.
vor allem fÃ¼r unstrukturierte Daten (Bilder, Sprache, Text) mit
verborgener Struktur
âˆ’ zahlreiche AnwendungsfÃ¤lle
âˆ’ Erkennung von Bildern
âˆ’ Erkennung von Handschriften
âˆ’ Spracherkennung
âˆ’ Verarbeitung von Texten â€¦

âˆ’ verschiedene Varianten von Netzen
âˆ’ Autoencoder networks (Erzeugung verbesserter ReprÃ¤sentationen)
âˆ’ Convolutional deep Neural Networks (CNN), v.a. fÃ¼r Bildklassifizierung
âˆ’ Recurrent Neural Networks (RNN), u.a. LSTM (Long short-term
memory),
âˆ’ u.a. fÃ¼r Text/Sprachverarbeitung oft unter Nutzung von Word Embeddings
Abteilung Datenbanken

DATA WAREHOUSING

6-49

BEWERTUNG VON KLASSIFIKATOREN
âˆ’ Klassifikator ist fÃ¼r die Trainingsdaten optimiert
âˆ’ liefert auf der Grundgesamtheit der Daten evtl. schlechtere Ergebnisse (->
Overfitting-Problem)

âˆ’ Bewertung mit von Trainingsmengen unabhÃ¤ngigen
Testmengen
âˆ’ Klassifikationsgenauigkeit/Accuracy:
âˆ’ Anteil der korrekten Klassenzuordnungen in Testmenge

âˆ’ Klassifikationsfehler:
âˆ’ Anteil der falschen
Klassenzuordnungen

Abteilung Datenbanken

DATA WAREHOUSING

6-50

BEWERTUNG (2)
Klassifikationsgenauigkeit (Accuracy)
âˆ’ n SÃ¤tze/Objekte, nr korrekt zugeordnet
ğ‘›ğ‘Ÿ
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =
ğ‘›
âˆ’ alle Kategorien gehen gleichberechtigt ein
âˆ’ bei 2 Kategorien (positive/negative) gilt
âˆ’ ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =

ğ‘‡ğ‘ƒ+ğ‘‡ğ‘
ğ‘›

âˆ’ Accuracy problematisch, falls eine der
Kategorien stark dominiert (z.B. nonMatches)

Abteilung Datenbanken

DATA WAREHOUSING

real:
class C

real:
not class
C

predicted True
:
Positive
class C
TP

False
Positive
FP

predicted False
:
Negative
not class FN
C

True
Negative
TN

6-51

BEWERTUNG (2)
Precision / Recall / F-Measure
âˆ’ fokussieren auf eine der Kategorien C (z.B. Risiko,
Krankheit, Match) in binÃ¤ren Entscheidungsproblemen
âˆ’ seien nc der n Objekte von dieser Kategorie
âˆ’ Klassifikationsmodell ordne mc Objekte dieser Klasse zu,
davon mcr (TP) richtig
âˆ’ ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =

ğ‘šğ‘ğ‘Ÿ
ğ‘›ğ‘

ğ‘šğ‘ğ‘Ÿ

ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =
ğ‘šğ‘
2 âˆ™ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› âˆ™ ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ¹ âˆ’ ğ‘€ğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ =
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› + ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™

âˆ’ Beispiel binÃ¤res Entscheidungsproblem
âˆ’ n=100, nc = 20, mc = 25, mcr = 15
âˆ’ Recall=
âˆ’ Precision=
âˆ’ Accuracy=
Abteilung Datenbanken

DATA WAREHOUSING

F-Measure=

6-52

BEWERTUNG (3)
âˆ’ Konfusionsmatrix
A

B

C

Summe

a

37

12

4

53

b

5

23

11

39

c

12

8

42

62

Summ
e

54

43

57

154

âˆ’ Accuracy: (37+23+42)/154=102/154 = 66,2%
âˆ’ kategoriebezogene Bewertungen
âˆ’ Recall fÃ¼r Kat. C:
42/57=0,74
âˆ’ Precision fÃ¼r Kat. C: 42/62=0,68

Abteilung Datenbanken

DATA WAREHOUSING

6-53

ZUSAMMENFASSUNG (1)
âˆ’ Data Mining/ML-Verfahrensklassen: Assoziationsregeln,
Clusteranalyse, Klassifikation,
âˆ’ zahlreiche NutzungsmÃ¶glichkeiten: Kundensegmentierung,
Vorhersage des Kundenverhaltens, Warenkorbanalyse etc.
âˆ’ Assoziationsregeln, u.a. zur Warenkorbanalyse
âˆ’ Berechnung der Frequent Itemsets mit minimalem Support
âˆ’ Ableitung der Regeln mit ausreichender Konfidenz
âˆ’ Nutzung der Anti-Monotonie: Itemset ist nur hÃ¤ufig, wenn es alle
Teilmengen davon sind
âˆ’ A-Priori-Algorithmus: Bottom-Up-Auswertung mit
Kandidatengenerierung fÃ¼r hÃ¤ufige Itemsets
âˆ’ FP-Baum: schnellere Alternative mit kompakter ReprÃ¤sentation von
Transaktionen und rekursivem Pattern Mining auf TeilbÃ¤umen (Divideand-Conquer)

Abteilung Datenbanken

DATA WAREHOUSING

6-54

ZUSAMMENFASSUNG (2)
âˆ’ Clusteranalyse: Segmentierung Ã¼ber DistanzmaÃŸ
âˆ’ K-Means: vorgegebene Anzahl von Clustern
âˆ’ Canopy Clustering: schnelle Berechnung Ã¼berlappender Cluster;
nÃ¼tzlich als Vorbereitung fÃ¼r K-Means

âˆ’ Klassifikation, z.B. Ã¼ber EntscheidungsbÃ¤ume
âˆ’ erfordert Trainingsdaten mit bekannter Klassenzuordnung
âˆ’ Bestimmung der Split-Attribute eines Entscheidungsbaums gemÃ¤ÃŸ
Informationsgewinn

âˆ’ Bewertung von Klassifikationsergebnissen
âˆ’ Accuracy
âˆ’ fÃ¼r 2 Klassenprobleme: Precision/Recall/F-Measure

âˆ’ neuronale Netze / Deep Learning
âˆ’ vielseitige NutzungsmÃ¶glichkeiten, u.a. fÃ¼r Klassifikationsprobleme fÃ¼r
unstrukturierte Daten (Bilder, Texte)
âˆ’ hoher Ressourcenbedarf
Abteilung Datenbanken

DATA WAREHOUSING

6-55

