DATA MINING

Kapitel 3: Dimensionreduktion
Dr. Christian Martin
Wintersemester 2025/26

Abteilung Datenbanken / ScaDS.AI
UniversitÃ¤t Leipzig

THEMENÃœBERSICHT
Hochdimensionale Daten

Graphdaten

DatenstrÃ¶me

Clustering

Dimensionsreduktion

Community
Detection

Windowing

Empfehlungssysteme

Assoziationsregeln

PageRank

Filtern

Locality Sensitive
Hashing

Supervised ML

Web Spam

Momente

3-2

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hauptkomponentenanalyse
âˆ’ SingulÃ¤rwertzerlegung
âˆ’ t-SNE - Algorithmus

3-3

DIMENSIONSREDUKTION
âˆ’ Aufdeckung versteckter Korrelationen
âˆ’ Entfernung redundanter und verrauschter Merkmale
âˆ’ Leichtere Interpretation und Visualisierung
âˆ’ Schnellere Speicherung und Verarbeitung der Daten

3-4

DIMENSIONSREDUKTION

ğ‘›=2
ğ‘‘=1

A

ğ‘›=3
ğ‘‘=2

âˆ’ Idee: Falls die Datenpunkte eines n-dimensionalen Raumes in der
NÃ¤he eines d-dimensionalen Unterraums liegen, Reduzierung der
Punkte auf deren Projektionen im Unterraum
Anstatt 2 Koordinaten, wird jeder
Punkt nur Ã¼ber eine Koordinate
reprÃ¤sentieren: Position auf der
roten Linie

Punkt A wird anstatt Ã¼ber 3
Koordinaten (z.B. [3,4,2]) nur Ã¼ber 2
Koordinaten (z.B. [2,1])
reprÃ¤sentiert

3-5

DIMENSIONSREDUKTION

A
ğ‘›=2
ğ‘‘=1

ğ‘›=3
ğ‘‘=2

âˆ’ Ziel: Aufdeckung der Datenachsen
âˆ’ Die Achsen des Unterraums bezeichnet man auch als Faktoren
âˆ’ Wahl der Faktoren:
âˆ’ Erster Faktor zeigt in die Richtung, in welcher die Punkte ihre grÃ¶ÃŸte
Streuung aufweisen
âˆ’ Zweiter Faktor ist orthogonal zum ersten Faktor und zeigt in die
Richtung mit der grÃ¶ÃŸten Streuung unter den Punkten
âˆ’ usw..

3-6

DIMENSIONSREDUKTION
âˆ’ Einfaches Beispiel:
Kunde

Montag

Dienstag

Mittwoch

Donnerstag

Freitag

ReprÃ¤sentation

A

1

1

1

0

0

[1, 0]

B

2

2

2

0

0

[2, 0]

C

1

1

1

0

0

[1, 0]

D

5

5

5

0

0

[5, 0]

E

0

0

0

2

2

[0, 2]

F

0

0

0

3

3

[0, 3]

âˆ’ Rang einer Matrix: Anzahl der linear unabhÃ¤ngigen Zeilen/Spalten
âˆ’ Im Beispiel: Zeilen-/Spaltenrang ist 2
Neue Achsen/Faktoren
1
2
1
âˆ’ Aufspaltung:
5
0
0

1
2
1
5
0
0

1
2
1
5
0
0

0
0
0
0
2
3

0
1
0
2
0
1
=
0
5
2
0
3
0

0
0
0 1 1
âˆ™
0 0 0
2
3

1 0 0
0 1 1
Neue Koordinaten

3-7

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hauptkomponentenanalyse
âˆ’ SingulÃ¤rwertzerlegung
âˆ’ t-SNE - Algorithmus

3-8

HAUPTKOMPONENTENANALYSE
âˆ’ alias
âˆ’ Hauptkomponententransformation
âˆ’ Principal Component Analysis (PCA)
âˆ’ Verfahren der multivariaten Statistik und Linearen Algebra
âˆ’ Idee: Daten sind normalverteilt, aber in einem rotierten
Koordinatensystem
âˆ’ beste lineare Approximation
âˆ’ Ziel
âˆ’ Suche der Hauptkomponenten einer
Datenverteilung
âˆ’ Projektion der Datenpunkte auf
Hauptkomponenten
âˆ’ Jeder Datenpunkt wird durch
Linearkombination von mÃ¶glichst wenigen
Hauptkomponenten approximiert.
Wikipedia, CC BY 4.0

3-9

HAUPTKOMPONENTENANALYSE
âˆ’ Sei A eine m x n Matrix mit m Zeilen (Datenpunkte) und n Spalten
(Dimensionen) und ğ´âˆ™1 , ğ´âˆ™2 , â€¦ , ğ´âˆ™ğ‘› die Spalten von A
âˆ’ Vorgehen:
1. Translation
2. Rotation
3. Projektion

1. Tranlation
âˆ’ Die Punkte werden so verschoben, dass Mittelpunkt im Ursprung
liegt, d.h. die Spalten werden zentriert, so dass
1 ğ‘š
Ïƒ ğ´ = 0 fÃ¼r alle ğ‘— = 1, . . , ğ‘›
ğ‘š ğ‘–=1 ğ‘–ğ‘—

3-10

HAUPTKOMPONENTENANALYSE

2. Rotation
âˆ’ Bildung der Kovarianzmatrix M von A: M = ğ´ğ‘‡ ğ´
âˆ’ M hat die GrÃ¶ÃŸe n x n (Anzahl der Dimensionen)
âˆ’ Bestimmung der Eigenvektoren ğ‘’1 , â€¦ ğ‘’ğ‘› und Eigenwerte ğœ†1 , â€¦ ğœ†ğ‘› von
M mit
ğ‘€ğ‘’ = ğœ†ğ‘’
âˆ’ Eigenschaften:
âˆ’ Die Eigenvektoren ğ‘’1 , â€¦ ğ‘’ğ‘› werden auch Ladungsvektoren
Ï†1 , â€¦ Ï†n genannt
âˆ’ Es entstehen n Eigenvektoren und n Eigenwerte
âˆ’ Der 1. Eigenvektor zeigt in die Richtung der Hauptdatenverteilung
(bildet die maximale Varianz ab), der 2. Eigenvektor in die
zweitgrÃ¶ÃŸte Datenrichtung (senkrecht zum 1. Eigenvektor), usw.
3-11

HAUPTKOMPONENTENANALYSE
âˆ’ Weitere Eigenschaften:
âˆ’ Alle Eigenvektoren stehen senkrecht aufeinander.
Damit bilden die Eigenvektoren ein neues Koordinatensystem
âˆ’ Sortierung nach GrÃ¶ÃŸe der Eigenwerte
âˆ’ Die Eigenvektoren haben die LÃ¤nge 1:
2
Ïƒğ‘›ğ‘—=1 ğœ‘ğ‘—ğ‘
= 1 fÃ¼r alle q = 1, . . , n
âˆ’ Die Eigenvektoren bilden eine Rotationsmatrix
âˆ’ Die Eigenwerte geben die Varianz entlang des jeweiligen
Eigenvektors an.
âˆ’ Die Parameter ğœ‘11 , ğœ‘21 , â€¦ , ğœ‘ğ‘›1 werden auch als Ladungen der
ersten Hauptkomponente bezeichnet

3-12

HAUPTKOMPONENTENANALYSE
3. Projektion
âˆ’ Projektion der Datenpunkte auf die neuen Hauptachsen
âˆ’ Die erste Hauptkomponente der Projektion von A ist gegeben durch
den Vektor
ğ‘§âˆ™1 = ğœ‘11 ğ´âˆ™1 + ğœ‘21 ğ´âˆ™2 + â‹¯ + ğœ‘ğ‘›1 ğ´âˆ™ğ‘› (Linearkombination)
2
âˆ’ mit Ïƒğ‘›ğ‘—=1 ğœ‘ğ‘—1
= 1 und
âˆ’ maximaler empirischer Varianz

1 ğ‘š
2
Ïƒğ‘–=1 ğ‘§ğ‘–1
ğ‘š

âˆ’ Werden weniger Hauptachsen verwendet als es ursprÃ¼nglich
Dimensionen gab, kommt es zur Dimensionsreduktion

3-13

PCA: BEISPIEL (1)

ğ´=

1
2
3
4

2
1
4
3

3-14

PCA: BEISPIEL (2)

ğ´=

1
2
3
4

2
1
4
3

Bestimmung des Mittelpunktes
3-15

PCA: BEISPIEL (3)

ğ´=

1
2
3
4

2
1
4
3

ğ´â€² =

âˆ’1.5
âˆ’0.5
0.5
1.5

âˆ’0.5
âˆ’1.5
1.5
0.5

Translation des Datensatzes
3-16

PCA: BEISPIEL (4)

(1,1)

ğ‹ğŸ

ğ‘‘=

(âˆ’1)2 +(âˆ’1)2 = 2

(-1,-1)

ğ‹ğŸ =

1/ 2
1/ 2

Berechnung des 1. Eigenvektors
3-17

PCA: BEISPIEL (5)

ğ‹ğŸ =

1/ 2
1/ 2

ğ‘§1 =

âˆ’ 2
âˆ’ 2
2
2

âˆ’ 2

ğ‹ğŸ

âˆ’ Rotation des Koordinatensystems
âˆ’ Projektion der Datenpunkte auf neue Achse, d.h.
Berechnung der neuen Koordinate ğ‘§i fÃ¼r die 4 Punkte

2

3-18

PCA: BERECHNUNG
âˆ’ Der Ladungsvektor ğ‹ğŸ ist der Eigenvektor des grÃ¶ÃŸten Eigenwerts
der Matrix ğ´ğ‘‡ ğ´ (Kovarianzmatrix von A)
âˆ’ Beispiel: ğ´ğ‘‡ ğ´ =

1
2

2 3
1 4

4
3

1
2
3
4

2
1
4
3

=

30
28

28
30

âˆ’ Sei ğ‘€ eine quadratische Matrix. Eine reelle Zahl ğœ† heiÃŸt Eigenwert
von ğ‘€ und ein Vektor ğ‘’ â‰  0 der dazugehÃ¶rige Eigenvektor, falls
ğ‘€ğ‘’ = ğœ†ğ‘’
âˆ’ Beispiel:

1
30
28

1

28
2
2
âˆ™
= 58
1
1
30
2
2
3-19

EIGENWERTE UND -VEKTOREN
âˆ’ Berechnung der Eigenvektoren und â€“werte einer Matrix M z.B. Ã¼ber die
Power-Iteration-Methode:
âˆ’ Zu Beginn: beliebiger Vektor ğ’™0 â‰  0
âˆ’ Iteration: ğ’™ğ‘˜+1 =

ğ‘€ğ’™ğ’Œ
, wobei || â€¦ || die euklidische Norm | v | =
| ğ‘€ğ’™ğ’Œ |

Î£ğ‘– ğ‘£ğ‘–2

âˆ’ Stopp, falls Ã„nderungen in ğ’™ğ‘˜ vernachlÃ¤ssigbar klein

âˆ’ Beispiel: M =

30
28

30 28
28 30
0.707
âˆ’ ğ‘¥1 =
0.707
30 30
âˆ’ ğ‘€ğ‘¥1 =
28 28
âˆ’ ğ‘€ğ‘¥0 =

âˆ’ ğ‘¥2 =

28
1
und ğ‘¥0 =
30
1
1
58
=
und ğ‘€ğ‘¥0 = 582 + 582 = 82.02
1
58

0.707
41.01
=
und ğ‘€ğ‘¥1 = 57.997
0.707
41.01

1/ 2
0.707
â‰ˆ
0.707
1/ 2
3-20

EIGENWERTE UND -VEKTOREN
âˆ’ Die Power-Iteration-Methode berechnet den ersten Eigenvektor ğ’™
(mit dem grÃ¶ÃŸten Eigenwert)
âˆ’ DazugehÃ¶riger Eigenwert: ï¬ = ğ’™T ğ‘€ğ’™
âˆ’ Beispiel:
0.707 ğ‘‡ 30 28 0.707
â‰ˆ 58
0.707 28 30 0.707
âˆ’ Reduzierung der Matrix ğ‘€ um den Anteil, der durch den ersten
Eigenwert und â€“vektor generiert/erklÃ¤rt wird:
ğ‘€âˆ— : = ğ‘€ â€“ ğœ† ğ‘¥ ğ‘¥ ğ‘‡
âˆ’ Power-Iteration-Method auf ğ‘€âˆ— berechnet den zweiten Eigenvektor
von ğ‘€ (mit dem zweitgrÃ¶ÃŸten Eigenwert von ğ‘€)
âˆ’ Analoges Vorgehen fÃ¼r die Berechnung weiterer Eigenvektoren/werte
3-21

PCA: ZWEITE KOMPONENTE
âˆ’ Der zweite Eigenvektor von ğ´ğ‘‡ ğ´ entspricht dem Ladungsvektor ğ‹2
der zweiten Hauptkomponente von A
âˆ’ Die zweite Komponente der Projektion von A ist gegeben durch den
Vektor
âˆ’ ğ‘§âˆ™2 = ğœ‘12 ğ´âˆ™1 + ğœ‘22 ğ´âˆ™2 + â‹¯ + ğœ‘ğ‘›2 ğ´âˆ™ğ‘›
1
2
2
mit Ïƒğ‘›ğ‘—=1 ğœ‘ğ‘—2
= 1 und maximaler empirischer Varianz Ïƒğ‘š
ğ‘–=1 ğ‘§ğ‘–2
ğ‘š

âˆ’ AuÃŸerdem muss gelten, dass ğ‘§âˆ™1 und ğ‘§âˆ™2 unkorreliert sind, d.h.
Ïƒğ‘š
ğ‘–=1 ğ‘§ğ‘–1 âˆ™ ğ‘§ğ‘–2 = 0
âˆ’ Beispiel:
1
ğ‹ğŸ
(âˆ’ 2 ,

âˆ’ ğ‘§2 =

1/ 2
âˆ’1/ 2
1/ 2
âˆ’1/âˆš2

âˆ’
und ğ‹2 =

1
2

1
2

2

(âˆ’ 2 , âˆ’

)

1
2

( 2,

1

ğ‹ğŸ
)

( 2 ,âˆ’

2

)

1
2

)

3-22

PCA: HINWEISE
âˆ’ Spalten sollten gleiche Skalierung haben: neben Zentrierung des
Mittelwertes auch gleiche Standardabweichung
âˆ’ Proportion Variance Explained (PVE): Anteil der Varianz, welcher
durch die k-te Hauptkomponenten erklÃ¤rt wird:
2
Ïƒğ‘š
ğ‘
ğ‘–=1 ğ‘–ğ‘˜
ğ‘ƒğ‘‰ğ¸ğ‘˜ = ğ‘›
21
Ïƒğ‘—=1 Ïƒğ‘š
ğ´
ğ‘–=1 ğ‘–ğ‘—
âˆ’ Kumulierter Anteil der erklÃ¤rten Varianz:
à· ğ‘ƒğ‘‰ğ¸ğ‘˜
ğ‘˜

âˆ’ WÃ¤hle die Anzahl der Hauptkomponenten
so, dass der kumulierte Anteil der erklÃ¤rten
Varianz durch eine weitere Komponente
nicht stark ansteigt
âˆ’ Ziel: Mit mÃ¶glichst wenigen Komponenten
mÃ¶glichst viel Varianz erklÃ¤ren

4
1
2
3
Anzahl Komponenten
3-23

PCA: BEISPIEL
Rekonstruktion eines Bildes Ã¼ber die wichtigsten Hauptkomponenten

800 x 633
Pixel
0.914 0.914 0.914 0.910 â€¦
0.929 0.929 0.925 0.918 â€¦
0.910 0.910 0.902 0.894 â€¦
0.906 0.902 0.898 0.894 â€¦
0.898 0.894 0.890 0.866 â€¦
â€¦

â€¦

â€¦

â€¦

Quelle: https://kieranhealy.org/blog/archives/2019/10/27/reconstructing-images-using-pca/

â€¦
3-24

PCA: BEISPIEL
Rekonstruktion eines Bildes Ã¼ber die wichtigsten Hauptkomponenten

800 x 633
Pixel
0.914 0.914 0.914 0.910 â€¦
0.929 0.929 0.925 0.918 â€¦
0.910 0.910 0.902 0.894 â€¦
0.906 0.902 0.898 0.894 â€¦
Jede Zeile ist ein Datenpunkt:
Datensatz mit 633 Datenpunkten mit
jeweils 800 Werten
â†’ 633 Eigenvektoren der Dimension 800

0.898 0.894 0.890 0.866 â€¦
â€¦

â€¦

â€¦

â€¦

Quelle: https://kieranhealy.org/blog/archives/2019/10/27/reconstructing-images-using-pca/

â€¦
3-25

PCA: BEISPIEL
ğ‘˜

1

2

3

4

5

6

7

8

9

à· ğ‘ƒğ‘‰ğ¸ğ‘˜

0.37

0.56

0.65

0.70

0.73

0.76

0.79

0.81

0.83

âˆ’ Hauptkomponententanalyse auf Spalten der Pixel-Matrix:
ğ‘˜

Quelle: https://kieranhealy.org/blog/archives/2019/10/27/reconstructing-images-using-pca/
3-26

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hauptkomponentenanalyse
âˆ’ SingulÃ¤rwertzerlegung
âˆ’ t-SNE - Algorithmus

3-27

SINGULÃ„RWERTZERLEGUNG
âˆ’ alias Singular Value Decomposition (SVD)
âˆ’ Verallgemeinerung der PCA
âˆ’ ErmÃ¶glicht eine Zerlegung der Spalten bzw. Zeilen einer allgemeinen
(auch rechteckigen) m x n Datenmatrix A

âˆ’ PCA mit k Hauptkomponenten:
âˆ’ Mit ğ’ âˆ¶= ğ‘âˆ™1 ğ‘âˆ™2 â€¦ ğ‘âˆ™ğ‘˜ , und
âˆ’ ğ‹ âˆ¶= ğ‹ğŸ ğ‹ğŸ â€¦ ğ‹ğ’Œ
âˆ’ gilt:

ğ’ ğ’Ã—ğ’Œ = ğ‘¨[ğ’Ã—ğ’] âˆ™ ğ‹ ğ’Ã—ğ’Œ

3-28

SINGULÃ„RWERTZERLEGUNG
âˆ’ Es existiert eine Zerlegung einer Matrix A in das Produkt dreier Matrizen:

r

n

ï‚´
m

A

=

n

r
ï“

ï‚´

VT

r

m U
m: Anzahl der Datenpunkte
n: Anzahl Dimensionen

ğ‘¨[ğ’Ã—ğ’] = ğ‘ˆ ğ‘šÃ—ğ‘Ÿ âˆ™ ï“ ğ‘ŸÃ—ğ‘Ÿ âˆ™ ğ‘‰ ğ‘›Ã—ğ‘Ÿ

ğ‘‡

âˆ’ r: Rang von A (Anzahl der Faktoren)
âˆ’ ï“: Diagonalmatrix mit nicht-negativen EintrÃ¤gen (SingulÃ¤rwerte)
âˆ’ die Spalten von U und V sind orthonormal
(d.h. ğ‘ˆ ğ‘‡ âˆ™ ğ‘ˆ = ğ¼ und ğ‘‰ ğ‘‡ âˆ™ ğ‘‰ = ğ¼)
3-29

Jill
Jenny
Jane

Amelie

Jack

Casablanca

John

Serenity

Jim

Alien

Joe

Matrix

BEISPIEL

1
3
4
5
0
0
0

1
3
4
5
2
0
1

1
3
4
5
0
0
0

0
0
0
0
4
5
2

0
0
0
0
4
5
2

m = 7 (Datenpunkte = Personen)
n = 5 (Dimensionen = Filme)

Bewertung von 0 bis 5

3-30

6

5

Dimension 2

BEISPIEL
â€¢ Matrix V gibt die Faktoren
â€¢ Abbildung zeigt 2-dimensionale Projektion
der Datenpunkte

4
3
2

v1

1

0

1
3
4
5
0
0
0

1
3
4
5
2
0
1

1
3
4
5
0
0
0

0
0
0
0
4
5
2

0
0.13 0.02 -0.01
0
0.41 0.07 -0.03
0
0.55 0.09 -0.04
0 = 0.68 0.11 -0.05
4
0.15 -0.59 0.65
5
0.07 -0.73 -0.67
2
0.08 -0.29 0.32

0

2

4

6

Dimension 1

x
v1

12.4 0 0
0
9.5 0
0
0 1.3

x

0.56 0.59 0.56 0.09 0.09
0.12 -0.02 0.12 -0.69 -0.69
0.40 -0.80 0.40 0.09 0.09
3-31

Dimension 2

BEISPIEL

0.13 0.02 -0.01
0.41 0.07 -0.03
0.55 0.09 -0.04
0.68 0.11 -0.05
0.15 -0.59 0.65
0.07 -0.73 -0.67
0.07 -0.29 0.32

10
9
8
7
6
5
4
3
2
1
0

0

2

4

6

8

Dimension 1

x

12.4 0 0
0
9.5 0
0
0 1.3

Projektionen auf die Achse
des ersten Faktors

=

1.61
5.08
6.82
8.43
1.86
0.86
0.86

0.19 -0.01
0.66 -0.03
0.85 -0.05
1.04 -0.06
-5.60 0.84
-6.93 -0.87
-2.75 0.41
3-32

U ist Nutzer-FaktorMatrix

Alien

Serenity

Casablanca

Amelie

Nutzer

Matrix
Romantik

SciFi

BEISPIEL: INTERPRETATION

1
3
4
5
0
0
0

1
3
4
5
2
0
1

1
3
4
5
0
0
0

0
0
0
0
4
5
2

0
0.13 0.02 -0.01
0
0.41 0.07 -0.03
0
0.55 0.09 -0.04
0 = 0.68 0.11 -0.05
4
0.15 -0.59 0.65
5
0.07 -0.73 -0.67
2
0.07 -0.29 0.32

SciFi-Faktor

V ist Film-Faktor-Matrix

Romantik-Faktor

StÃ¤rke des SciFi-Faktors

x

12.4 0 0
0
9.5 0
0
0 1.3

x

0.56 0.59 0.56 0.09 0.09
0.12 -0.02 0.12 -0.69 -0.69
0.40 -0.80 0.40 0.09 0.09
3-33

SVD: DIMENSIONSREDUKTION
Reduktion der Dimensionen, falls Spaltenrang r kleiner als n
ZusÃ¤tzliche Reduktion: Setze kleinste SingulÃ¤rwerte auf Null
1
3
4
5
0
0
0

1
3
4
5
2
0
1

1
3
4
5
0
0
0

0
0
0
0
4
5
2

0
0.13 0.02 -0.01
0
0.41 0.07 -0.03
0
0.55 0.09 -0.04
0 â‰ˆ 0.68 0.11 -0.05
4
0.15 -0.59 0.65
5
0.07 -0.73 -0.67
2
0.07 -0.29 0.32

x

Rang-2-Approximation von A (je grÃ¶ÃŸer der Rang,
desto genauer die Approximation)

12.4 0 0
0
9.5 0
0
0 0

x

0.56 0.59 0.56 0.09 0.09
0.12 -0.02 0.12 -0.69 -0.69
0.40 -0.80 0.40 0.09 0.09
3-34

SVD: DIMENSIONSREDUKTION
1
3
4
5
0
0
0
ğ´

B

1
3
4
5
2
0
1

1
3
4
5
0
0
0

0
0
0
0
4
5
2

0
0.13 0.02 -0.01
0
0.41 0.07 -0.03
0
0.55 0.09 -0.04
0 â‰ˆ 0.68 0.11 -0.05
4
0.15 -0.59 0.65
5
0.07 -0.73 -0.67
2
0.07 -0.29 0.32

x

0.92 0.95 0.92 0.01 0.01
2.91 3.01 2.91 -0.01 -0.01
3.90 4.04 3.90 0.01 0.01
= 4.82 5.00 4.82 0.03 0.03
0.70 0.53 0.70 4.11 4.11
-0.69 1.34 -0.69 4.78 4.78
0.32 0.23 0.32 2.01 2.01

12.4 0 0
0
9.5 0
0
0 0

x

0.56 0.59 0.56 0.09 0.09
0.12 -0.02 0.12 -0.69 -0.69
0.40 -0.80 0.40 0.09 0.09

Genauigkeit Ã¼ber
Frobeniusnorm:

Çğ´ âˆ’ ğµÇğ¹

=

Î£ğ‘–ğ‘— (ğ´ğ‘–ğ‘— âˆ’ ğµğ‘–ğ‘— )2
3-35

SVD: DIMENSIONSREDUKTION
âˆ’ Satz: Sei ğ‘˜ mit 0 â‰¤ ğ‘˜ â‰¤ ğ‘Ÿ die Anzahl der gewÃ¼nschten Faktoren,
ğ‘¨ = ğ‘¼ ğ›´ ğ‘½ğ‘‡ und ğ‘© = ğ‘¼ ğ‘º ğ‘½ğ‘‡ wobei ğ‘º aus ï“ konstruiert wurde, indem
die letzten ğ‘Ÿ âˆ’ ğ‘˜ Diagonalelemente auf Null gesetzt wurden. Dann
gilt:
B = argminÇğ´ âˆ’ ğµÇğ¹
B

âˆ’ FÃ¼r eine gegebene Anzahl an Faktoren k minimiert SVD den Fehler
Çğ´ âˆ’ ğµÇğ¹ , so dass B die beste Rang-k-Approximation fÃ¼r A darstellt
âˆ’ Wie klein sollte man k wÃ¤hlen?
âˆ’ Behalte 80-90% der â€Energieâ€œ Ïƒğ‘– ğœğ‘–2 (Summe Ã¼ber die quadrierten
Diagonalelemente von ï“)
âˆ’ Beispiel: SingulÃ¤rwerte 12.4, 9.5, und 1.3 â†’ Energie: 245.7
âˆ’ Entfernen des letzten SingulÃ¤rwertes setzt Energie auf 244 (99%)
âˆ’ Mit nur dem grÃ¶ÃŸten SingulÃ¤rwert wÃ¤re die Energie auf 63% reduziert
3-36

SVD: BERECHNUNG
âˆ’ SVD fÃ¼r eine Matrix ğ‘¨ = ğ‘¼ï“ğ‘½ğ‘»
âˆ’ Es gilt: ğ´ = ğ‘ˆï“ğ‘‰
T

T T

= ğ‘‰

T T T

ï“ ğ‘ˆ T = ğ‘‰ï“ğ‘ˆ T

âˆ’ Regel fÃ¼r die Transponierte eines Produkts von Matrizen
âˆ’ Zweifache Transposition lÃ¶st sich auf
âˆ’ Transposition einer Diagonalmatrix ergibt die selbe Diagonalmatrix

âˆ’ Somit gilt: ğ‘¨T ğ‘¨ = ğ‘½ï“ğ‘¼T ğ‘¼ï“ğ‘½T = ğ‘½ï“2 ğ‘½T
âˆ’ Da Spalten von U orthonormal: ğ‘ˆ T ğ‘ˆ = I (IdentitÃ¤tsmatrix)
âˆ’ ï“2 ist eine Diagonalmatrix deren i-tes Diagonalelement das Quadrat des
i-ten Diagonalelements von ï“ ist

âˆ’ Da auch die Spalten von V orthonormal:
ğ‘¨T ğ‘¨ğ‘½ = ğ‘½ï“2 ğ‘½T ğ‘½ = ğ‘½ï“2 = ï“2 ğ‘½
D.h. V ist die Matrix aus Eigenvektoren von ğ‘¨T ğ‘¨ und die Diagonalelemente
von ï“2 sind die dazugehÃ¶rigen Eigenwerte.

âˆ’ Analog: ğ‘¨ğ‘¨T ğ‘¼ = ğ‘¼ï“2 = ï“2 ğ‘¼

3-37

INHALTSVERZEICHNIS
âˆ’ EinfÃ¼hrung
âˆ’ Hauptkomponentenanalyse
âˆ’ SingulÃ¤rwertzerlegung
âˆ’ t-SNE - Algorithmus

3-38

t-SNE - Algorithmus
âˆ’ t-distributed Stochastic Neighbor Embedding (t-SNA)
âˆ’ Nichtlineares Verfahren zur Dimensionsreduktion
âˆ’ UnÃ¼berwachtes Lernverfahren
âˆ’ L. van der Maaten and G. Hinton (2008)
âˆ’ Idee:
âˆ’ Jeder Punkt aus dem hochdimensionalem Raum wird auf einen Punkt
im niedrigdimensionalem Raum projektiert.
âˆ’ Bei der Projektion in einen niedrig dimensionalen Raum sollen die
AbstÃ¤nde zwischen den Punkte so gut wie mÃ¶glich erhalten bleiben.
âˆ’ Zur Visualisierung findet hÃ¤ufig eine Projektion in den 2- oder 3dimensionalen Raum statt

3-39

t-SNE - Grundlagen
Distanzmetrik
âˆ’ Definiert den Abstand zwischen zwei Punkten
âˆ’ HÃ¤ufig verwendet werden das euklidische AbstandsmaÃŸ, aber auch
die Manhatten-Metrik oder die Cosinus-Ã„hnlichkeit sind mÃ¶glich

Wahrscheinlichkeitsverteilung (WK-Verteilung)
âˆ’ Die AbstÃ¤nde werden in Wahrscheinlichkeiten Ã¼berfÃ¼hrt
âˆ’ FÃ¼r jeden Datenpunkt ğ‘¥ğ‘– wird die bedingte Wahrscheinlich zu jedem
anderen Datenpunkt ğ‘¥ğ‘— ermittelt, dass der Punkt ğ‘¥ğ‘– den Punkt ğ‘¥ğ‘— als
Nachbarn wÃ¤hlen wÃ¼rde
â†’ Stochastic Neighbor Embedding (SNE)
âˆ’ FÃ¼r jeden Datenpunkt ğ‘¥ğ‘– summieren sich die bedingten
Wahrscheinlichkeiten zu 1 auf.
3-40

t-SNE - Beispiel

âˆ’ Hohe WK gibt an, dass Punkte nahe beieinander liegen
âˆ’ Niedrige WK gibt an, dass Punkte weit auseinander liegen
âˆ’ Originalpaper: WK von Punkt zu sich selbst wird als 0 definiert
3-41

t-SNE â€“ Berechnung der bedingten WK
2

ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘—
ğ‘’ğ‘¥ğ‘ âˆ’
2
2ğœ
ğ‘(ğ‘—|ğ‘–) =
ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘˜ 2
Ïƒkâ‰ ğ‘– ğ‘’ğ‘¥ğ‘ âˆ’
2ğœ 2

âˆ’ ğ‘¥ğ‘– , ğ‘¥ğ‘— : Datenpunkte im hochdimensionalen Raum
âˆ’ ğ‘(ğ‘—|ğ‘–) : WK, dass der Punkt ğ‘¥ğ‘– den Punkt ğ‘¥ğ‘— als direkten Nachbarn
wÃ¤hlen wÃ¼rde
âˆ’

ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘— : Distanz zwischen den Punkten ğ‘¥ğ‘– und ğ‘¥ğ‘—

3-42

t-SNE â€“ Berechnung der bedingten WK
2

ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘—
ğ‘’ğ‘¥ğ‘ âˆ’
2
2ğœ
ğ‘(ğ‘—|ğ‘–) =
ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘˜ 2
Ïƒkâ‰ ğ‘– ğ‘’ğ‘¥ğ‘ âˆ’
2ğœ 2
ğ‘¥ğ‘– âˆ’ğ‘¥ğ‘—

2

âˆ’ ğ‘’ğ‘¥ğ‘ âˆ’
: Die GauÃŸâ€˜sche Kernelfunktion wandelt die
2ğœ2
Distanzen in Wahrscheinlichkeiten um
âˆ’ Ïƒkâ‰ ğ‘– ğ‘’ğ‘¥ğ‘ âˆ’

ğ‘¥ğ‘– âˆ’ğ‘¥ğ‘˜ 2
2ğœ2

: Normalisierungsterm

âˆ’ Summe aller bedingten Wken zwischen dem Punkt ğ‘¥ğ‘– und allen anderen
Datenpunkten im Datensatz
âˆ’ durch Normalisierungsterm wird Summe der bedingten Wken 1

âˆ’ Die bedingten WKen mÃ¼ssen nicht gleich sein: ğ‘(ğ‘—|ğ‘–) â‰  ğ‘(ğ‘–|ğ‘—)
â†’ Normalisierung:

ğ‘ğ‘–ğ‘— =

ğ‘(ğ‘–|ğ‘—) +ğ‘(ğ‘—|ğ‘–)
2ğ‘›

3-43

t-SNE - Wahrscheinlichkeitsverteilungen
âˆ’ im hochdimensionalem Raum: GauÃŸâ€˜sche Normalverteilung
âˆ’ Im niedrigdimensionalem Raum: t-Verteilung
âˆ’ Die RÃ¤nder liegen etwas hÃ¶her (â€heavy tailsâ€œ)
â†’ weiter entfernte Punkte kÃ¶nnen noch ausreichend unterschieden werden

3-44

t-SNE â€“ Berechnung der bedingten WK (t-Verteilung)

ğ‘ğ‘–ğ‘— =

1 + ğ‘¦ğ‘– âˆ’ ğ‘¦ğ‘—

2 âˆ’1

Ïƒğ‘˜ Ïƒkâ‰ ğ‘™ 1 + ğ‘¦ğ‘˜ âˆ’ ğ‘¦ğ‘™ 2 âˆ’1

âˆ’ ğ‘ğ‘–ğ‘— : Ã„hnlichkeit zwischen den Punkten i und j im
niedrigdimensionalem Raum
âˆ’

1 + ğ‘¦ğ‘– âˆ’ ğ‘¦ğ‘—

2 âˆ’1

: Ã„hnlichkeit der 2 Punkte (ohne Normalisierung)

âˆ’ Ïƒğ‘˜ Ïƒkâ‰ ğ‘™ 1 + ğ‘¦ğ‘˜ âˆ’ ğ‘¦ğ‘™ 2 âˆ’1 : Normalisierungsterm (Summe Ã¼ber alle
Paare)
âˆ’ Ziel: GauÃŸâ€˜sche Verteilung im hochdimensionalem Raum in tVerteilung im niedrigdimensionalem Raum Ã¼berfÃ¼hren
âˆ’ Durch t-Verteilung wird das sog. â€crowding problemâ€œ minimiert.
Die Punkte wÃ¼rden sonst zu dicht um den Mittelpunkt herum plaziert. 3-45

t-SNE - Optimierung
âˆ’ Ziel: GauÃŸâ€˜sche Verteilung im hochdimensionalem Raum in t-Verteilung
im niedrigdimensionalem Raum Ã¼berfÃ¼hren
âˆ’ Dazu wird die Kullback-Leibler (KL) Divergenz berechnet. Diese gibt an, wie
weit die approximierende WK-Funktion Q von der wahren WK-Funktion P
abweicht, bzw. wie weit ğ‘ğ‘–ğ‘— von den wahren ğ‘ğ‘–ğ‘— abweicht.
ğ¶ = ğ·ğ¾ğ¿ (ğ‘ƒÔ¡ğ‘„) = à· ğ‘ƒ ğ‘¥ ğ‘™ğ‘œğ‘”
ğ‘¥âˆˆğ‘‹

ğ‘ƒ(ğ‘¥)
ğ‘„(ğ‘¥)

ğ‘ğ‘–ğ‘—
ğ¶ = à· à· ğ‘ğ‘–ğ‘— ğ‘™ğ‘œğ‘”
ğ‘ğ‘–ğ‘—
ğ‘–

ğ‘—

âˆ’ Durch Bildung des Gradienten der Fehlerfunktion kann der Fehler bzw. die
Divergenz schrittweise minimiert werden.
ğœ•ğ¶
= 4 Ïƒğ‘— ğ‘ğ‘–ğ‘— âˆ’ ğ‘ğ‘–ğ‘—
ğœ•ğ‘¦ğ‘–

ğ‘¦ğ‘– âˆ’ ğ‘¦ğ‘—

1 + ğ‘¦ğ‘– âˆ’ ğ‘¦ğ‘—

2 âˆ’1

âˆ’ Man erhÃ¤lt keine festen Koordinaten im niedrigdimensionalem Raum, sondern
die Datenpunkte ğ‘¦ğ‘– werden verschoben bis Divergenz minimal ist (lokales
3-46
Minimum)

t-SNE - Hyperparameter
âˆ’ AbstandsmaÃŸ
âˆ’ Anzahl der Dimensionen des niedrigdimensionalen Raum
âˆ’ PerplexitÃ¤t: Gibt an, wie viele Nachbarn bei jedem Punkt berÃ¼cksichtigt
werden sollen
âˆ’ Initialisierung des Punkte im niedrigdimensionalen Raum: zufÃ¤llig
âˆ’ Lernrate: fÃ¼r das Gradientenabstiegsverfahren
âˆ’ Stopp-Kriterium

3-47

t-SNE - Anwendung
âˆ’ MNIST Datensatz: Datensatz mit handgeschriebenen Ziffern

âˆ’ Jede Ziffer ist ein 28x28 Pixel groÃŸes Graustufen-Bild
âˆ’ Bild wird interpretiert als Merkmalsvektor der GrÃ¶ÃŸe 28x28 = 784
â†’ 784 dimensionaler Vektorraum
âˆ’ Trainingsdatensatz: 60.000 Datenpunkte (Ziffern)
âˆ’ Testdatensatz: 10.000 Datenpunkte (Ziffern)
3-48

t-SNE - Anwendung

Visualisierung mit Sammon mapping

Visualisierung mit t-SNE

Visualisierung mit IsoMap

3-49

t-SNE - Eigenschaften
âˆ’ Verfahren gut geeignet fÃ¼r Visualisierung
âˆ’ t-SNE ist nicht-deterministisch
âˆ’ Auch bei gleichen Hyperparametern kann das Ergebnis unterschiedlich
ausfallen
âˆ’ Insbesondere kÃ¶nnen die Ergebnisse rotiert sein (wegen zufÃ¤lliger
Initialisierung)

âˆ’ Keine allgemeine Embedding-Funktion
âˆ’ Die neuen Achsen haben keine Interpretation.
âˆ’ Die WK-Verteilung ist nur fÃ¼r den aktuellen Datensatz gÃ¼ltig.

âˆ’ Lokale Strukturen werden gut erfasst, globale oft nicht so gut.

3-50

t-SNE â€“ Vergleich zu PCA
âˆ’ PCA: lineare Methode zur Dimensionsreduktion, die auf der
Kovarianzmatrix beruht
âˆ’ T-SNE: Nicht-linearer Ansatz mit dem Ziel, die AbstÃ¤nde im Originalund Zielraum mÃ¶glichst genau bezubehalten
âˆ’ LinearitÃ¤t
âˆ’ PCA kann nur lineare ZusammenhÃ¤nge erfassen

âˆ’ Interpretierbarkeit
âˆ’ PCA liefert bessere Interpretierbarkeit: Daten werden lediglich rotiert, die
einzelnen Komponenten sind Linearkombination der ursprÃ¼nglichen
Merkmale

âˆ’ Rechenzeit
âˆ’ Effiziente Methoden fÃ¼r PCA
âˆ’ T-SNE ist komplexes, iteratives Optimierungsverfahren

âˆ’ Hyperparameter
âˆ’ PCA benÃ¶tigt keine Hyperparameter, t-SNE schon
3-51

t-SNE â€“ Vergleich zu PCA
âˆ’ Determinismus
âˆ’ PCA ist deterministisch (das Ergebnis ist immer dasselbe)
âˆ’ t-SNE ist nicht deterministisch, Ergebnis ist abhÃ¤ngig von Initialisierung und
DurchfÃ¼hrung der Optimierung

âˆ’ Anwendung auf neue Daten
âˆ’ Bei PCA kÃ¶nnen die gelernten Achsen auf neue Daten angewendet werden
âˆ’ Bei t-SNE schwieriger, da lediglich ein WK-Verteilung gelernt wurde

âˆ’ DimensionalitÃ¤t
âˆ’ t-SNE lÃ¶st Crowding â€“ Problem, dies hat Vorteile, wenn Daten auf nur 2
oder 3 Dimensionen (z.B. zur Visualisierung) reduziert werden
âˆ’ bei mehr Dimensionen keine Vorteile fÃ¼r t-SNE

âˆ’ Anwendungsbereiche
âˆ’ PCA: dauerhafte Umwandlung in niedrige Dimension und Einsatz auch auf
neuen Daten, Einsatz z.B. bei der Merkmalsextraktion oder
Datenkompression
âˆ’ t-SNE: explorative Datenanalyse, Visualisierung

3-52

t-SNE - Quellen
âˆ’ L. van der Maaten, G. Hinton. Visualizing Data using t-SNE. Journal of Machine
Learning Research 9(86):2579-2605, 2008
âˆ’ https://mbrenndoerfer.com/writing/tsne-dimensionality-reduction-visualization
âˆ’ https://databasecamp.de/statistik/tsne
âˆ’ https://medium.com/data-science/t-sne-clearly-explained-d84c537f53a

3-53

DIMENSIONSREDUKTION - VERFAHREN
Principal Component
Analysis (PCA)
Reduziert die
Dimensionen durch
lineare Transformation.
Identifiziert
Hauptkomponenten (PC),
die die meiste Varianz in
den Daten erklÃ¤ren.

Linear Discriminant
Analysis (LDA)

SingulÃ¤rwertzerlegung
(SVD)
Zerlegt Matrix in das
Produkt von drei Matrizen,
um die wichtigste
Informationen zu
extrahieren.
Robust und numerisch
stabil, geeignet fÃ¼r nichtquadratische Matrizen.

Autoencoder (AE)

Wie PCS, aber
nutztzusÃ¤tzlich
Klasseninformationen zur
Dimensionsreduktion.

Neuronale Netzwerke, die
lernen, Eingabedaten in
eine niedrigdimensionale
Darstellung zu
komprimieren.

Optimiert die Trennung
zwischen verschiedenen
Klassen in den Daten.

Besonders nÃ¼tzlich fÃ¼r
nichtlineare
Datenstrukturen.

t-Distributed Stochastic
Neighbor Embedding
(t-SNE)
Effektiv fÃ¼r die
Visualisierung
hochdimensionaler Daten.

Uniform Manifold
Approximation and
Projection (UMAP)
Schnelle und skalierbare
Methode zur
Dimensionsreduktion.

ErhÃ¤lt lokale Strukturen
und zeigt Cluster in den
Daten.

Beibehaltung sowohl
lokaler als auch globaler
Strukturen der Daten.

Random Projection (RP)

Feature Selection

Reduziert die
Dimensionen durch
zufÃ¤llige Projektion der
Daten in einen
niedrigerdimensionalen
Raum.
Beibehaltung der Struktur
der Daten mit hoher
Wahrscheinlichkeit.

Auswahl der
relevantesten Merkmale
aus den Daten zur
Reduzierung der
Dimensionen.
Methoden wie Filter,
Wrapper und eingebettete
Verfahren werden
verwendet.

3-54

