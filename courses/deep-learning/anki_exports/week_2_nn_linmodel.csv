"What are the four primary components of an artificial neuron's forward equation?","<ul><li><strong>Inputs</strong> (\(z_i\))</li><li><strong>Weights</strong> (\(w_{ij}\))</li><li><strong>Bias</strong> (\(b_j\))</li><li><strong>Activation function</strong> (\(g\))</li></ul>","week-2 winter-2025-2026 neural-networks source::week-2-slides-l4-nn-linmodel"
"What are two critical requirements for an activation function to be effective in deep learning?","<ol><li>It must introduce <strong>non-linearity</strong>.</li><li>It must be <strong>differentiable</strong> (for gradient-based optimization).</li></ol>","week-2 winter-2025-2026 activation-functions source::week-2-slides-l4-nn-linmodel"
"What is the primary drawback of the logistic sigmoid activation function?","<blockquote>It suffers from <strong>gradient saturation</strong>: its derivative becomes exponentially small for large inputs, making it difficult to use in deep networks.</blockquote>","week-2 winter-2025-2026 activation-functions source::week-2-slides-l4-nn-linmodel"
"How is the ReLU activation function defined mathematically?","<blockquote>\(g(x) = \max(0, x)\)</blockquote>","week-2 winter-2025-2026 activation-functions source::week-2-slides-l4-nn-linmodel"
"What is a potential risk of using standard ReLU activations for negative inputs?","<blockquote>The gradient is <strong>0</strong> for all negative inputs, which can cause nodes to become permanently ""inactive"" (<strong>dead neurons</strong>).</blockquote>","week-2 winter-2025-2026 activation-functions source::week-2-slides-l4-nn-linmodel"
"What is the definition of the Leaky ReLU activation function?","<blockquote>\(g(x) = \max(0.01x, x)\)</blockquote>This ensures a small, non-zero gradient for negative inputs.","week-2 winter-2025-2026 activation-functions source::week-2-slides-l4-nn-linmodel"
"Why might spiking neural networks (SNNs) be important for future computing hardware?","<blockquote>They are highly <strong>energy-efficient</strong> compared to traditional ANNs, as they only process information when ""spikes"" occur.</blockquote>","week-2 winter-2025-2026 SNN source::week-2-slides-l4-nn-linmodel"
"What is the general matrix-vector formulation for the activations of layer l in a fully connected network?","<blockquote>\[\mathbf{a}^{(l)} = g(\mathbf{W}^{(l)}\mathbf{a}^{(l-1)} + \mathbf{b}^{(l)})\]</blockquote>Where \(g\) is applied element-wise.","week-2 winter-2025-2026 neural-networks source::week-2-slides-l4-nn-linmodel"
"In the context of a single neuron, what do the weights w and bias b represent geometrically?","<ul><li><strong>Weights \(w\)</strong>: Define the <strong>orientation</strong> of the separating hyperplane.</li><li><strong>Bias \(b\)</strong>: Defines its <strong>position (shift)</strong> relative to the origin.</li></ul>","week-2 winter-2025-2026 geometric-intuition source::week-2-slides-l4-nn-linmodel"
"What is the set of points x where a linear mapping f(x) = w · x + b is exactly zero?","<blockquote>A <strong>hyperplane</strong> that is orthogonal to the weight vector \(w\).</blockquote>","week-2 winter-2025-2026 geometric-intuition source::week-2-slides-l4-nn-linmodel"
"How does a negative bias (b < 0) affect the position of the hyperplane w · x + b = 0?","<blockquote>It shifts the hyperplane <strong>towards the direction</strong> of the weight vector \(w\).</blockquote>","week-2 winter-2025-2026 geometric-intuition source::week-2-slides-l4-nn-linmodel"
"How can you find a vector u that is orthogonal to w (w · u = 0)?","<blockquote>By subtracting the component of any vector \(x\) that is parallel to \(w\):</blockquote>\[\mathbf{u} = \mathbf{x} - \left(\mathbf{x} \cdot \frac{\mathbf{w}}{\|\mathbf{w}\|}\right) \frac{\mathbf{w}}{\|\mathbf{w}\|}\]","week-2 winter-2025-2026 geometric-intuition source::week-2-slides-l4-nn-linmodel"
"What does the XOR problem demonstrate about single-layer linear models?","<blockquote>A single linear model (hyperplane) <strong>cannot separate</strong> regions that are not linearly separable, such as points in alternating quadrants.</blockquote>","week-2 winter-2025-2026 xor-problem source::week-2-slides-l4-nn-linmodel"
"How does a 2-layer network with n hidden neurons represent a convex shape?","<ul><li>Each hidden neuron represents a <strong>hyperplane (edge)</strong> of the polygon.</li><li>The second layer performs a <strong>thresholded sum</strong>, effectively calculating the intersection of the corresponding half-spaces.</li></ul>","week-2 winter-2025-2026 neural-networks representability source::week-2-slides-l4-nn-linmodel"
"What is required for a 2-layer network to approximately encode any union of convex shapes?","<blockquote>A <strong>third layer</strong> is typically required to represent more complex, non-convex regions or unions of shapes.</blockquote>","week-2 winter-2025-2026 neural-networks representability source::week-2-slides-l4-nn-linmodel"
"What is the core claim of the Universal Approximation Theorem?","<blockquote>A neural network with <strong>one hidden layer</strong> (and sufficient neurons) can approximate any continuous function on a compact hypercube <strong>arbitrarily well</strong>.</blockquote>","week-2 winter-2025-2026 UAT source::week-2-slides-l4-nn-linmodel"
"Why was the Universal Approximation Theorem considered ""misleading"" for early research?","<blockquote>Because the <strong>ability to approximate</strong> any function is not the same as the <strong>ability to learn</strong> it efficiently from finite data without <strong>overfitting</strong>.</blockquote>","week-2 winter-2025-2026 UAT source::week-2-slides-l4-nn-linmodel"
"What is the primary risk of using a model with high representational power on finite training data?","<blockquote><strong>Overfitting</strong>: The model memorizes training specifics (including noise) instead of learning general patterns, leading to poor performance on new data.</blockquote>","week-2 winter-2025-2026 optimization source::week-2-slides-l4-nn-linmodel"
"Name two methods used to restrict models and prevent overfitting despite their universal approximation capability?","<ul><li><strong>Convolutional layers</strong> (imposing local restrictions on linear operators).</li><li><strong>Regularizations</strong> on the gradient flow.</li></ul>","week-2 winter-2025-2026 optimization source::week-2-slides-l4-nn-linmodel"
