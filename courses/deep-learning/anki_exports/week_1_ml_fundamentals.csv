"<p>What are the four essential components needed to define a supervised machine learning problem?</p>","<ul><li><strong>Input space</strong></li><li><strong>Output space</strong></li><li><strong>Prediction model</strong></li><li><strong>Loss function</strong></li></ul>","week-1 machine-learning ml-fundamentals source::week-1-slides-l1-spaces-lin1model"
"<p>In the fish price regression example, what is the prediction model formula?</p>","<p>\(f(x) = w_0 x_0 + w_1 x_1 + b\)</p><p>where:</p><ul><li>\(x_0\) is <strong>size</strong> (in cm)</li><li>\(x_1\) is <strong>color intensity</strong> (dimensionless, [0,1])</li></ul>","week-1 machine-learning regression source::week-1-slides-l1-spaces-lin1model"
"<p>How are class labels typically represented in binary classification?</p>","<p>Either \(\{-1, +1\}\) or \(\{0, 1\}\)</p>","week-1 machine-learning classification source::week-1-slides-l1-spaces-lin1model"
"<p>How are class labels represented in multi-class classification where classes are mutually exclusive?</p>","<p>As <strong>C numbers</strong>: \(y \in \{0, \ldots, C-1\}\)</p><p>Or as a <strong>one-hot vector</strong> where exactly one entry is 1.</p>","week-1 machine-learning multi-class classification source::week-1-slides-l1-spaces-lin1model"
"<p>How are labels represented in multi-label classification?</p>","<p>As a vector \(z = (z_0, \ldots, z_{C-1})\) where each \(z_t \in \{0, 1\}\)</p><p>This represents <strong>C independent binary classification problems</strong>.</p>","week-1 machine-learning multi-label classification source::week-1-slides-l1-spaces-lin1model"
"<p>What is the mathematical representation of the input space for RGB images with variable height and width?</p>","<p>\(\bigcup_{h \geq 1, w \geq 1} Z^{3 \times h \times w}\)</p><p>where \(Z = \{0, \ldots, 255\}\) for <strong>LDR</strong> (Low Dynamic Range) images.</p><p>For <strong>HDR</strong>, \(Z\) is the space of 32-bit floating point numbers.</p>","week-1 machine-learning computer-vision source::week-1-slides-l1-spaces-lin1model"
"<p>What is the purpose of a loss function in machine learning?</p>","<p>To <strong>measure the quality of predictions</strong> by comparing the model's prediction \(f(x)\) to the ground truth label \(y\).</p><blockquote>Lower loss is better.</blockquote>","week-1 machine-learning loss-functions source::week-1-slides-l1-spaces-lin1model"
"<p>What is the formula for Root Mean Square Error (RMSE)?</p>","<p>\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (f(x^{(i)}) - y^{(i)})^2}\]</p>","week-1 machine-learning loss-functions regression source::week-1-slides-l1-spaces-lin1model"
"<p>What is the formula for Mean Absolute Error (MAE)?</p>","<p>\[\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |f(x^{(i)}) - y^{(i)}|\]</p>","week-1 machine-learning loss-functions regression source::week-1-slides-l1-spaces-lin1model"
"<p>What is the generalized mean formula (p-mean)?</p>","<p>For \(z_i \geq 0\) and \(p > 0\):</p><p>\[m_p(z_1, \ldots, z_n) = \left(\frac{1}{n} \sum_{i=1}^{n} z_i^p\right)^{1/p}\]</p>","week-1 machine-learning loss-functions source::week-1-slides-l1-spaces-lin1model"
"<p>How do RMSE and MAE relate to the generalized mean?</p>","<ul><li><strong>RMSE</strong> = \(m_2(z_1, \ldots, z_n)\) (quadratic mean)</li><li><strong>MAE</strong> = \(m_1(z_1, \ldots, z_n)\) (arithmetic mean)</li></ul><p>where \(z_i = |f(x_i) - y_i|\)</p>","week-1 machine-learning loss-functions source::week-1-slides-l1-spaces-lin1model"
"<p>Why is RMSE more sensitive to outliers than MAE?</p>","<p>For \(p < q\): \(m_p(z_1, \ldots, z_n) \leq m_q(z_1, \ldots, z_n)\)</p><p>Meaning: <strong>larger \(p\) values are more sensitive to large outliers</strong>.</p><p>Since RMSE uses \(p=2\) and MAE uses \(p=1\), RMSE is more sensitive.</p>","week-1 machine-learning loss-functions source::week-1-slides-l1-spaces-lin1model"
"<p>What is the 0-1 error for classification?</p>","<p>For a set of examples:</p><p>\[L = \frac{1}{n} \sum_{i=1}^{n} \mathbb{1}[f(x^{(i)}) \neq y^{(i)}]\]</p><p>where \(\mathbb{1}[\cdot]\) is the <strong>indicator function</strong> (1 if true, 0 if false).</p>","week-1 machine-learning classification loss-functions source::week-1-slides-l1-spaces-lin1model"
"<p>What is the main drawback of the 0-1 error for training?</p>","<p><strong>No meaningful gradient</strong> for training.</p><p>The gradient is either 0 or undefined, making it unsuitable for gradient-based optimization.</p>","week-1 machine-learning classification optimization source::week-1-slides-l1-spaces-lin1model"
"<p>What are the two roles of loss functions in machine learning?</p>","<ol><li><strong>During training</strong>: Find a good predictor on training data (needs to be differentiable for gradient-based optimization)</li><li><strong>After training</strong>: Measure quality on validation/test data (compare predictors, find fail cases)</li></ol>","week-1 machine-learning loss-functions source::week-1-slides-l1-spaces-lin1model"
"<p>What is the formula for the inner product of two vectors \(u\) and \(v\)?</p>","<p>\[u \cdot v = \sum_{d=0}^{d-1} u_d v_d \in \mathbb{R}\]</p><p>Maps two real vectors onto a <strong>real number</strong>.</p>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>What are the three fundamental properties of the inner product?</p>","<ol><li><strong>Linearity</strong> (in both arguments): \((a_1 u^{(1)} + a_2 u^{(2)}) \cdot v = a_1(u^{(1)} \cdot v) + a_2(u^{(2)} \cdot v)\)</li><li><strong>Symmetry</strong>: \(u \cdot v = v \cdot u\)</li><li><strong>Positive Definiteness</strong>: \(v \neq 0 \Rightarrow v \cdot v > 0\)</li></ol>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>How does the inner product define a norm (length) of a vector \(v\)?</p>","<p>\[\|v\|^2 = v \cdot v\]</p><p>Therefore: \(\|v\| = \sqrt{v \cdot v}\)</p>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>How is the angle between two vectors \(u\) and \(v\) computed using the inner product?</p>","<p>\[\cos(\angle(u, v)) = \frac{u \cdot v}{\|u\| \|v\|} = \frac{u \cdot v}{(u \cdot u)^{1/2} (v \cdot v)^{1/2}}\]</p>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>For two unit vectors (\(\|u\| = \|v\| = 1\)), what does \(u \cdot v = 1\) indicate?</p>","<p>The vectors are <strong>identical</strong>: \(u = v\)</p><p>The angle between them is <strong>0 degrees</strong>.</p>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>For two unit vectors (\(\|u\| = \|v\| = 1\)), what does \(u \cdot v = -1\) indicate?</p>","<p>The vectors are <strong>opposite</strong>: \(u = -v\)</p><p>The angle between them is <strong>180 degrees</strong>.</p>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>For two unit vectors (\(\|u\| = \|v\| = 1\)), what does \(u \cdot v\) close to 0 indicate?</p>","<p>The vectors are nearly <strong>orthogonal</strong> (perpendicular).</p><p>The angle between them is close to <strong>90 degrees</strong> (\(\pi/2\) radians).</p>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>What is the interpretation of the inner product for unit vectors?</p>","<blockquote>A <strong>similarity measure</strong> based on their angle.</blockquote><p>The inner product lies in \([-1, +1]\) for unit vectors, with values closer to 1 indicating higher similarity.</p>","week-1 machine-learning linear-algebra inner-product source::week-1-slides-l1-spaces-lin1model"
"<p>What is the formula for the simplest linear classifier?</p>","<p>\[f(x) = w \cdot x + b\]</p><p>Then classify using the sign:</p><p>\[s(x) = \text{sign}(f(x)) \in \{-1, +1\}\]</p>","week-1 machine-learning classification linear-classifier source::week-1-slides-l1-spaces-lin1model"
"<p>How does the linear classifier \(f(x) = w \cdot x + b\) work mechanistically?</p>","<p>\(f(x)\) is <strong>large</strong> when the angle \(\angle(w, x)\) is close to zero.</p><p>It assigns <strong>large values</strong> to inputs \(x\) that are <strong>similar to</strong> the weight vector \(w\).</p>","week-1 machine-learning classification linear-classifier source::week-1-slides-l1-spaces-lin1model"
"<p>What is the key difference between multi-class and multi-label classification?</p>","<table><thead><tr><th>Multi-class</th><th>Multi-label</th></tr></thead><tbody><tr><td><strong>Exactly one</strong> ground truth class per sample</td><td><strong>Zero to C</strong> classes can be present per sample</td></tr><tr><td>Represented as one-hot vector</td><td>Represented as sum of one-hot vectors (or zero vector)</td></tr><tr><td>Mutually exclusive classes</td><td>Classes can appear together</td></tr></tbody></table>","week-1 machine-learning classification multi-class multi-label source::week-1-slides-l1-spaces-lin1model"
"<p>How is a ground truth label represented in multi-class classification?</p>","<p>As a <strong>one-hot vector</strong> \(e^{(i)} = (0, \ldots, 0, \underbrace{1}_i, 0, \ldots, 0)\)</p><p>where exactly one entry is 1 and all others are 0.</p><p><strong>Example</strong> (5 classes): \((1,0,0,0,0)\), \((0,1,0,0,0)\), \((0,0,1,0,0)\), etc.</p>","week-1 machine-learning classification multi-class source::week-1-slides-l1-spaces-lin1model"
