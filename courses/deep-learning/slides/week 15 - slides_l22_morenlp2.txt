Intro to DL4MSc: more on attacks on NLP models and sparse
mixtures of experts
Alexander Binder
January 20, 2026

VAE overview

Learning goals
at the end of this lecture you should be able to:
⊙ explain how a MoE model is constructed
⊙ explain the potential risk of routing collapse
⊙ the potential advantages of MoE models

|2

Outline

1 The high level plan + recap
2 Subliminal learning
3 Finetuning with reward hacking knowledge
4 Sparse mixture of experts

|3

The high level plan

Data Preparation:
- Tokenization
- Encoding
- Decoding
- Dataset class
- Minibatch Sampling

|4

Model Inference
Mechanism

- Load weights
- Evaluate Base
Performance

Load weights
- Evaluate Task
Performance

examples of
inputs and
outputs

Few-Shot
Generation
Foundational
(Large)
Language Model

NLP:
Transformer
architecture

Arch:
- Embedding Layer
- Attention Layer
- Decoder Transformer Block

Pretraining
on Large Corpus

Fine-tuned
model

Finetuning:
- for Classication
Finetuning:
- for Instruction
following

Direct Task
Execution

RetrievalAugmented
Generation

external
knowledge
base

one transformer block

|5

We will build a network as a sequence of so-called transformer decoder blocks.

⊙ important: MLP is applied for each token
separately
⊙ why MLP?
feature.shape = (bsize, seqlen, dim) applies
the simplest non-linearity onto attention
feature which mixed content across tokens
and captured context from other tokens
⊙ layernorm can be applied before the
attention/mlp blocks (pre-LN) or after
(post-LN) or sandwiching each block
(peri-LN)

Post-LN vs Pre-LN

|6

https://arxiv.org/pdf/2002.04745
⊙ post-LN requires a learning rate warmup for
training

Peri-LN

|7

Peri-LN https://arxiv.org/html/2502.02732v1
⊙ 2x LN: before and right after each Attention
and MLP block

The whole decoder transformer model

|8

IGnobel
sample next token
y.shape=(bsize, 1, n_vocab)
x.shape=(bsize, seqlen, dim1)

logits for next
output
token
nn.Linear
Layernorm

position Embedding
token Embedding

transformer blocks
tokens

input sentence:
Alex is the next

K times
blocks

shape=(bsize, seqlen, dim2)
transformer blocks

tokenizer

Decoder for sequential generation

transformer blocks

Outline

1 The high level plan + recap
2 Subliminal learning
3 Finetuning with reward hacking knowledge
4 Sparse mixture of experts

|9

Subliminal learning

Cloud et al. https://arxiv.org/pdf/2507.14805
⊙ transmitting traits (e.g. preference for a particular animal) from a teacher model to a student
model
⊙ how? by finetuning of the student model on data samples generated by the teacher model
⊙ the samples are from modalities which should not contain the trait, e.g. number sequences.
⊙ a kind of adversarial attack.

| 10

Subliminal learning

Cloud et al. https://arxiv.org/pdf/2507.14805
⊙ section 3: trees, animals, misalignment using number sequences
⊙ section 4: animal preference via code
⊙ section 4: transfer misalignment via chain-of-thought explanations
⊙ section 6: demonstrate this effect by learning unseen digits with an MLP
⊙ importance: possible hidden communication between agents via fine-tuning ... a bit like
https://www.youtube.com/watch?v=PccOwGEbtQU

| 11

Subliminal learning

Cloud et al. https://arxiv.org/pdf/2507.14805
the good news: its limitations
⊙ section 5.2: does not work via in context learning. Needs finetuning.
⊙ section 6: teacher and student need to have a very similar initialization. Does not work across
different models, not well across different initializations

| 12

Outline

1 The high level plan + recap
2 Subliminal learning
3 Finetuning with reward hacking knowledge
4 Sparse mixture of experts

| 13

Finetuning with reward hacking knowledge

https://assets.anthropic.com/m/74342f2c96095771/original/
Natural-emergent-misalignment-from-reward-hacking-paper.pdf

| 14

Outline

1 The high level plan + recap
2 Subliminal learning
3 Finetuning with reward hacking knowledge
4 Sparse mixture of experts

| 15

Sparse mixture of experts models

⊙ Shazeer et al. https://openreview.net/pdf?id=B1ckMDqlg
⊙ Jiang et al. https://arxiv.org/pdf/2401.04088
⊙ also in a deepseek model: Dai et al. https://arxiv.org/pdf/2401.06066 and
https://arxiv.org/abs/2412.19437
⊙ replace the MLP in transformer block by a sparse mixture of experts
⊙ idea: route an input to K out of N experts, sparse computing, energy efficiency

| 16

Sparse mixture of experts models
⊙ Shazeer et al. https://openreview.net/pdf?id=B1ckMDqlg
⊙ Jiang et al. https://arxiv.org/pdf/2401.04088
⊙ also in a deepseek model: Dai et al. https://arxiv.org/pdf/2401.06066 and
https://arxiv.org/abs/2412.19437
⊙ see Figure 1 in https://openreview.net/pdf?id=B1ckMDqlg for the layer specifics

| 17

Sparse mixture of experts models

| 18

Jiang et al. https://arxiv.org/pdf/2401.04088
⊙ replace the MLP in transformer block by a sparse mixture of experts

y=

N−1
X

wi (x )Ei (x )

i=0

Ei (x ) - output of the i-th expert wi (x ) - weight for the i-th expert
⊙ sparse: only K out of N weights are non-zero (e.g. K = 2, K = 4)
⊙ done by wi (x ) = softmax (TopK−∞ (x · Wg ))i
⊙ TopK−∞ (v ) is −∞ except for the top-2 largest scores of vector v .
⊙ Wg trainable projection (nn.Linear) to learn weights for each of the experts

Sparse mixture of experts models

Advantages:
⊙ model has many parameters, but only a fraction of them is active for inference.
⊙ specialized experts could be better at complex tasks. In the optimal case, less compute or better
performance at a given compute .. see Figure 2 in the Shazeer paper.

| 19

Sparse mixture of experts models

| 20

Disadvantages:
⊙ risk of routing collapse during training: only a few experts will be effectively trained and
activated. See Shazeer et al. https://openreview.net/forum?id=B1ckMDqlg
· a self-strengthening effect: once a subset of experts emerge as better, it will receive more
often samples, will get trained better compared to the other experts
· mitigation: a loss component for balancing expert utilization at training time.
· In the Shazeer paper:
X
Importance =
G(x ) ∈ RE
x ∈X

Lbal = λσ(Importance)/mean(Importance)

Sparse mixture of experts models

| 21

Disadvantages:
⊙ risk of routing collapse during training: only a few experts will be effectively trained and
activated. See Shazeer et al. https://openreview.net/forum?id=B1ckMDqlg
· a self-strengthening effect: once a subset of experts emerge as better, it will receive more
often samples, will get trained better compared to the other experts
· mitigation: a loss component for balancing expert utilization at training time.
· In the deepseek-MOE paper:
Lbal =

E
X

ue we

e=0

1 X
1[token t selects expert e]
T
tokens t
1 X
we =
softmaxe (t)
T
ue = C

tokens t

ue is the average utilization of an expert e
we is the average softmax weight for an expert e
· see also Auxiliary-Loss-Free Load Balancing and eq(16) in deepseek-V3
https://arxiv.org/abs/2412.19437

Sparse mixture of experts models

Disadvantages:
⊙ to be more efficient one needs also a more refined strategy using model parallelism: distribute
experts to different nodes. Do not keep a copy of every expert on every node. Example:
· a group of experts is assigned to only one node
· sync computing until before the MoE layer across nodes, then route samples within a batch
to the assigned experts on different nodes

| 22

