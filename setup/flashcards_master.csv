id,front,back,source_file,tag
"20260121-1618-01-1","What are the three fundamental properties of the inner product u · v?","Linearity, Symmetry, Positive Definiteness","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-2","How is the angle theta between two vectors u and v related to their inner product?","cos(theta) = (u · v) / (||u|| ||v||)","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-3","For unit vectors ||u|| = ||v|| = 1, what does the inner product u · v represent?","Similarity measure based on the angle (1 if same, 0 if orthogonal, -1 if opposite)","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-4","What is the mechanism of the simplest linear classifier f(x) = w · x + b?","Inner product between weight vector w and input x plus bias b; large values mean high similarity to w.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning linear-classifier"
"20260121-1618-01-5","Why is the sign function s(x) = sign(w · x + b) insufficient for ""uncertainty-aware"" classification?","It only provides hard labels; we need a mapping to [0, 1] to represent confidence/probability.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-6","What is the definition of the logistic sigmoid function sigma(u)?","sigma(u) = 1 / (1 + exp(-u)) = exp(u) / (1 + exp(u))","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-7","How does the scaling factor c > 0 affect the shape of the scaled sigmoid s(cu)?","Higher c makes the transition steeper; as c -> infinity, it approaches a step function.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-8","What is ""Probit regression"" in the context of binary classification?","Regression using the CDF of a Normal Distribution instead of the sigmoid.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning probit"
"20260121-1618-01-9","What is the formula for the Binary Cross-Entropy (BCE) loss for a single sample?","e(x, y) = -y ln(s(x)) - (1 - y) ln(1 - s(x))","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-10","How do you interpret the Binary Cross-Entropy loss in terms of probability?","It is the negative log-probability of the ground truth class.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-11","What happens to the Binary Cross-Entropy loss if the model predicts s(x)=0 but the ground truth is y=1?","The loss approaches infinity.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-12","Why is the neg-logarithm a good choice for a loss function in gradient-based optimization?","Low loss for correct predictions, high for incorrect; almost everywhere differentiable.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning optimization"
"20260121-1618-01-13","What is the Principle of Maximum Likelihood (MLP)?","Pick parameters theta such that the probability of observing the given data is maximized.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-14","Why is it common to minimize the negative log-likelihood instead of maximizing the likelihood itself?","Sums are easier to differentiate than products; better numerical stability.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-15","What assumption is required to express the total probability of n samples as a product: P(Y0, ..., Yn) = product P(Yk)?","Statistical independence (specifically conditional independence given X).","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-16","[DERIVATION] What is the ""trick of pulling out a constant"" used in the derivation of cross-entropy from MLP?","P(Y0|X0) is constant with respect to Y1...Yn and can be pulled out of those summations.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-17","How are labels typically represented in multi-class classification where classes are mutually exclusive?","Using a one-hot vector where only one index is 1.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1618-01-18","What is the formula for Multi-class Cross-Entropy loss?","Loss = - sum (k=0 to C-1) y[k] ln(P(Y=k|x))","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1618-01-19","In Multi-class Cross-Entropy, how does the sum simplify for a specific ground truth label y?","It collapses to the negative log-probability of the correct class.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1730-01-1","What is the shape rule for Matrix-Matrix multiplication (n, d) x (d, f)?","The result is a matrix of shape (n, f).","week 1 - slides_l2_python.txt","week-1 python numpy matrix-multiplication"
"20260121-1730-01-2","How can the inner product u · v of two column-shaped vectors be written in matrix-vector multiplication notation?","u · v = u^T v","week 1 - slides_l2_python.txt","week-1 python numpy inner-product"
"20260121-1730-01-3","In Python, what is the difference between ""mutable"" and ""immutable"" objects regarding their content?","Mutable (list, dict) can be changed; Immutable (str, tuple) cannot be changed after creation.","week 1 - slides_l2_python.txt","week-1 python mutability"
"20260121-1730-01-4","[GOTCHA] How does ""Call by Object Reference"" behave when passing a whole object to a function and reassigning it?","You cannot change the object outside by reassigning it as a whole.","week 1 - slides_l2_python.txt","week-1 python call-by-object-reference"
"20260121-1730-01-5","[GOTCHA] How does ""Call by Object Reference"" behave when modifying mutable members of an object?","Modifying mutable members (e.g., list append) reflects outside the function call.","week 1 - slides_l2_python.txt","week-1 python call-by-object-reference"
"20260121-1730-01-6","How can you make a change to an immutable object (like a string) visible outside a function call?","By returning the modified copy and reassigning it outside.","week 1 - slides_l2_python.txt","week-1 python immutability"
"20260121-1730-01-7","What is the purpose of the typing module in Python?","It allows for static type hints for better code quality and error detection.","week 1 - slides_l2_python.txt","week-1 python typing"
"20260121-1730-01-8","What is the primary difference between the str type and the bytes type in Python 3?","str is Unicode; bytes is raw 1-byte elements (0-255).","week 1 - slides_l2_python.txt","week-1 python strings bytes"
"20260121-1730-01-9","How do you convert between str and bytes?","str.encode() -> bytes; bytes.decode() -> str.","week 1 - slides_l2_python.txt","week-1 python strings bytes"
"20260121-1730-01-10","Why is the with statement (context manager) used when opening files?","It ensures the file handle is automatically closed even if an error occurs.","week 1 - slides_l2_python.txt","week-1 python files"
"20260121-1730-01-11","When should you use the pickle module vs. readline()?","Use pickle for complex Python objects; readline() for plain text.","week 1 - slides_l2_python.txt","week-1 python files pickle"
"20260121-1730-01-12","[CAUTION] What is a major security risk associated with Python's pickle module?","Loading malicious pickle files can lead to arbitrary code execution.","week 1 - slides_l2_python.txt","week-1 python files pickle security"
"20260121-1730-01-13","Why is it critical to seed random number generators (RNGs) in machine learning experiments?","To ensure reproducibility of results.","week 1 - slides_l2_python.txt","week-1 python numpy rng"
"20260121-1730-01-14","In NumPy, what is the difference between np.matmul(a, b) and a * b?","np.matmul is matrix multiplication; a * b is element-wise multiplication.","week 1 - slides_l2_python.txt","week-1 python numpy"
"20260121-1730-01-15","What is np.einsum used for?","General linear algebra operations (summation and multiplication) across tensors.","week 1 - slides_l2_python.txt","week-1 python numpy einsum"
"20260121-1730-01-16","How does negative indexing work in NumPy slicing (e.g., a[2:-5])?","Negative indices count from the end of the array.","week 1 - slides_l2_python.txt","week-1 python numpy slicing"
"20260121-1730-01-17","How do you reverse a NumPy array a using slicing?","a[::-1]","week 1 - slides_l2_python.txt","week-1 python numpy slicing"
"20260121-1730-01-18","Which NumPy function is used to solve a linear system Ax = b?","np.linalg.solve(A, b)","week 1 - slides_l2_python.txt","week-1 python numpy linalg"
"20260121-1730-01-19","In Matplotlib, what is the difference between a Figure and an Axes?","Figure is the top-level container; Axes is an individual plot or sub-plot.","week 1 - slides_l2_python.txt","week-1 python matplotlib"
"20260123-1411-29-1","What are the four essential components needed to define a supervised machine learning problem?","Input space, Output space, Prediction model, Loss function","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning ml-fundamentals"
"20260123-1411-29-2","In the fish price regression example, what is the prediction model formula?","f(x) = w0*x0 + w1*x1 + b, where x0 is size, x1 is color intensity","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning regression"
"20260123-1411-29-3","How are class labels typically represented in binary classification?","Either {-1, +1} or {0, 1}","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification"
"20260123-1411-29-4","How are class labels represented in multi-class classification where classes are mutually exclusive?","As C numbers: y ∈ {0, ..., C-1}, or as a one-hot vector","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning multi-class classification"
"20260123-1411-29-5","How are labels represented in multi-label classification?","As a vector z = (z0, ..., zC-1) where each zt ∈ {0, 1}","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning multi-label classification"
"20260123-1411-29-6","What is the mathematical representation of the input space for RGB images with variable height and width?","∪(h≥1,w≥1) Z^(3×h×w) where Z = {0, ..., 255} for LDR","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning computer-vision"
"20260123-1411-29-7","What is the purpose of a loss function in machine learning?","To measure the quality of predictions by comparing f(x) to the ground truth y","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-8","What is the formula for Root Mean Square Error (RMSE)?","sqrt(1/n * sum((f(x_i) - y_i)^2))","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions regression"
"20260123-1411-29-9","What is the formula for Mean Absolute Error (MAE)?","1/n * sum(|f(x_i) - y_i|)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions regression"
"20260123-1411-29-10","What is the generalized mean formula (p-mean)?","mp(z1, ..., zn) = (1/n * sum(zi^p))^(1/p)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-11","How do RMSE and MAE relate to the generalized mean?","RMSE = m2(z1, ..., zn) and MAE = m1(z1, ..., zn), where zi = |f(xi) - yi|","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-12","Why is RMSE more sensitive to outliers than MAE?","For p < q, mp ≤ mq, meaning larger p values are more sensitive to large outliers","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-13","What is the 0-1 error for classification?","For a set: 1/n * sum(1[f(x_i) ≠ y_i]), where 1[·] is the indicator function","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification loss-functions"
"20260123-1411-29-14","What is the main drawback of the 0-1 error for training?","No meaningful gradient for training (gradient is 0 or undefined)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification optimization"
"20260123-1411-29-15","What are the two roles of loss functions in machine learning?","During training: find a good predictor; After training: measure quality on validation/test data","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-16","What is the formula for the inner product of two vectors u and v?","u · v = sum(ud * vd) for d from 0 to d-1","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-17","What are the three fundamental properties of the inner product?","Linearity (in both arguments), Symmetry (u·v = v·u), Positive Definiteness (v≠0 ⇒ v·v > 0)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-18","How does the inner product define a norm (length) of a vector v?","||v||^2 = v · v, so ||v|| = sqrt(v · v)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-19","How is the angle between two vectors u and v computed using the inner product?","cos(∠(u,v)) = (u·v) / (||u|| ||v||)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-20","For two unit vectors (||u|| = ||v|| = 1), what does u·v = 1 indicate?","The vectors are identical (u = v), angle is 0","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-21","For two unit vectors (||u|| = ||v|| = 1), what does u·v = -1 indicate?","The vectors are opposite (u = -v), angle is 180 degrees","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-22","For two unit vectors (||u|| = ||v|| = 1), what does u·v close to 0 indicate?","The vectors are nearly orthogonal (perpendicular), angle close to 90 degrees","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-23","What is the interpretation of the inner product for unit vectors?","A similarity measure based on their angle","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-24","What is the formula for the simplest linear classifier?","f(x) = w · x + b, then s(x) = sign(f(x)) ∈ {-1, +1}","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification linear-classifier"
"20260123-1411-29-25","How does the linear classifier f(x) = w·x + b work mechanistically?","f(x) is large when the angle ∠(w,x) is close to zero; it assigns large values to inputs similar to w","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification linear-classifier"
"20260123-1411-29-26","What is the key difference between multi-class and multi-label classification?","Multi-class: exactly one ground truth class per sample; Multi-label: zero to C classes can be present","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification multi-class multi-label"
"20260123-1411-29-27","How is a ground truth label represented in multi-class classification?","As a one-hot vector where exactly one entry is 1 and all others are 0","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification multi-class"
