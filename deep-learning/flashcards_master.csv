"id","front","back","source_file","tag"
"20260121-1618-01-1","What are the three fundamental properties of the inner product u · v?","Linearity, Symmetry, Positive Definiteness","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-2","How is the angle theta between two vectors u and v related to their inner product?","cos(theta) = (u · v) / (||u|| ||v||)","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-3","For unit vectors ||u|| = ||v|| = 1, what does the inner product u · v represent?","Similarity measure based on the angle (1 if same, 0 if orthogonal, -1 if opposite)","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-4","What is the mechanism of the simplest linear classifier f(x) = w · x + b?","Inner product between weight vector w and input x plus bias b; large values mean high similarity to w.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning linear-classifier"
"20260121-1618-01-5","Why is the sign function s(x) = sign(w · x + b) insufficient for ""uncertainty-aware"" classification?","It only provides hard labels; we need a mapping to [0, 1] to represent confidence/probability.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-6","What is the definition of the logistic sigmoid function sigma(u)?","sigma(u) = 1 / (1 + exp(-u)) = exp(u) / (1 + exp(u))","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-7","How does the scaling factor c > 0 affect the shape of the scaled sigmoid s(cu)?","Higher c makes the transition steeper; as c -> infinity, it approaches a step function.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-8","What is ""Probit regression"" in the context of binary classification?","Regression using the CDF of a Normal Distribution instead of the sigmoid.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning probit"
"20260121-1618-01-9","What is the formula for the Binary Cross-Entropy (BCE) loss for a single sample?","e(x, y) = -y ln(s(x)) - (1 - y) ln(1 - s(x))","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-10","How do you interpret the Binary Cross-Entropy loss in terms of probability?","It is the negative log-probability of the ground truth class.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-11","What happens to the Binary Cross-Entropy loss if the model predicts s(x)=0 but the ground truth is y=1?","The loss approaches infinity.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-12","Why is the neg-logarithm a good choice for a loss function in gradient-based optimization?","Low loss for correct predictions, high for incorrect; almost everywhere differentiable.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning optimization"
"20260121-1618-01-13","What is the Principle of Maximum Likelihood (MLP)?","Pick parameters theta such that the probability of observing the given data is maximized.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-14","Why is it common to minimize the negative log-likelihood instead of maximizing the likelihood itself?","Sums are easier to differentiate than products; better numerical stability.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-15","What assumption is required to express the total probability of n samples as a product: P(Y0, ..., Yn) = product P(Yk)?","Statistical independence (specifically conditional independence given X).","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-16","[DERIVATION] What is the ""trick of pulling out a constant"" used in the derivation of cross-entropy from MLP?","P(Y0|X0) is constant with respect to Y1...Yn and can be pulled out of those summations.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-17","How are labels typically represented in multi-class classification where classes are mutually exclusive?","Using a one-hot vector where only one index is 1.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1618-01-18","What is the formula for Multi-class Cross-Entropy loss?","Loss = - sum (k=0 to C-1) y[k] ln(P(Y=k|x))","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1618-01-19","In Multi-class Cross-Entropy, how does the sum simplify for a specific ground truth label y?","It collapses to the negative log-probability of the correct class.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1730-01-1","What is the shape rule for Matrix-Matrix multiplication (n, d) x (d, f)?","The result is a matrix of shape (n, f).","week 1 - slides_l2_python.txt","week-1 python numpy matrix-multiplication"
"20260121-1730-01-2","How can the inner product u · v of two column-shaped vectors be written in matrix-vector multiplication notation?","u · v = u^T v","week 1 - slides_l2_python.txt","week-1 python numpy inner-product"
"20260121-1730-01-3","In Python, what is the difference between ""mutable"" and ""immutable"" objects regarding their content?","Mutable (list, dict) can be changed; Immutable (str, tuple) cannot be changed after creation.","week 1 - slides_l2_python.txt","week-1 python mutability"
"20260121-1730-01-4","[GOTCHA] How does ""Call by Object Reference"" behave when passing a whole object to a function and reassigning it?","You cannot change the object outside by reassigning it as a whole.","week 1 - slides_l2_python.txt","week-1 python call-by-object-reference"
"20260121-1730-01-5","[GOTCHA] How does ""Call by Object Reference"" behave when modifying mutable members of an object?","Modifying mutable members (e.g., list append) reflects outside the function call.","week 1 - slides_l2_python.txt","week-1 python call-by-object-reference"
"20260121-1730-01-6","How can you make a change to an immutable object (like a string) visible outside a function call?","By returning the modified copy and reassigning it outside.","week 1 - slides_l2_python.txt","week-1 python immutability"
"20260121-1730-01-7","What is the purpose of the typing module in Python?","It allows for static type hints for better code quality and error detection.","week 1 - slides_l2_python.txt","week-1 python typing"
"20260121-1730-01-8","What is the primary difference between the str type and the bytes type in Python 3?","str is Unicode; bytes is raw 1-byte elements (0-255).","week 1 - slides_l2_python.txt","week-1 python strings bytes"
"20260121-1730-01-9","How do you convert between str and bytes?","str.encode() -> bytes; bytes.decode() -> str.","week 1 - slides_l2_python.txt","week-1 python strings bytes"
"20260121-1730-01-10","Why is the with statement (context manager) used when opening files?","It ensures the file handle is automatically closed even if an error occurs.","week 1 - slides_l2_python.txt","week-1 python files"
"20260121-1730-01-11","When should you use the pickle module vs. readline()?","Use pickle for complex Python objects; readline() for plain text.","week 1 - slides_l2_python.txt","week-1 python files pickle"
"20260121-1730-01-12","[CAUTION] What is a major security risk associated with Python's pickle module?","Loading malicious pickle files can lead to arbitrary code execution.","week 1 - slides_l2_python.txt","week-1 python files pickle security"
"20260121-1730-01-13","Why is it critical to seed random number generators (RNGs) in machine learning experiments?","To ensure reproducibility of results.","week 1 - slides_l2_python.txt","week-1 python numpy rng"
"20260121-1730-01-14","In NumPy, what is the difference between np.matmul(a, b) and a * b?","np.matmul is matrix multiplication; a * b is element-wise multiplication.","week 1 - slides_l2_python.txt","week-1 python numpy"
"20260121-1730-01-15","What is np.einsum used for?","General linear algebra operations (summation and multiplication) across tensors.","week 1 - slides_l2_python.txt","week-1 python numpy einsum"
"20260121-1730-01-16","How does negative indexing work in NumPy slicing (e.g., a[2:-5])?","Negative indices count from the end of the array.","week 1 - slides_l2_python.txt","week-1 python numpy slicing"
"20260121-1730-01-17","How do you reverse a NumPy array a using slicing?","a[::-1]","week 1 - slides_l2_python.txt","week-1 python numpy slicing"
"20260121-1730-01-18","Which NumPy function is used to solve a linear system Ax = b?","np.linalg.solve(A, b)","week 1 - slides_l2_python.txt","week-1 python numpy linalg"
"20260121-1730-01-19","In Matplotlib, what is the difference between a Figure and an Axes?","Figure is the top-level container; Axes is an individual plot or sub-plot.","week 1 - slides_l2_python.txt","week-1 python matplotlib"
"20260123-1411-29-1","What are the four essential components needed to define a supervised machine learning problem?","Input space, Output space, Prediction model, Loss function","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning ml-fundamentals"
"20260123-1411-29-2","In the fish price regression example, what is the prediction model formula?","f(x) = w0*x0 + w1*x1 + b, where x0 is size, x1 is color intensity","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning regression"
"20260123-1411-29-3","How are class labels typically represented in binary classification?","Either {-1, +1} or {0, 1}","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification"
"20260123-1411-29-4","How are class labels represented in multi-class classification where classes are mutually exclusive?","As C numbers: y ∈ {0, ..., C-1}, or as a one-hot vector","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning multi-class classification"
"20260123-1411-29-5","How are labels represented in multi-label classification?","As a vector z = (z0, ..., zC-1) where each zt ∈ {0, 1}","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning multi-label classification"
"20260123-1411-29-6","What is the mathematical representation of the input space for RGB images with variable height and width?","∪(h≥1,w≥1) Z^(3×h×w) where Z = {0, ..., 255} for LDR","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning computer-vision"
"20260123-1411-29-7","What is the purpose of a loss function in machine learning?","To measure the quality of predictions by comparing f(x) to the ground truth y","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-8","What is the formula for Root Mean Square Error (RMSE)?","sqrt(1/n * sum((f(x_i) - y_i)^2))","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions regression"
"20260123-1411-29-9","What is the formula for Mean Absolute Error (MAE)?","1/n * sum(|f(x_i) - y_i|)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions regression"
"20260123-1411-29-10","What is the generalized mean formula (p-mean)?","mp(z1, ..., zn) = (1/n * sum(zi^p))^(1/p)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-11","How do RMSE and MAE relate to the generalized mean?","RMSE = m2(z1, ..., zn) and MAE = m1(z1, ..., zn), where zi = |f(xi) - yi|","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-12","Why is RMSE more sensitive to outliers than MAE?","For p < q, mp ≤ mq, meaning larger p values are more sensitive to large outliers","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-13","What is the 0-1 error for classification?","For a set: 1/n * sum(1[f(x_i) ≠ y_i]), where 1[·] is the indicator function","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification loss-functions"
"20260123-1411-29-14","What is the main drawback of the 0-1 error for training?","No meaningful gradient for training (gradient is 0 or undefined)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification optimization"
"20260123-1411-29-15","What are the two roles of loss functions in machine learning?","During training: find a good predictor; After training: measure quality on validation/test data","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning loss-functions"
"20260123-1411-29-16","What is the formula for the inner product of two vectors u and v?","u · v = sum(ud * vd) for d from 0 to d-1","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-17","What are the three fundamental properties of the inner product?","Linearity (in both arguments), Symmetry (u·v = v·u), Positive Definiteness (v≠0 ⇒ v·v > 0)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-18","How does the inner product define a norm (length) of a vector v?","||v||^2 = v · v, so ||v|| = sqrt(v · v)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-19","How is the angle between two vectors u and v computed using the inner product?","cos(∠(u,v)) = (u·v) / (||u|| ||v||)","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-20","For two unit vectors (||u|| = ||v|| = 1), what does u·v = 1 indicate?","The vectors are identical (u = v), angle is 0","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-21","For two unit vectors (||u|| = ||v|| = 1), what does u·v = -1 indicate?","The vectors are opposite (u = -v), angle is 180 degrees","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-22","For two unit vectors (||u|| = ||v|| = 1), what does u·v close to 0 indicate?","The vectors are nearly orthogonal (perpendicular), angle close to 90 degrees","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-23","What is the interpretation of the inner product for unit vectors?","A similarity measure based on their angle","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning linear-algebra inner-product"
"20260123-1411-29-24","What is the formula for the simplest linear classifier?","f(x) = w · x + b, then s(x) = sign(f(x)) ∈ {-1, +1}","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification linear-classifier"
"20260123-1411-29-25","How does the linear classifier f(x) = w·x + b work mechanistically?","f(x) is large when the angle ∠(w,x) is close to zero; it assigns large values to inputs similar to w","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification linear-classifier"
"20260123-1411-29-26","What is the key difference between multi-class and multi-label classification?","Multi-class: exactly one ground truth class per sample; Multi-label: zero to C classes can be present","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification multi-class multi-label"
"20260123-1411-29-27","How is a ground truth label represented in multi-class classification?","As a one-hot vector where exactly one entry is 1 and all others are 0","week 1 - slides_l1_spaces_lin1model.txt","week-1 machine-learning classification multi-class"
"20260123-1533-55-1","What is an informal definition of a tensor?","An ordered set of numbers which is indexable along several axes.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-2","What three key properties define a PyTorch tensor?","Shape vector (dimensions/counts), Data type (dtype), and Device placement.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-3","Why is using double precision (torch.float64) often discouraged in PyTorch?","It is slow on most GPUs.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-4","Which functions are used to save and load PyTorch tensors to/from disk?","torch.save() and torch.load().","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-5","What is the difference between torch.tensor(np_array) and torch.from_numpy(np_array)?","torch.tensor() copies the data, while torch.from_numpy() shares the same memory (making edits reflected in both).","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning numpy"
"20260123-1533-55-6","How does the ""joker"" index -1 behave in x.view(-1, 5)?","PyTorch automatically calculates the required dimension size based on the total number of elements.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-7","How can you restrict a Python script to use only a specific GPU on a multi-GPU system?","Set the environment variable CUDA_VISIBLE_DEVICES=N (where N is the GPU index).","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning cuda"
"20260123-1533-55-8","Why should you prefer broadcasting or built-in torch operations over nested for-loops?","Broadcasting is significantly faster than for-loops and avoids the overhead of interpreted Python loops.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning broadcasting"
"20260123-1533-55-9","What is the first rule of broadcasting when two tensors have different numbers of dimensions?","The smaller tensor is conceptually filled with singleton dimensions (size 1) from the left until it has the same dimensionality as the larger tensor.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning broadcasting"
"20260123-1533-55-10","What makes two tensors incompatible for broadcasting in a specific dimension?","If both tensors have sizes > 1 in that dimension and those sizes are not equal.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning broadcasting"
"20260123-1533-55-11","How is a dimension of size 1 handled during a broadcasting operation?","It is conceptually replicated/copied until it matches the size of the corresponding dimension in the other tensor.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning broadcasting"
"20260123-1533-55-12","What is the result shape of torch.dot(a, b) for two 1D tensors of size d?","A scalar (rank-0 tensor).","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning linalg"
"20260123-1533-55-13","What is the constraint for matrix multiplication torch.mm(A, B)?","A must be (i, k) and B must be (k, l); the inner dimensions must match. It does not support broadcasting.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning linalg"
"20260123-1533-55-14","What does torch.bmm(A, B) perform on two 3D tensors of shape (b, i, k) and (b, k, l)?","Batched matrix multiplication: it performs b independent matrix multiplications, one for each batch index.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning linalg"
"20260123-1533-55-15","What is the difference between torch.squeeze() and torch.unsqueeze()?","squeeze() removes singleton dimensions (size 1), while unsqueeze() inserts a singleton dimension at a specified axis.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-16","What is the difference between torch.transpose() and torch.permute()?","transpose() swaps exactly two dimensions, while permute() can reorder any number of dimensions simultaneously.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-17","What are the six typical steps in the computational flow of a PyTorch model?","1. Define Dataset/DataLoader, 2. Define Prediction Model, 3. Define Loss, 4. Define Optimizer, 5. Initialize Parameters, 6. Loop over epochs (train/validation).","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-18","Why is transforms.Normalize() applied to image data during training?","It centers data around 0 with a limited scale, making training more stable and gradients easier to optimize.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning transforms"
"20260123-1533-55-19","When should you use an Iterable-style Dataset instead of a Map-style Dataset?","For streaming or dynamic data sources (e.g., Kafka, database queries) where a fixed index is not readily available.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning data"
"20260123-1533-55-20","What is the primary role of the __getitem__ method in a Map-style Dataset?","It handles data loading and processing (e.g., loading an image from disk and applying transforms) for the i-th sample.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning data"
"20260123-1533-55-21","What is the primary role of the DataLoader in PyTorch?","It aggregates individual samples from a Dataset into minibatches for parallel processing and handles shuffling/parallel loading.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning data"
"20260123-1533-55-22","Why is it critical to use model.train() and model.eval() correctly?","Certain layers (like BatchNorm and Dropout) behave differently in training vs. evaluation mode; using the wrong mode leads to invalid results.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1533-55-23","What is the purpose of the with torch.no_grad(): context manager?","It disables gradient tracking, reducing memory usage and speeding up computations during validation or inference.","week 2 - slides_l2_pytorch_part1.txt","week-2 winter-2025-2026 pytorch machine-learning"
"20260123-1548-00-1","What are the four primary components of an artificial neuron's forward equation?","Inputs (zi), Weights (wij), Bias (bj), and Activation function (g).","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 neural-networks"
"20260123-1548-00-2","What are two critical requirements for an activation function to be effective in deep learning?","It must introduce non-linearity and be differentiable (for gradient-based optimization).","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 activation-functions"
"20260123-1548-00-3","What is the primary drawback of the logistic sigmoid activation function?","It suffers from gradient saturation (derivatives become extremely small for large inputs), making training difficult.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 activation-functions"
"20260123-1548-00-4","How is the ReLU activation function defined mathematically?","g(x) = max(0, x)","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 activation-functions"
"20260123-1548-00-5","What is a potential risk of using standard ReLU activations for negative inputs?","The gradient is 0 for negative inputs, which can cause nodes to become permanently ""inactive"" (dead neurons).","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 activation-functions"
"20260123-1548-00-6","What is the definition of the Leaky ReLU activation function?","g(x) = max(0.01x, x) (ensures a non-zero gradient for negative inputs).","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 activation-functions"
"20260123-1548-00-7","Why might spiking neural networks (SNNs) be important for future computing hardware?","They are significantly more energy-efficient than traditional ANNs, which is critical for scaling and deployment costs.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 SNN"
"20260123-1548-00-8","What is the general matrix-vector formulation for the activations of layer l in a fully connected network?","a(l) = g(W(l)a(l-1) + b(l)), where g is applied element-wise.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 neural-networks"
"20260123-1548-00-9","In the context of a single neuron, what do the weights w and bias b represent geometrically?","The weights w define the orientation of the separating hyperplane, and the bias b defines its position/shift.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 geometric-intuition"
"20260123-1548-00-10","What is the set of points x where a linear mapping f(x) = w · x + b is exactly zero?","A hyperplane orthogonal to the weight vector w.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 geometric-intuition"
"20260123-1548-00-11","How does a negative bias (b < 0) affect the position of the hyperplane w · x + b = 0?","It shifts the hyperplane in the direction of the weight vector w.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 geometric-intuition"
"20260123-1548-00-12","How can you find a vector u that is orthogonal to w (w · u = 0)?","By subtracting the component of any vector x that is parallel to w: u = x - (x · w/||w||) * w/||w||.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 geometric-intuition"
"20260123-1548-00-13","What does the XOR problem demonstrate about single-layer linear models?","A single hyperplane cannot separate regions that are not linearly separable (like XOR data in four quadrants).","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 xor-problem"
"20260123-1548-00-14","How does a 2-layer network with n hidden neurons represent a convex shape?","Each hidden neuron represents an edge (hyperplane), and the second layer performs a thresholded sum/intersection of these half-spaces.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 neural-networks representability"
"20260123-1548-00-15","What is required for a 2-layer network to approximately encode any union of convex shapes?","A third layer is typically required.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 neural-networks representability"
"20260123-1548-00-16","What is the core claim of the Universal Approximation Theorem?","A neural network with one hidden layer can approximate any continuous function on a compact hypercube arbitrarily well.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 UAT"
"20260123-1548-00-17","Why was the Universal Approximation Theorem considered ""misleading"" for early research?","The ability to approximate any function does not imply the ability to learn it efficiently from finite, noisy training data without overfitting.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 UAT"
"20260123-1548-00-18","What is the primary risk of using a model with high representational power on finite training data?","Overfitting: the model might perfectly memorize the training set (including noise) but fail to generalize to new data.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 optimization"
"20260123-1548-00-19","Name two methods used to restrict models and prevent overfitting despite their universal approximation capability.","Convolutional layers (locally restricted operators) and regularizations on gradient flow.","week 2 - slides_l4_nn_linmodel.txt","week-2 winter-2025-2026 optimization"
