links

know where to look for
âŠ™ http://neuralnetworksanddeeplearning.com/ Chapter 1
âŠ™ d2l.ai Chapter 3 and Chapter 4
âŠ™ https://numpy.org/doc/

|1

What you will see today

âŠ™ Artificial Neurons
âŠ™ Neural networks
âŠ™ theory: understand what that linear model is computing
âŠ™ how neural networks can represent decision boundaries in classification

|2

Outline

1 Artificial neurons
2 Neural Networks
3 Understanding the output of the linear model
4 Example: The XOR problem
5 More on representability

|3

What is an artificial neuron?

|4

A mathematical function that for instance can:
âŠ™ Take some inputs {zi }di=1 .
âŠ™ Depend on some weights wij and bias bj .
âŠ™ Apply some non-linear activation function g(Â·).
âŠ™ Output zj .
Forward equation:
aj =

d
X

zi wij + bj

linear mapping

i=1

zj = g(aj )

nonlinear activation

Activation functions

âŠ™ A function g : R1 âˆ’â†’ R1
âŠ™ Introduces non-linearity.
âŠ™ Should be differentiable if you are planning to use gradient-based optimisation.
âŠ™ Typically prevents or reduces weak signals from passing through.
âŠ™ Intuition: Determines what the neuron fires for different outputs of the linear mapping.

|5

Sigmoid activation

|6

We could for instance use the logistic sigmoid
function:
g(x ) =

1
exp(âˆ’x ) + 1

âŠ™ Called sigmoid activation.
âŠ™ Historically popular.
âŠ™ Currently rarely used.

Suffer from gradient saturation: its derivative gets exponentially small for large |x |:
(
e âˆ’x
x â‰¥0
âˆ‚s(x )
e âˆ’x
= âˆ’x
â‰¤
1
2
âˆ‚x
(e + 1)
x <0
e âˆ’x +1

Sigmoid activation

|6

We could for instance use the logistic sigmoid
function:
g(x ) =

1
exp(âˆ’x ) + 1

âŠ™ Called sigmoid activation.
âŠ™ Historically popular.
âŠ™ Currently rarely used.

Suffer from gradient saturation: its derivative gets exponentially small for large |x |:
(
e âˆ’x
x â‰¥0
âˆ‚s(x )
e âˆ’x
= âˆ’x
â‰¤
1
2
âˆ‚x
(e + 1)
x <0
e âˆ’x +1

Sigmoid activation

|6

We could for instance use the logistic sigmoid
function:
g(x ) =

1
exp(âˆ’x ) + 1

âŠ™ Called sigmoid activation.
âŠ™ Historically popular.
âŠ™ Currently rarely used.

Suffer from gradient saturation: its derivative gets exponentially small for large |x |:
(
e âˆ’x
x â‰¥0
âˆ‚s(x )
e âˆ’x
= âˆ’x
â‰¤
1
2
âˆ‚x
(e + 1)
x <0
e âˆ’x +1

Rectified linear unit (ReLU) activation

An alternative is to simply set negative values to 0,
i.e. let:
g(x ) = max(0, x )
âŠ™ Called ReLU activation.
âŠ™ Gradient do not saturate.
âŠ™ Not differentiable in 0.
Â· Not of practical concern.
Â· Could define it to be 1.

|7

Rectified linear unit (ReLU) activation

|8

The gradient is 0 for negative inputs.
âŠ™ Might cause inactive nodes to remain inactive.
âŠ™ Might still work fine in practice.
âŠ™ There exists alternatives with non-zero
gradient for negative inputs, e.g. leaky ReLU:
g(x ) = max(0.01x , x )
A popular choice, particularly with convolutional
neural networks.
ReLU, not leaky one

Artificial vs biological neurons

Artificial (non-spiking) neurons as used in Deep Learning are not a model of biological
neurons. not even close!.

|9

Artificial vs biological neurons
âŠ™ Biological neurons are spiking neurons with time-dependency.
âŠ™ Fire spikes, if their voltage (= difference in energy compared to the outside) has increased over a
threshold. The voltage can be increased by currents from the outside. Current = flow rate of
charged particles (Na+ , K+ ions in biological systems, electrons in metals).
âŠ™ Biological models often model the voltage over time as a function of incoming currents (and
voltage decay, a neuron is not a perfect storage for energy)
âŠ™ the formulas are not for exams:
It
Vt âˆ’ EL
âˆ†t +
âˆ†t
Ï„m
Ï„m gL
if Vt+âˆ†t > V0 : emit spike and reset Vt+2âˆ†t := EL

LiF: = Vt+âˆ†t âˆ’ Vt = âˆ’

âˆ†t - small time amount, V - Voltage, I - incoming current, EL - resting potential, Ï„ membrane
time constant (how fast Vt decays), gL Leak conductance
For examples see: https://compneuro.neuromatch.io/tutorials/W2D3 BiologicalNeuronModels/
student/W2D3 Tutorial1.html

| 10

Compare these teo

| 11

Compare above formulation against artificial
neurons
âŠ™ spiking NNs: voltage over time steps. Current
spikes.
âŠ™ ANNs:
Â· one-step function computation
(but layers as weak analogy to time
steps)
Â· Affine mapping + nonlinearity as basic
building block
Â· no physical measures.

aj = g(w Â· z + b)

Artificial vs biological neurons

Spiking neural network architectures might be the next generation of computing hardware (more likely
than the quantum compute hype) because systems using them might use dramatically less energy.
Energy consumption is a deployment cost issue for deep learning models:
https://www.wionews.com/business-economy/
chatgpt-costs-700000-daily-creator-openai-may-go-bankrupt-by-2024-report-625306
https://www.firstpost.com/tech/news-analysis/
openai-may-go-bankrupt-by-2024-chatgpt-costs-company-700000-dollars-every-day-12986012.html.
For a spiking neuron toolbox:
https://snntorch.readthedocs.io/en/latest/tutorials/tutorial 1.html

| 12

Outline

1 Artificial neurons
2 Neural Networks
3 Understanding the output of the linear model
4 Example: The XOR problem
5 More on representability

| 13

Neural network

A neural network is a directed graph structure made from connected neurons.
âŠ™ A neuron can be input to many other neurons.
âŠ™ A neuron can receive input to many other neurons.
P
âŠ™ Each neuron has same structure (g(Â·), ) but different parameters (wij , bj ).
âŠ™ Can stack neurons in layers.

| 14

Neural network

| 15

Definition: Neural Network
Any directed graph built from neurons is a neural network.
Two important types: recurrent and feedforward neural networks

recurrent (not covered in this lecture)
for sequence processing

feedforward
e.g. for image classification

Simplest Architecture: Fully connected Neural network
layer instead of general graph
For computation efficiency, one usually organizes neurons in layers .
For the simplest type of networks: one layer is fully connected to the next layer.

| 16

Base architecture

| 17

Nodes and layers

| 18

Dense (fully-connected) feedforward neural network

| 19

General formulation: using linear algebra

| 20

Each neuron (circle) computes this:
output = activation(inner product(w, input) + bias)
(l)



(l)

(l)

ak = g w [:,k] Â· a (lâˆ’1) + bk



,

(k âˆˆ {1, 2, . . . , n[l] }, l âˆˆ {1, 2, . . . , L})

General formulation

| 21

(l)



(l)

(l)

ak = g w [:,k] Â· a (lâˆ’1) + bk
(l)

(l)



(l)

output ak for a single neuron is g(zk ) where zk is the result from an inner product between
âŠ™ the activation vector a (lâˆ’1) of the previous layer l âˆ’ 1
(l)

âŠ™ the k-th slice w [:,k] of the weight matrix w (l) of the current layer l
(l)

âŠ™ added a bias term bk to the inner product

General formulation

| 22

(l)



(l)

(l)

ak = g w [:,k] Â· a (lâˆ’1) + bk
(l)



âŠ™ if one concatenates all outputs ak of layer (l) , and applies g(Â·) element-wise, then:


a (l) = g W (l) a (lâˆ’1) + b (l)
âŠ™ where W (l) a (lâˆ’1) is a matrix-vector multiplication

What is happening in the hidden layer nodes



a (l) = g W (l) a (lâˆ’1) + b (l)

| 23



âŠ™ The W and b are â€œtrainableâ€, and will be adjusted according to some optimization

routine.
âŠ™ By convention:
Â· ak(0) = xk and ak(L) = yÌ‚k
Â· The network has L layers (we do not include the input layer in the count) and L âˆ’ 1 hidden
layers.

What is happening in the hidden layer nodes



a (l) = g W (l) a (lâˆ’1) + b (l)

| 23



âŠ™ The W and b are â€œtrainableâ€, and will be adjusted according to some optimization

routine.
âŠ™ By convention:
Â· ak(0) = xk and ak(L) = yÌ‚k
Â· The network has L layers (we do not include the input layer in the count) and L âˆ’ 1 hidden
layers.

What does a neuron compute?

âŠ™ What does a neuron compute?
âŠ™ What could a neural network compute?

| 24

Outline

1 Artificial neurons
2 Neural Networks
3 Understanding the output of the linear model
4 Example: The XOR problem
5 More on representability

| 25

What does a neuron compute? / What does the affine model actually
represent?

f (x ) = w Â· x + b
Next: What does the linear model actually represent / compute, once we have a (w , b) ?

| 26

What does a neuron compute?

| 27

w
d
oi
t
co
ns

gm
va

lu

e

of

Si

Re aj
LU

j

a

z2

st

-b

n
co

z3
(0,0,0)

z1

âŠ™ the red plane are the set of points where f (x ) = const
âŠ™ The weights w defines the orientation of the plane, b shifts position of it.
âŠ™ Output of activation function is constant in the red plane.

What does a linear mapping represent?
Goal: understand what the mapping f (Â·) does.
Approach: characterizing the set of points x with a constant output f (x ) = c.
Thinking task
What is the set of points x = (x1 , x2 ) âˆˆ R2 such that
3x1 âˆ’ 2x2 + 3 = 0 ?
What is the set of points x = (x1 , x2 , x3 ) âˆˆ R3 such that
x1 âˆ’ x2 âˆ’ 2x3 + 2 = 0 ?
A. What is the set of points x : fw ,b (x ) = 0 ?
B. What is the set of points x the prediction is a constant c, that is fw ,b (x ) = c ?

| 28

What is the set of points x : fw ,b (x ) = 0?

fw ,b (x ) = 0 â‡” x Â· w = âˆ’b
To understand how the bias b influences the zero set, lets consider three cases:
âŠ™ b=0
âŠ™ b>0
âŠ™ b<0

| 29

The set of points x : fw ,b (x ) = 0 â€“ The case b = 0
âŠ™ We know that for b = 0: fw ,0 (x ) = w Â· x = 0 holds for the zero vector x = 0.
âŠ™ The set of points x such that the inner product
x Â·w =0
is in 2 dims a one-dimensional line, which goes through the origin (x1 , x2 ) = (0, 0), and which is
orthogonal to w .

w

x1 ,x2 ,x3 are all orthogonal to the
vector w

x2
x1
x3

hyperplane orthogonal to w

| 30

The set of points x : fw ,b (x ) = 0 â€“ The case b = 0
The analogy also holds for 3 or more dimensions. So for 3 dims it is a two-dimensional plane,
which goes through the origin (x1 , x2 , x3 ) = (0, 0, 0).
x2

w

x1

hyperplane
through origin
orthogonal to w
vectors x1 and x2 lie inside
this hyperplane

For n dimensions the plane of orthogonal vectors has n âˆ’ 1 dimensions (+ goes through the
origin), but is still a hyperplane

| 31

The set of points x : fw ,b (x ) = 0 â€“ The case b = 0

| 32

Recap hyperplane of dimension n âˆ’ 1
âŠ™ P is a hyperplane (affine space) if it holds:

x1 âˆˆ P, x2 âˆˆ P, o âˆˆ P â‡’ a1 (x1 âˆ’ o) + a2 (x2 âˆ’ o) + o âˆˆ P (space closed under
linear operations)
âŠ™ can find n âˆ’ 1 basis vectors such that each point of P can be represented as an

offset plus a linear combination of the basis vectors
âˆƒ o, v1 , . . . , vnâˆ’1 such that
âˆ€x âˆˆ P âˆƒ a1 , . . . , anâˆ’1 such that x = o +

nâˆ’1
X

ai vi = a Â· V

i=1

âŠ™ o is any vector which lies on the hyperplane
âŠ™ If the affine hyperplane is a vector space, then it contains the zero-vektor, and one can

choose o = 0, then the definition simplifies to x1 âˆˆ P, x2 âˆˆ P â‡’ a1 x1 + a2 x2 âˆˆ P

The set of points x : fw ,b (x ) = 0 â€“ The case b < 0

| 33

b < 0, w Â· x + b = 0 â‡’ w Â· x = âˆ’b > 0
We know: w Â· x > 0 for all points x that are on that side of the hyperplane through the
origin orthogonal to w , in which w points to.
hyperplane through origin
orthogonal to w, wx=0 there

w
x1

x2
x3

In the above figure x1 ,x2 ,x3 all have w Â· xi > 0 because relative to the hyperplane orthogonal
to w which goes through the origin, they all point towards the direction of w

The set of points x : fw ,b (x ) = 0 â€“ The case b < 0
b < 0, w Â· x + b = 0 â‡’ w Â· x = âˆ’b > 0
We know: w Â· x > 0 for all points x that are on that side of the hyperplane through the
origin, in which w points to.
The same also holds for 3 or more dimensions. All the vectors below solve wx + b = 0 for
some bias b < 0!
x1
w

x2
x3

hyperplane
through origin
orthogonal to w

In above figure x1 ,x2 ,x3 all have w Â· xi > 0

| 34

The set of points x : fw ,b (x ) = 0 â€“ any bias

| 35

The bias b shifts the hyperplane corresponding to w Â· x + b = 0 parallel/anti-parallel to the
direction of w .

wx+b=0
b<<0

wx+b=0
b>>0

w

wx=0

zone:
wx<0

zone:
wx>0

the hyperplane is parallel to the hyperplane orthogonal to w which goes through the origin

The set of points x : fw ,b (x ) = 0 â€“ The case b < 0

The set of points x such that
{x : w Â· x + b = 0}
for b < 0 is a hyperplane which is parallel to the hyperplane {x : x Â· w = 0} orthogonal to w
going through the origin, and which is shifted towards the direction of w

| 36

The set of points x : fw ,b (x ) = 0 â€“ The case b > 0

The set of points x such that
{x : wx + b = 0}
for b > 0 is a hyperplane which is parallel to the hyperplane {x : x Â· w = 0} orthogonal to w
going through the origin, and which is shifted opposite to the direction of w

| 37

The set of points x : fw ,b (x ) = 0 â€“ any bias

hyperplane dependency on bias b
âŠ™ {x : w Â· x = 0} is a hyperplane orthogonal to w , which contains the zero vector

(0, 0, . . . , 0).
âŠ™ Negative b < 0 shift the hyperplane {x : wx + b = 0} towards the direction of w ,
âŠ™ positive b > 0 shift the hyperplane {x : wx + b = 0} against the direction of w .
âŠ™ Large absolute values |b| shift it far away.

| 38

The set of points x : fw ,b (x ) = 0 â€“ any bias

| 39

hyperplane explicit
The linear mapping f (x ) = w Â· x + b is zero for x which consist of the plane of points


w
x : x = u + âˆ’b
, u such that w Â· u = 0
âˆ¥w âˆ¥2



In this representation:
âŠ™ u such that w Â· u = 0 is the hyperplane of vectors u orthogonal to w .
âŠ™ the vector âˆ’b âˆ¥wwâˆ¥2 shifts the hyperplane in direction of Â±w .
That holds because w Â· u = 0 and w Â· w = âˆ¥w âˆ¥22 :
w
+b
w Â· x + b = w Â· u + âˆ’b
âˆ¥w âˆ¥2
w Â·w
=w
| {zÂ· u} âˆ’b âˆ¥w âˆ¥2 + b = 0 âˆ’ b + b = 0


=0



How to find a vector u such that w Â· u = 0 ?

By subtracting its component parallel to w !
Be x any vector. Subtract its component parallel to w .
w
Important: use length-normalized w : âˆ¥w
âˆ¥





w
w
x Â· âˆ¥w
âˆ¥ is the component of x in direction of âˆ¥w âˆ¥



u=xâˆ’ xÂ·

w
âˆ¥w âˆ¥



w
1
= x âˆ’ (x Â· w )
w
âˆ¥w âˆ¥
âˆ¥w âˆ¥2

â‡’uÂ·w =0
cf. QR-decomposition to find an orthonormal basis of such vectors.

| 40

What is the set of points x where the prediction is a constant, that is
gw ,b (x ) = c ?

Answered by reducing it to a zero set:
gw ,b (x ) = wx + b = c
wx + (b âˆ’ c) = 0
The set of points x such that gw ,b (x ) = c is just the set x : gw ,bâˆ’c (x ) = 0.

| 41

Takeaway

The points x such that w Â· x + b = c is a hyperplane orthogonal to w , shifted in direction of w by the
amount of (c âˆ’ b) âˆ¥wwâˆ¥2 .
given this, the next slide should be clear:

| 42

What does a neuron do?

| 43

in short:

w
d
oi
st
co
n

gm
va
lu

e

of

Si

Re aj
LU

j

a

z2

st

-b

n
co

z3
(0,0,0)
âŠ™ The weights w defines the orientation of the plane.
âŠ™ The bias b defines the position of the plane.

z1

You should be able to solve that now

thinking task
What is the set of points x = (x1 , x2 ) âˆˆ R2 such that
3x1 âˆ’ 2x2 + 3 = 0 ?
What is the set of points x = (x1 , x2 , x3 ) âˆˆ R3 such that
x1 âˆ’ x2 âˆ’ 2x3 + 2 = 0 ?

| 44

You should be able to solve that now

| 45

0.6
0.4
0.2
0.0
0.2
0.4
0.6
0.6

0.4

0.2

0.0

0.2

What would be a separating hyperplane in this case ?

0.4

0.6

You should be able to solve that now

| 46

0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
2.4

2.2

2.0

1.8

1.6

What would be a separating hyperplane in this case ?

1.4

1.2

Outline

1 Artificial neurons
2 Neural Networks
3 Understanding the output of the linear model
4 Example: The XOR problem
5 More on representability

| 47

The XOR problem

To illustrate how concatenation of neurons allows learning non-linear mappings, consider
separating red from blue samples in the following example:

Samples lie in quadrants around coordinates (Â±1, Â±1).

| 48

The XOR problem

z1 = tanh(10x1 )
z2 = tanh(10x2 )

âŠ™ tanh pushes points towards

(Â±1, Â±1)

| 49

The XOR problem

| 50

Idea:

âŠ™ For proper choice of w1 and b1 , g1 = Ïƒ(w1 z + b1 ) has
high values â‰ˆ 1 in the upper right and low values â‰ˆ 0
elsewhere.

âŠ™ For proper choice of w2 and b2 , g2 = Ïƒ(w2 z + b2 ) has
high values â‰ˆ 1 in the lower left and low values â‰ˆ 0
elsewhere.

âŠ™ Then, g1 + g2 will be â‰ˆ 1 in the upper right and lower left corners.
âŠ™ In the middle zone, g1 + g2 will be â‰ˆ 0.

The XOR problem

âŠ™ w1 = (?, ?) , w2 = (?, ?)

| 51

The XOR problem

| 52

âŠ™ If w1 = (1, 1), w2 = (âˆ’1, âˆ’1), then

w1 Â· z â‰ˆ 0 , w2 Â· z â‰ˆ 0 for the middle zone,
while w1 Â· z â‰« 0 in the upper right corner and
w2 Â· z â‰« 0 in the lower left corner.
âŠ™ Could then choose b1 < 0 and b2 < 0 suitably.
âŠ™ then for the middle zone: both g1 (z) â‰ˆ 0 and

g2 (z) â‰ˆ 0
(exact 0 if using hard thresholding neuron)
âŠ™ outer zones: one of the g1 , g2 will be â‰ˆ 1
âŠ™ Final output: f (x ) = Ïƒ(c(g1 + g2 ) + cb3 )

The XOR problem

| 53

âŠ™ Then, g1 + g2 â‰ˆ 1 in the upper right and lower left corners.
âŠ™ In the middle zone, g1 + g2 â‰ˆ 0
âŠ™ add a simple third layer with weights 1,1 and bias âˆ’0.5:
y = thresh(g1 + g2 âˆ’ 0.5)

Outline

1 Artificial neurons
2 Neural Networks
3 Understanding the output of the linear model
4 Example: The XOR problem
5 More on representability

| 54

Polygonal shapes

âŠ™ Use threshold activation (for simplicity):

âŠ™ A 2-layer network with n neurons in the hidden

layer, and a thresholded sum in the second layer can
represent any convex polygonal shape with n edges.
âŠ™ With n â†’ âˆž neurons in the first layer, any convex

shape can be approximately encoded.

| 55

Polygonal shapes

âŠ™ Adding a third layer allows the neural network to

approximately encode any union of convex shapes.
âŠ™ However, being able to (approximately) represent

does not imply that this would be the result when
learning from finite data.

| 56

Universal approximation theorem

| 57

Universal approximation theorem (a version)
Let g(Â·) be a continuous function on a m-dimensional hypercube [0, 1]m . Let a(Â·) be
a non-constant, bounded, continuous (activation) function. Then g(Â·) can be approximated arbitrarily well, that is, for every maximal deviation Ïµ > 0, there exists a set of
weights ui , wi and biases bi such that:
âˆ€x âˆˆ [0, 1]m : |g(x ) âˆ’ (

X

ui a(wi x + bi ) + b)| < Ïµ

i=1

Neural networks with one hidden layer and two layers of weights is able to approximate any
smooth function on a compact hypercube. If there is a good algorithm for learning the
parameters from finite data, we are done!

Universal approximation theorem

| 58

Hinton & Co.
âŠ™ Universal approximation theorem was

misleading for neural network research in
the 90s.
âŠ™ Kolmogorov (1957), Hornik (1989),

Cybenko (1989) and others: Neural
network is able to approximate any
continuous function on a compact region.
the usual 90s
neural net
researcher

Universal approximation theorem

âŠ™ Kolmogorov (1957), Hornik (1989), Cybenko (1989) and others: Neural network is able

to approximate any continuous function on a compact region.
âŠ™ However, ability to approximate any function Ì¸= ability to learn any function well from

finite training data (... overfitting)

| 59

Universal approximation theorem

| 60

4

4

learn from
nite training data

3

2

3

2

1

1

0

0

âˆ’

âˆ’

âˆ’

1

âˆ’

2

âˆ’

3

âˆ’

4

1

2

âˆ’

3

âˆ’

4

âˆ’

3

âˆ’

2

âˆ’

1

0

1

2

risk: overtting to
specics of the
nite training set

3

3

âˆ’

âˆ’

2

âˆ’

1

0

1

2

3

4

3

2

1

0

âˆ’

1

âˆ’

2

âˆ’

3

âˆ’

4

âˆ’

3

âˆ’

2

âˆ’

1

0

1

2

3

Right because you can represent any shape, you will easily reserve a blue area for the blue point in the
training data on the wrong side. This results in overfitting. need to restrict models to prevent this!!!
âŠ™ convolution layers as locally restricted linear operators
âŠ™ regularizations on gradient flow

