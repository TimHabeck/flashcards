L7 – Gradients and Gradient Descent II
Alexander Binder
November 4, 2025

links

know where to look for
⊙ d2l.ai Appendix A.3 and A.4
⊙ https://pytorch.org/

|2

Takeaway points

Linear and Bilinear mappings are the most common in deep learning. Combinations of these together
with activation functions can be dealt with using the chain rule.

|3

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

|4

Gradient Descent Algorithm

Gradient Descent
Basic Algorithm: name: Gradient Descent:
⊙ given: step size parameter η, initialize start vector u (0) to a value.
⊙ run while loop, until function value changes (δL ) drop below a threshold, do at iteration t:
· u (t+1) = u (t) − η∇(u) L(u (t) )
· compute change of objective to last value: δL = ∥L(u (t+1) ) − L(u (t) )∥
questions:
⊙ sensitivity to starting point
⊙ sensitivity to learning rate
⊙ quality of obtained solutions

|5

Gradient Descent: sensitivity to starting point

Lets explore starting point effects:
⊙ in learnThu8.py run tGD2([initvalue]) with initvalue ∈ [−4, +4] to see the effect of a
constant stepsize, but different starting points – see in what minimum you end up.
possible problems of gradient descent I
⊙ we find a local minimum, not the global minimum of a function. Local optimum can be
good or bad.
⊙ effects of starting point – for non-convex functions: different starting point leads to
possibly different solutions u ∗ .

|6

Gradient Descent: sensitivity to starting point
Lets explore starting point effects:
⊙ in learnThu8.py run tGD2([initvalue]) with initvalue ∈ [−4, +4] to see the effect of a
constant stepsize, but different starting points – see in what minimum you end up.
solutions with respect to starting point
⊙ Do not train from scratch for any practical applications. ALWAYS fine-tune a neural net
pretrained on a large corpus like ImageNet (later lecture)
⊙ if – in rare cases – one would train from scratch (e.g. a totally new architecture), then
careful initialization, and special learning rate treatments are important (later lecture)
see e.g. https://arxiv.org/abs/1502.01852 for examples how initialization matters for training deep
neural networks
training deep neural networks from scratch without care about initialization can result in very bad
performance

|7

Gradient Descent: sensitivity to learning rate

Lets explore learning rate effects:
⊙ in learnThu8.py run tGD([stepsize]) to see the effect of different stepsizes.
effects of bad stepsize choices:
⊙ too large ⇒ divergence/no solution.
⊙ too small ⇒ slow convergence

|8

Gradients

|9

possible problems of gradient descent
⊙ the size of the update step ut+1 = ut − η∇u L(ut ) depends on the norm of the

gradient ∥∇L(u)∥, too. So when starting in a steep region (∥∇L(u)∥ is large),
even a small stepsize can lead to divergence.

large, f'(x) large

small, f'(x) small

learning rate adjustment

| 10

⊙ one way to deal with the question of how to set the stepsize, is to reduce it over time:
learning rate adjustment schemes / learning rate annealing schemes
In practice: one starts with a learning rate, and decreases it over time, either with a polynomial
decrease, or by a factor every N iterations.
polynomial: λ(t) = c0 ∗
regular step at each T : λ(t) = c0 ∗
in code:
pytorch: torch.optim.lr scheduler
a1

1

What happens if one decreases the learning rate very fast?

(t + 1)−α , α > 0
c ⌊t/T ⌋ , c ∈ (0, 1)

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

| 11

Batch gradient descent

| 12

Consider setting with an average of losses over all training samples, and a predictor which depends
on trainable parameters u:
n

1X
L(fu (xi ), yi )
n i=1
The application of gradient descent to a loss computed over all training samples results in the following
algorithm:
!
n
1X
ut+1 = ut − η∇u
L(fut (xi ), yi )
n i=1
This is called batch gradient descent because it uses the set of all training data samples to
compute the gradient in each step.

Stochastic gradient descent / minibatches
The alternative is stochastic gradient descent (SGD). This is the default in deep learning.
Stochastic gradient descent for an average of losses
The core idea of Stochastic gradient descent is to compute the gradient only over a randomly
selected subset of all training samples. After updating the parameters, one draws a new randomly
selected subset of all training samples for gradient computation.
SGD in practice
⊙ shuffle/permute your training data {zi = (xi , yi ), i = 0, . . . , n − 1}2 ,
⊙ iterate over minibatches , computing the gradient of the loss in each iteration – until all data has
been used once
⊙ repeat the two above steps
For example, stochastic gradient descent when starting at index m and using the next k samples uses
as update:
m+k−1
1 X
L(fut (xi ), yi )
ut+1 = ut − η∇u
k i=m+0
2

make sure that features x and labels y of one sample (x , y ) will stay together!!

| 13

Gradients

| 14

stochastic gradient descent for an average of losses
⊙ initialize start vector u0 as something, choose step size parameter η
⊙ shuffle/permute your training data {zi = (xi , yi ), i = 0, . . . , n − 1}
⊙ until all data has been used once:
· compute the gradient of the loss for the next k samples (here it starts at an index m)
· apply it to update the parameters of the mapping f :
ut+1 = ut − η∇u

m+k−1
1 X
L(fut (xi ), yi )
k i=m+0

⊙ measure loss on validation data . If low enough, stop.
⊙ otherwise repeat from the shuffle step

Advantages of stochastic gradient descent

⊙ Full-batch is often too costly to compute a gradient using all samples when its more than

tens of thousands
⊙ SGD is a noisy, approximated version of the batch gradient (it is based on a random

subset, that causes the noise).
· injecting small noise is one way to prevent overfitting! For deep neural networks SGD can be
better than full batch gradient descent in finding good local optima.

| 15

Advantages of stochastic gradient descent

⊙ An illustration why noise to the loss function (e.g. by randomized sampling of training

batches) may help to jump out of bad local optima

Left: a loss surfaces at some point (orange). Middle and Right: changes in the loss surfaces as different
training data subsets are used. In the right it allows to jump out of the local minimum.

| 16

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

| 17

Why gradient descent is hard in practice?

What makes it hard to use gradient descent in deep neural networks?
⊙ one cannot simply stack convolution layers: see for example Fig 1 in

https://arxiv.org/abs/1512.03385
⊙ the problem of vanishing or exploding gradients
· the graphics on gradient magnitudes in
http://neuralnetworksanddeeplearning.com/chap5.html

| 18

Challenge 1: vanishing gradients

What makes it hard to use gradient descent in deep neural networks?
⊙ vanishing gradients: scale of gradient gets smaller and smaller as neural network

architectures become deeper - in particular in layers closer to the input

| 19

Challenge 2: exploding gradients

What makes it hard to use gradient descent in deep neural networks?
⊙ exploding gradients: scale of gradient gets unbounded as neural network architectures

become deeper, if learning rates are too high

| 20

Challenge 3: imbalanced gradient scales

What makes it hard to use gradient descent in deep neural networks?
⊙ imbalanced gradient scales (e.g. Bjorck et al, Understanding Batchnorm

https://arxiv.org/abs/1806.02375): the scale of gradients has large variation across
neurons - this means some neurons have faster updates than others

| 21

Challenge 3: imbalanced gradient scales

What makes it hard to use gradient descent in deep neural networks?
⊙ You should know about vanishing/exploding gradients and imbalanced gradient scales
⊙ you should know that gradient descent does not guarantee convergence in every case,

due to effects of starting points and learning rate scales

| 22

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

| 23

recap: compute a gradient

How to, way 1: see gradtest1.py
⊙ create tensors a,b,c explicitly (eg create explicitly using torch.randn, or return them from a
dataloader)
⊙ set requires_grad=True on those tensors
· leaf tensors:
https://docs.pytorch.org/docs/2.7/generated/torch.Tensor.is leaf.html#torch.Tensor.is leaf
⊙ run operations to obtain a scalar tensor res computed from these leaf tensors
⊙ res.backward() computes the gradients of res with respect to all leaf tensors used in its
computation
· inputs to neural networks and weights (due to manual creation, and wrapping as
torch.nn.Parameter) are usually leaf tensors

| 24

compute a gradient using torch.autograd.grad

How to, way 2: see gradtest2.py
https://docs.pytorch.org/docs/stable/generated/torch.autograd.grad.html
⊙ still need to set requires_grad=True on those tensors
⊙ more flexible: can compute grad for multiple outputs ... but
⊙ still need scalar outputs (or use grad_outputs parameter )

| 25

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

| 26

Gradient descent in PyTorch

see fmnist pytorch logreg class.py
⊙ Step 1: wrap trainable tensors in class torch.nn.Parameter
· sets requires grad flag
· allows to return all parameters of a model using one function
· together with buffers will be included in the state dict when saving model parameters
class onelinear(torch.nn.Module):
def __init__(self,dims, numout):
super().__init__() #initialize base class
self.bias=torch.nn.Parameter(data=torch.zeros(numout), requires_grad=True)
self.w=torch.nn.Parameter(data=torch.randn( (dims,numout) ), requires_grad=True)

| 27

Gradient descent in PyTorch

see fmnist pytorch logreg class.py
⊙ Step 2: hand over all parameters meant to be trained to the optimizer
model = onelinear(indims,numcl).to(device)
[...]
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)

| 28

Gradient descent in PyTorch

see fmnist pytorch logreg class.py

for batch_idx, data in enumerate(trainloader):
inputs=data[0].to(device)
labels=data[1].to(device)
outputs = model(inputs)
loss = criterion(outputs, labels)
optimizer.zero_grad() #reset accumulated gradients
loss.backward() #compute new gradients
optimizer.step() # apply new gradients to change model parameters

| 29

Gradient descent in PyTorch
see fmnist pytorch logreg class.py

for batch_idx, data in enumerate(trainloader):
inputs=data[0].to(device)
labels=data[1].to(device)
outputs = model(inputs)
loss = criterion(outputs, labels)
optimizer.zero_grad() #reset accumulated gradients
loss.backward() #compute new gradients
optimizer.step() # apply new gradients to change model parameters

⊙ step 3: compute model prediction, and loss from the prediction
⊙ Step 4: optimizer.zero_grad() to erase all previously computed gradients in all instances of
class torch.nn.Parameter which were passed to the optimizer
⊙ Step 5: loss.backward() to get new gradients in the .grad fields of all instances of
class torch.nn.Parameter
⊙ Step 6: optimizer.step() to Perform a variant of Gradient descent
(using the .grad fields of class torch.nn.Parameter to update the parameter values)

| 30

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

| 31

Chainrule via matrix multiplication

We had for two mappings, that the chainrule tells us that we have to concate the three linearizations:
D(g ◦ h)|x [v] = Dg|h(x) [Dh|x [v]]
We had for three mappings, that the chainrule tells us that we have to concate the three linearizations:
D(f ◦ g ◦ h)|x [v] = Df|g◦h(x) [Dg|h(x) [Dh|x [v]]]
We know for a real-valued function f (x ) ∈ R that Df |x [v ] = ∇f |x · v

| 32

Chainrule via matrix multiplication

| 33

⊙ We know for a real-valued function f (x ) ∈ R that Df |x [v ] = ∇f |x · v
if we write ∇f |x as a column vector, then
 ∂f


∇f |x = 





∂x0 |x

∂f
∂x1 |x 

..
.

∂f
∂xk−1 |x





when writing this as matrix-multiplication, we obtain:
Df |x [v ] = [∇f |x ]⊤ v
⊙ this extends naturally to vector-valued outputs f (x ) = (f0 (x ), . . . , fr −1 (x )) and the corresponding
Jacobi-matrix
Jf |x = (∇f0 |x , . . . , ∇fr −1 |x )

Chainrule via matrix multiplication

| 34

⊙ this extends naturally to vector-valued outputs f (x ) = (f0 (x ), . . . , fr −1 (x )) and the corresponding
Jacobi-matrix
Jf |x = (∇f0 |x , . . . , ∇fr −1 |x )
 ∂f0
∂f1
∂f2
∂x0 |x
∂x0 |x
∂x0 |x
∂f1
∂f2
 ∂f0 |
 ∂x1 x
∂x1 |x
∂x1 |x
 ∂f0
∂f1
∂f2
|
=
∂x2 |x
∂x2 |x
 ∂x2 x
..
 ..
..
 .
.
.
∂f0
∂f1
∂f2
|
|
∂xk−1 x
∂xk−1 x
∂xk−1 |x
Df |x [v] = [Jf |x ]⊤ v
⊙ this can be directly applied to the chainrule

···
···
···
···
···

∂fr −1 
∂x0 |x
∂fr −1 
∂x1 |x 
∂fr −1 

∂x2 |x

..
.

∂fr −1
∂xk−1 |x





Chainrule via matrix multiplication

| 35

Jf |x = (∇f0 |x , . . . , ∇fr −1 |x )
 ∂f0
∂f1
∂f2
∂x0 |x
∂x0 |x
∂x0 |x
∂f
∂f
∂f2
 0|
1
 ∂x1 x
∂x1 |x
∂x1 |x
 ∂f0
∂f1
∂f2
|
=
∂x2 |x
∂x2 |x
 ∂x2 x
..
 ..
..
 .
.
.
∂f1
∂f2
∂f0
|
|
∂xk−1 x
∂xk−1 x
∂xk−1 |x

···
···
···
···
···

∂fr −1 
∂x0 |x
∂fr −1 
∂x1 |x 
∂fr −1 

∂x2 |x 

..
.

∂fr −1
∂xk−1 |x




Df |x [v] = [Jf |x ]⊤ v

⊙ this can be directly applied to the chainrule
D(f ◦ g ◦ h)|x [v] = Df|g◦h(x) [Dg|h(x) [Dh|x [v]]]
= [Jf|g◦h(x) ]⊤ [Jg|h(x) ]⊤ [Jh|x ]⊤ v
⊙ insight: we can implement chainrule as a sequence of matrix multiplications of Jacobians !
(directional derivatives are linear operations, matrix multiplications are also linear operations)

Chainrule via matrix multiplication
key insight
We can implement chainrule for a sequence of functions
f (n) ◦ f (n−1) ◦ . . . ◦ f (2) ◦ f (1)
as a sequence of matrix multiplications of transposed Jacobian matrices
[Jf (n) ]⊤ [Jf (n−1) ]⊤ . . . [Jf (2) ]⊤ [Jf (1) ]⊤
⊙ we will apply it to the chain of function layers in a neural network!

| 36

Chainrule via matrix multiplication

| 37

Chainrule via matrix multiplication

| 38

The whole neural network is a chain of functions
ŷ = g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
in particular
x = a(0)
f (k) (a(k−1) ) = a(k)
f (n) (a(n−1) ) = a(n) = ŷ

Chainrule via matrix multiplication

| 39

The whole neural network is a chain of functions (corresponding to layers)
ŷ = g(x ) = f (n) f (n−1) f (n−2)

· · ·f (2) f (1) (x )

Dg(x )[v ] = [Jf (n) ]⊤ [Jf (n−1) ]⊤ [Jf (n−2) ]⊤

· · ·[Jf (2) ]⊤ [Jf (1) ]⊤ v

Its derivative is a chain of matrix multiplications of corresponding Jacobians for the respective layers
⊙ how to use this to run backpropagation ?

Backpropagation

| 40

The whole neural network is a chain of functions (corresponding to layers). We make the dependence
on parameters w (n) of layer function more explicit:
(n)

(2)

(n−1) (n−2)

(1)

· · ·fw (2) fw (1) (x )

ŷ = fw (n) fw (n−1) fw (n−2)

For every layer function f (k) we can compute a Jacobian (cf. derivative)
[J (a

(k−1)

) (k) ⊤

f

]

wrt. to the layer inputs a(k−1) and a Jacobian
[J (w

(k)

) (k) ⊤

f

]

wrt. to the layer parameters w (k) .
⊙ for updating the network parameters w of the network function g in x we need all derivatives
k
J (w ) g for all w k , k = 1, . . . , n

Backpropagation

| 41

The derivatives for the top-most layer can be computed directly:
ŷ = g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
n

n

[J (w ) g]⊤ = [J (w ) f (n) ]⊤

Backpropagation

| 42

The derivatives for the second layer from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
[J (w

n−1

)

g]⊤ =???

Backpropagation

| 43

The derivatives for the second layer from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
(n−1)

note: an−1 = fw n−1 (a(n−2) )
and: g(x ) = f (n) (an−1 )

Backpropagation

| 44

The derivatives for the second layer from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
(n−1)

note: an−1 = fw n−1 (a(n−2) )
and: g(x ) = f (n) (an−1 )
⇒ [J (w

n−1

)

g]⊤ = [J (a

n−1

) (n) ⊤

f

] [J (w

n−1

) (n−1) ⊤

f

]

Backpropagation

| 45

The derivatives for the third layer from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
[J (w

n−1

[J (w

n−2

)
)

g]⊤ = [J (a

n−1

g]⊤ = [J (a

n−1

) (n) ⊤

n−1

) (n) ⊤

n−2

f

f

] [J (w

] [J (a

) (n−1) ⊤

f

]

) (n−1) ⊤

f

] [J (w

n−2

) (n−2) ⊤

f

]

Backpropagation

| 46

The derivatives for few more layers from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
[J (w
[J

)

(w n−2 )

[J (w
[J

n−1

n−3

)

(w n−4 )

g]⊤ = [J (a
⊤

g] = [J

g] = [J

) (n) ⊤

f

] [J (w

(an−1 ) (n) ⊤

f

g]⊤ = [J (a
⊤

n−1

n−1

] [J

) (n) ⊤

f

f

] [J

This can be computed recursively!!

) (n−1) ⊤

f

]

(an−2 ) (n−1) ⊤

f

] [J (a

(an−1 ) (n) ⊤

n−1

n−2

] [J (w

) (n−1) ⊤

f

] [J (a

(an−2 ) (n−1) ⊤

f

] [J

n−2

n−3

) (n−2) ⊤

f

]

) (n−2) ⊤

f

] [J (w

(an−3 ) (n−2) ⊤

f

] [J

n−3

) (n−3) ⊤

f

]

(an−4 ) (n−3) ⊤

f

] [J (w

n−4

) (n−4) ⊤

f

]

Backpropagation

| 47

The derivatives for few more layers from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
[J (w

n−1

(w

n−2

[J (w

n−3

(w

n−4

[J

[J

)
)
)
)

g]⊤ = [J (a

n−1

(a

n−1

g]⊤ = [J (a

n−1

⊤

g] = [J
⊤

g] = [J

(a

n−1

) (n) ⊤

n−1

) (n) ⊤

(a

n−2

) (n) ⊤

] [J (a

n−2

) (n) ⊤

n−2

f
f

f

f

] [J (w
] [J

] [J

(a

) (n−1) ⊤

f

]

) (n−1) ⊤

n−2

) (n−1) ⊤

n−3

) (n−1) ⊤

n−4

] [J (w

f

] [J (a

f
f

] [J

(a

) (n−2) ⊤

f

]

) (n−2) ⊤

n−3

) (n−2) ⊤

n−4

f
f

] [J (w
] [J

(a

) (n−3) ⊤

f

f

This can be computed recursively!! The start:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
n

n

[J (w ) g]⊤ = [J (w ) f (n) ]⊤
BP (n) := [J (a

n−1

) (n) ⊤

f

]

]

) (n−3) ⊤

] [J (w

n−4

) (n−4) ⊤

f

]

Backpropagation

| 48

The derivatives for few more layers from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
[J (w

n−1

[J (w

n−2

[J

)
)

(w n−3 )

[J (w

n−3

)

g]⊤ = [J (a

n−1

g]⊤ = [J (a

n−1

⊤

g] = [J

) (n) ⊤

n−1

) (n) ⊤

n−2

f
f

] [J (w
] [J (a

(an−1 ) (n) ⊤

g]⊤ = [J (a

f

n−1

] [J

f

]

) (n−1) ⊤

f

] [J (w

(an−2 ) (n−1) ⊤

f

) (n) ⊤

f

) (n−1) ⊤

] [J (a

n−2

] [J

) (n−2) ⊤

f

]

(an−3 ) (n−2) ⊤

] [J (w

f

) (n−1) ⊤

f

n−2

] [J (a

n−4

) (n−2) ⊤

] [J (a

f

n−3

n−4

) (n−3) ⊤

f

) (n−3) ⊤

f

This can be computed recursively!! The continuation:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
BP (n) := [J (a
[J (w

n−1

BP
[J (w

)

)

) (n) ⊤

f

g]⊤ = BP (n) [J (w

(n−1)

n−2

n−1

:= BP

(n)

[J

]

n−1

) (n−1) ⊤

f

]

(an−2 ) (n−1) ⊤

g]⊤ = BP (n−1) [J (w

f

]

n−2

) (n−2) ⊤

f

]

]

] [J (w

n−4

) (n−4) ⊤

f

]

Backpropagation

| 49

The derivatives for few more layers from the top:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
[J (w

n−1

[J (w

n−2

[J

)
)

(w n−3 )

[J (w

n−3

)

g]⊤ = [J (a

n−1

g]⊤ = [J (a

n−1

⊤

g] = [J

) (n) ⊤

n−1

) (n) ⊤

n−2

f
f

] [J (w
] [J (a

(an−1 ) (n) ⊤

g]⊤ = [J (a

f

n−1

] [J

f

]

) (n−1) ⊤

f

] [J (w

(an−2 ) (n−1) ⊤

f

) (n) ⊤

f

) (n−1) ⊤

] [J (a

n−2

] [J

) (n−2) ⊤

f

]

(an−3 ) (n−2) ⊤

] [J (w

f

) (n−1) ⊤

f

n−2

] [J (a

n−4

) (n−2) ⊤

] [J (a

f

n−3

n−4

) (n−3) ⊤

f

) (n−3) ⊤

f

This can be computed recursively!! The whole:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
n

n

[J (w ) g]⊤ = [J (w ) f (n) ]⊤ init
BP (n) := [J (a
BP
[J (w

(k−1)

k−2

)

:= BP

n−1

(k)

) (n) ⊤

f

[J

]

init

(ak−2 ) (k−1) ⊤

g]⊤ = BP (k−1) [J (w

f

]

k−2

) (k−2) ⊤

f

]

]

] [J (w

n−4

) (n−4) ⊤

f

]

Backpropagation

| 50

This can be computed recursively!! The whole:
g(x ) = f (n) f (n−1) f (n−2) · · · f (2) f (1) (x )
n

n

[J (w ) g]⊤ = [J (w ) f (n) ]⊤ init
BP (n) := [J (a
BP
[J (w

(k−1)

k−2

)

:= BP

n−1

(k)

) (n) ⊤

f

[J

]

init

(ak−2 ) (k−1) ⊤

g]⊤ = BP (k−1) [J (w

f

]

k−2

) (k−2) ⊤

⊙ why this is efficient ? ... what are the alternatives ?

f

]

Backpropagation

| 51

⊙ why backprop is efficient ? ... what are the alternatives ?
· gradient by finite difference approximation
∂g

≈
(k)

+ ϵ) − g(wi )
ϵ

(k)

+ ϵ) − g(wi
2ϵ

∂wi

∂g

≈
(k)

∂wi

(k)

(k)

g(wi
g(wi

(k)

− ϵ)

... Needs as many forward passes through the whole network as we have parameters.

Backpropagation

⊙ gradient by finite difference approximation
... Needs as many forward passes through the whole network as we have parameters.

... LLama-7B: 7 Billion forward passes ... enjoy

| 52

Backpropagation

| 53

⊙ why backprop is efficient ? ... what are the alternatives ?
· gradient by finite difference approximation ... Needs as many forward passes through the
whole network as we have parameters. ... LLama-7B: 7 Billion forward passes ... enjoy
⊙ Backprop needs 1 forward pass to get all layer feature maps a(k)
[J (w

k−1

)

g]⊤ = BP (k) [J (w

k−1

BP (k−1) := BP (k) [J (a

) (k−1) ⊤

k−2

f

]

) (k−1) ⊤

f

]

⊙ Backprop needs also for each layer:
·
·
·
·

one gradient computation for a single layer f (k−1) with respect to its direct weights w k−1
(k−1)
one matrix multiplication to get J w
g for the parameters of a layer
one gradient computation for a single layer f (k−1) with respect to its direct inputs ak−2
one matrix multiplication to get/update BP (k−1)

Backpropagation

| 54

What about the gradients for a single layer ?
For an affine layer they are trivial:
a(n) = W (n) a(n−1) + b (n)
(n)

= W (n) [j, :]a(n−1) + b (n) [j]
X
(n)
W (n) [j, k]a(n−1) [k] + b (n) [j]
aj =

aj

k
(n)
(n)
∇W [j,:] aj = a(n−1)

∇a

(n−1)

(n)

aj

= W (n) [j, :]

obviously because
∂
∂W (n) [j, r ]
∂
∂a(n−1) [r ]

X

W (n) [j, k]a(n−1) [k] + b (n) [j] = a(n−1) [r ]

k

X
k

W (n) [j, k]a(n−1) [k] + b (n) [j] = W (n) [j, r ]

Backpropagation

| 55

⊙ why backprop is efficient ?
⊙ Backprop needs 1 forward pass to get all layer feature maps a(k)
[J (w

k−1

)

g]⊤ = BP (k) [J (w

k−1

BP (k−1) := BP (k) [J (a

) (k−1) ⊤

k−2

f

]

) (k−1) ⊤

f

]

⊙ Backprop needs also for each layer:
·
·
·
·

one gradient computation for a single layer f (k−1) with respect to its direct weights w k−1
(k−1)
one matrix multiplication to get J w
g for the parameters of a layer
one gradient computation for a single layer f (k−1) with respect to its direct inputs ak−2
one matrix multiplication to get/update BP (k−1)

⊙ the gradient computation steps are trivial for affine layers !

Why backprop is efficient ?

| 56

Why backprop is efficient ? Answer (II) via a simplified count:
⊙ a neural network only composed of matrix multiplications x ∈ Rd . All feature maps are of
dimensionality d. All matrices are of shape (d, d).
y = W (n) W (n−1) W (n−2) . . . W (1) x

Why backprop is efficient ?

| 57

A simplified count:
⊙ a neural network only composed of matrix multiplications x ∈ Rd . All feature maps are of
dimensionality d. All matrices are of shape (d, d).
y = W (n) W (n−1) W (n−2) . . . W (1) x
⊙ finite difference:
· one forward pass needs n matrix-vector multiplications W (k) a(k−1) which are O(d 2 ). Total:
O(nd 2 )
· we have nd 2 parameters for applying finite difference. In total O(n2 d 4 ) Ops
⊙ backprop:
· obtaining the layer derivatives is trivial (for affine layers)
· matrix multiplication is O(d 3 ). we have 2 per layer. This is in total O(nd 3 ) Ops

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

| 58

What is if ... ?

| 59

I have to implement:

Ti,n,r ,s =

X

Ai,k,m,n,o Bm,o,r Ck,m,r ,s

k,m,o

It might be slow and memory-consuming, but it is easy to be done!
It is a generalized matrix multiplication. Compare:
A, B 7→ (AB)
X
(AB)il =
Aik Bkl
k

torch.matmul https://pytorch.org/docs/stable/generated/torch.matmul.html works for the simple
cases with broadcasting, but it is complicated in its rules.
Something easier ?

What is if ... ?

| 60

(AB)il =

X

Aik Bkl

k

ab= torch.einsum( 'ik,kl -> il' ,[a,b])

(Av )i =

X

Aik vk

k

av= torch.einsum( 'ik,k -> i' ,[a,v])

w ·v =

X
k

wdotv= torch.einsum( 'k,k -> ' ,[w,v])

w k vk

Einsum

| 61

a general way to do all kinds of batched and non-batched tensor multiplications:
torch.einsum
https://rockt.github.io/2018/04/30/einsum
4 rules:
⊙ left of − >: all tensors separated by , which are to be multiplied and summed.
⊙ right of − > the result tensor with remaining indices (maybe multiplied, but not

summed over).
⊙ indices that have same name in multiple tensors, will get multiplied

together (must have same count in .shape[i] !)
⊙ All indices missing right of − > are summed out so that they are not

present in the result.

Einsum Exercises

| 62

What are the einsum notations for ?
(a) x [:], v [:] 7→

X

xi vi

i

(b) A[:, :], v [:] 7→

X

vi Aik

i

(c) v [:], A[:, :], v [:] 7→

X

vi Aik vj

i,k

(d) A[:, :], C [:, :] 7→

X

Aik Cil

i

(e) A[:, :], C [:, :] 7→

X
i

Put your solutions in the sli.do link

Aik Cik

Einsum Exercises

| 63

What are the einsum notations for ?
(f ) A[:, :], C [:, :] 7→

X

Aik Cik

i,k

(g) v [:], A[:, :], B[:, :] 7→

X

vi Aik Bkl

i,k

(h) v [:], A[:, :, :], B[:, :] 7→

X

vl Aikl Blr

k,l

(i) A(5), B(3), C (4) 7→

X
k,m,o

Put your solutions in the sli.do link

Ai,k,m,n,o Bm,o,r Ck,m,r ,s

Einsum Limitations (off quizzes, read: humor)

⊙ slower than built-in routines for standard matrix ops
⊙ quantum computers ... indexing support for 3000-tensors ... Chinese writing as

mandatory course ? :)

| 64

Outline
1 Properties of Gradient Descent
2 Stochastic gradient descent
3 Why gradient descent is hard in practice?
4 Compute the Gradient in PyTorch
5 Gradient descent in PyTorch
6 Chainrule via matrix multiplication and the Backpropagation algorithm
7 Einsum for generalized matrix-matrix multiplication type operations
8 Derivatives of Bilinear mappings

| 65

Derivatives of Bilinear mappings

| 66

f (X, Y) is bilinear, if it is linear in argument X and also linear in argument Y.
f (x) = x⊤ Ax
∇fk (x) =?
Df (x)[h] =?
Idea: get the directional derivatives for
f (A, B) = AB

Derivatives of Bilinear mappings

| 67

f (X, Y) is bilinear, if it is linear in argument X and also linear in argument Y.
Idea: get the directional derivatives for
f (A, B) = AB
f (A + tH, B + tK ) = f (A, B + tK ) + f (tH, B + tK ) = f (A, B) + tf (A, K ) + tf (H, B + tK )
= f (A, B) + tf (A, J) + tf (H, B) + t 2 f (H, K )
tf (A, K ) + tf (H, B) t 2 f (H, K )
f (A + tH, B + tK ) − f (A, B)
= lim
+
t→0
t→0
t
t
t
= f (A, K ) + f (H, B) + lim O(t) = f (A, K ) + f (H, B)

Df (A, B)[H, K ] = lim

t→0

Derivatives of Bilinear mappings

f (X, Y) is bilinear, if it is linear in argument X and also linear in argument Y. Then:
Df (A, B)[H, K ] = f (H, B) + f (A, K )
simple rule!

| 68

Derivatives of Bilinear mappings

f (X, Y) is bilinear, if it is linear in argument X and also linear in argument Y. Then:
Df (A, B)[H, K ] = f (H, B) + f (A, K )

| 69

Derivatives of Bilinear mappings

| 70

f (x) = x⊤ Ax
Variant 1: take note that
g(x, y) = x⊤ Ay
is bilinear and
f (x) = g(x, x)
So:
Df (x)[h] = Dg(x = x, y = x)[H = h, K = h] = g(h, x) + g(x, h) = h⊤ Ax + x⊤ Ah
Done!!

Derivatives of Bilinear mappings

| 71

Variant 2: use chainrule
f (x) = x⊤ Ax
Df (x)[h] = chain rule!
So:
f (x) = mul(x ⊤ , mul(A, x)) = mul(a(x ), b(x ))
a(x) = x⊤ = Id ⊤ (x)
b(x) = Ax = mul(A, x)
Dmul(a(x ), b(x ))[h] = mul(Da(x )[h], b(x )) + mul(a(x ), Db(x )[h])

Derivatives of Bilinear mappings

f (x) = x⊤ Ax = mul(x⊤ , mul(A, x)) = mul(a(x), b(x))
Dmul(a(x), b(x))[h] = mul(Da(x)[h], b(x)) + mul(a(x), Db(x)[h])
So:
with a(x) = x⊤ , b(x) = Ax
Df (x)[h] = mul(Da(x)[h], b(x)) + mul(a(x), Db(x)[h])
= mul(h⊤ , b(x)) + mul(a(x), Ah)
= mul(h⊤ , Ax) + mul(x⊤ , Ah)
= h⊤ Ax + x⊤ Ah

| 72

