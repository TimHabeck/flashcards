Intro to DL4MSc: More on Optimizers
Alexander Binder
December 1, 2025

Links

know where to look for
⊙ ...

|2

High level content

know what
⊙ computing the gradient:
· chain rule on a general graph
⊙ applying the computed gradient
· weight decay, momentum, EMA, Adam

|3

...

|4

⊙ ...

Outline

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

|5

Recovering the chainrule by writing f (g(x )) as a graph

∂f ◦ g
∂f
∂g1
∂f
∂g2
∂f
∂g3
(x ) =
+
+
∂x2
∂z1 z=g(x ) ∂x2
∂z2 g(x ) ∂x2
∂z3 g(x ) ∂x2
Three steps:
∂h
⊙ we assign to an edge zi 7→ h(zi , other vars) the edge term: ∂z
i

⊙ we multiply all edge terms along a backward path x2 → f (g(x ))
⊙ at a node x2 we sum terms from all backward paths

|6

Exercise the chain rule

|7

∂y
∂y
∂y
, ∂z
, ∂z
Compute: ∂z
6
3
7

y

y

y

Exercise the chain rule

|8

∂y
∂y
∂y
, ∂z
, ∂z
Compute: ∂z
6
3
7

y

Good for an understanding of the flow of the gradient,
⊙ ... neural networks are simpler, see lecture on backpropagation

Outline

|9

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

Neural network

We have now computed the gradient.
Goal: see what we can do beyond plain gradient descent!

| 10

Neural network

| 11

What we had for optimization: want to find a parameter w corresponding to a mapping
fw : x 7→ f (x ) ∈ Y
n
1X
Ê (w , L) =
L(fw (xi ), yi )
n i=1
argminw Ê (fw , L)
Basic Algorithm idea (Gradient Descent):

⊙ initialize start vector w0 as something reasonable, step size parameter η
⊙ run while loop until changes are very little, do at iteration t:
dE
· wt+1 = wt − η∇w Ê (wt , L) = wt − learningrate · dw
(wt )
· compute change to last: ∥E (wt+1 ) − E (wt )∥

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 12

Weight decay

| 13

Replace
wt+1 = wt

−ηt ∇w Ê (wt , L) by

wt+1 = wt (1 − ληt )

−ηt ∇w Ê (wt , L)

shrinks weight towards zero. Comes from quadratic regularization:
n

ÊReg (w , L) =

1X
L(fw (xi ), yi )
n i=1
n

∇w ÊReg (w , L) = ∇w

1X
L(fw (xi ), yi )
n i=1

∇w ÊReg (w , L) = ∇w Ê (w , L)

1
+ ληt ∥w ∥22
2
1
+∇w ληt ∥w ∥22
2
+ληt w

Weight decay

| 14

therefore: wt+1 = wt − ηt ∇w Ê (w , L) − ληt w
therefore: wt+1 = wt (1 − ληt ) − ηt ∇w Ê (w , L)
Important:
in case of using SGD, weight decay is the same as ℓ2 -regularization.
Weight decay in general is the multiplication of a weight with a small number w = w ·(1−γ), γ ∈
(0, 1) to shrink it towards zero.

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 15

Momentum term

| 16

many more heuristics replace wt+1 = wt − ηt ∇w Ê (wt , L) by something related to it.

m0 = 0, α ∈ (0, 1)
mt+1 = αmt + ηt ∇w Ê (wt , L)
wt+1 = wt − mt+1

Momentum term

| 17

What does the momentum compute? Assume ηt = η is constant.
Lets shorten: gt = ∇w Ê (wt , L)
m1 = αm0 + ηg0 = ηg0
m2 = αm1 + ηg1 = α1 ηg0 + ηg1
m3 = αm2 + ηg2 = α2 ηg0 + α1 ηg1 + ηg2
m4 = αm3 + ηg3 = α3 ηg0 + α2 ηg1 + α1 ηg2 + ηg3
m5 = αm4 + ηg4 = α4 ηg0 + α3 ηg1 + α2 ηg2 + α1 ηg3 + ηg4
general rule:
mt = η

t−1
X
s=0

!
α

t−1−s

gs

Momentum term

| 18

general rule:
mt = η

t−1
X

!
α

t−1−s

gs

s=0

What does this represent: consider g0 , g1 , g2 , . . . as a time series. Then
⊙ mt is a weighted average up to multiplication with a constant.
⊙ the weights of this average decrease exponential as we go back into the past

Momentum term

| 19

Vanilla average over g0 , g1 , g2 , . . .:
t−1
t−1
X
1X
1
gs =
gs
t s=0
t
s=0

a weighted average would be:
t−1
X

ws gs

s=0

ws ≥ 0,

t−1
X
s=0

ws = 1

Momentum term

| 20

Vanilla average is a weighted average with constant (time-independent weights): ws = 1t .
For the momentum term:
t−1
X

αt−1−s = αt−1 + αt−2 + αt−3 + . . . + α2 + α1 + α0

s=0

=

t−1
X
s=0

αs =

1 − αt
1−α
t

It is a weighted average up to division of weights by 1−α
1−α .
Exponential decay from terms in the past: Earliest term:
s = 0 ⇒ αt−1−s = αt−1
Since 0 < α < 1 this is a very small term. Latest term has weight a0 = 1.
It is an average. Weights decrease exponentially towards the past. ... It looks more at the recent past.
In practice often α = 0.9 ... to give a slow increase towards the present.

Momentum term

Overview over momentum
⊙ Computes an average mt+1 between current gradient ηt ∇w Ê (wt , L) and gradients from
the past mt . Use this average for updating weights
⊙ acts as a memory for gradients in the past
⊙ it can help in flat valleys because it remembers the bigger stepsize from the past steps
⊙ with one additional parameter α

| 21

Momentum term

| 22

Important:
SGD with a learning rate η, momentum using α = 0.9 and weight decay constant β is a very
common baseline choice
mt+1 = αmt + η∇w Ê (wt , L)
wt+1 = wt − mt+1 − ηβwt
Momentum term can be understood as a rescaled, time-dependent weighted average of gradients.
Meaning of Rescaled: weights do not sum up to 1 and depend on the number of steps T . Momentum
is related to first moment of gradients and thus to the idea of replacing a random variable by its first
moment.
Pytorch ?
optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 23

Nesterov momentum

| 24

Recap momentum:
m0 = 0, α ∈ (0, 1)
mt+1 = αmt + ηt ∇w Ê (wt , L)
wt+1 = wt − mt+1
Can rewrite last update wt+1 as:
wt+1 = wt − αmt − ηt ∇w Ê (wt , L)
Largest component of it is usually the old weight and the momentum term wt − αmt . Meaning:
approximate next position is around wt − αmt .

Nesterov momentum

| 25

Idea of Nesterov: compute gradient at the approximate next position wt − αmt which comes when
using momentum:

m0 = 0, α ∈ (0, 1)
mt+1 = αmt + ηt ∇w Ê (wt − αmt , L)
wt+1 = wt − mt+1

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 26

Exponential moving average

| 27

For a time series gs , s ≥ 0 the term
EMA(gs )0 = 0

+(1 − α)g0

EMA(gs )t = αEMA(gs )t−1

+(1 − α)gt

defines an exponential moving average. Moving – because weights are always high for the recent past.
Difference to Momentum explicit weighting with 1 − α.

Exponential moving average

The recursion yields here
EMA(gs )0 = α0 (1 − α)g0
EMA(gs )1 = α1 (1 − α)g0 + (1 − α)g1
EMA(gs )2 = α2 (1 − α)g0 + α1 (1 − α)g1 + (1 − α)g2
EMA(gs )3 = α3 (1 − α)g0 + α2 (1 − α)g1 + α1 (1 − α)g2 + (1 − α)g3
EMA(gs )t =

t
X

αt−s (1 − α)gs

s=0

The weights of EMA(gs )t sum up to 1 − αt+1 . Used in RMSProp, Adam, AdamW

| 28

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 29

RMSProp

| 30

An idea to deal with the flat regions – Unpublished method by Geoffrey Hinton. Uses EMA.
Observation: size of update of weights, as measured by euclidean length is proportional to the norm of
the gradient:
gt = ∇w Ê (wt , L)
wt+1 = wt − ηt gt
∥wt+1 − wt ∥ = ηt ∥gt ∥
So in a flat region with ∥gt ∥ ≈ 0, the steps taken are very small.
First idea: use gradient divided by norm of gradient
wt+1 = wt − ηt

gt
∥gt ∥

Problem with this: whether one is in a looong flat region or not cannot be decided by looking at a
single gradient at the current point - need to look a bit more into the past.

RMSProp

| 31

So use an average of norms of gradients from the past, and divide by them. Divide by EMA(·)t of
norms of gradients:
wt+1 = wt − ηt

gt
EMA(∥gs ∥)t

Idea: flat valley, for many time steps s around the current time step t norms of gradients are small, so
EMA will be small. Dividing by a small term makes the stepsize bigger.
Still not perfect: We need to reduce the stepsize fast when we enter more steep regions. That means:
if a current gradient norm ∥gt ∥ at time t is large, the EMA needs to become large quickly (so that
dividing by a large EMA leads to a small step)!
Need to make the EMA more sensitive to large gradient in the current time step.

RMSProp

| 32

must quickly reduce
stepsize here!

otherwise huge step into NaNs

Squared norms are better, as squares are more sensitive to large outliers in a sum (x 2 grows quicker
than x ). so use ∥gt ∥2 - squared norms in the EMA (and take a root of the EMA).
wt+1 = wt − ηt p

gt
EMA(∥gs ∥2 )t

One can show mathematically that the root of an average of squared norms is larger than the vanilla
average: inequality between the arithmetic and the quadratic mean.
Still not perfect: What is if all gradients are near-zero? Huge step into the world of NaNInf.

RMSProp

| 33

Better: add a small ϵ
gt
wt+1 = wt − ηt p
EMA(∥gs ∥2 )t + ϵ
Now upscaling factor is limited by √1ϵ
This RMSProp Algorithm can be rewritten in an iterative form, which is easier to code:
Parameters: α, ϵ, η
d0 = 0
compute gt := ∇w Ê (wt , L)
dt = αdt−1 + (1 − α)∥gt ∥2
gt
wt+1 = wt − ηt √
dt + ϵ

# dt is EMA(∥gs ∥2 )t

RMSProp

can be remembered as:
maintain an EMA for squared norms of gradient, divide gradient by the square-root of it plus
some stabilizing ϵ. Use this for update of weights.
Its effect can be understood as:
∂L
⊙ divide gradient ∂w
by a history of gradient norms with time-limited horizon

⊙ upscales stepsize in flat region
⊙ downscales stepsize when changes become steep
RMSProp can be understood as a normalization of the gradient by a rescaled, time-dependent
weighted average of gradient norms. The normalizer in RMSProp is related to the second moment of
gradient norms.

| 34

RMSProp

| 35

Compare also to standard deviation. The normalizer in RMS prop differs from standard deviation in
what way ?
X random variable
E [X ] first moment
E [X 2 ] second moment
σ 2 (X ) = E [X 2 ] − (E [X ])2
It is related to normalizing the standard deviation a variable by dividing it by an ϵ-stabilized estimate of
its standard deviation:
X
X −→ p
2
σ (X ) + ϵ
Compare to RMSprop:
Z = gt , S 2 = EMA(∥gt ∥2 ) rescaled second moment estimator for ∥gt ∥
Z
gt
ut = √
=p
2
S +ϵ
EMA(∥gt ∥2 ) + ϵ
wt+1 = wt − ηt ut

RMSProp

| 36

RMSprop does something similar to:
Xd
Xd −→ p
E [∥X ∥2 ] + ϵ
What can we do next ? Take the above, replace the gradient Xd by a momentum estimate of it, as
done for momentum.
Putting these two ideas together will result in Adam

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 37

Adam

| 38

Similar to a combination RMSprop with a second momentum term on the gradients
... and two small ideas as improvement over RMSprop.

can be remembered as:
⊙ a combination of RMSProp with momentum
⊙ one momentum term for the gradient (one EMA),
⊙ and another EMA for the element-wise squared gradient
⊙ plus two smaller modifications

Adam

| 39

How would RMSprop with Momentum Term look like in step t?

compute gt := ∇w Ê (wt , L)
st = α1 st−1 + (1 − α1 )∥gt ∥2
gt
rpropterm = √
st + ϵ

# dt is EMA(∥gs ∥2 )t

In RMSProp one would apply rpropterm to update the weights wt with a stepsize ηt . Now one
replaces in rpropterm the gradient gt by its momentum mt :
mt = α2 mt−1 + (1 − α2 )gt
mt
wt+1 = wt − ηt √
st + ϵ

Adam

| 40

The two improvements are made in Adam over the algorithm above which you do not need to
memorize for quizzes:
1

normalize every dimension of the update separately – dont use the norm of the gradient, but the
square of every single dimension

2

turn all used/defined terms which use an EMA into a true weighted average by multiplying them
1
with the appropriate normalizer (time-dependent) 1−α
t

We explain both steps in detail.

Adam

| 41

Point 1. normalize every dimension of the update separately:
(0)

(d−1)

(1)

The gradient gt is a vector gt = (gt , gt , . . . , gt
dimension d of gt is scaled by the same constant:

). When computing rpropterm above every

1
p

EMA(∥gs ∥2 )t + ϵ

=√

1
st + ϵ

In Adam one computes an EMA for every dimension gt [d] of the gradient. One uses the square
(gt [d])2 of the gradient in dimension d:
st [d] = α1 st−1 [d] + (1 − α1 )(gt [d])2
st [d] is a scalar. Note summing
the component squared for all dimensions d results in the squared
P
gradient norm ∥gt ∥2 = d (gt [d])2 .

Adam

| 42

For a single dimension of the gradient this follows√the idea of replacing a random variable with its
mean, and to normalize it by an estimate of the · of second moment which is related to the standard
deviation:
gt [d] −→ p
It does not hold exactly because
– the weights of the EMA do not sum up to one

E [ gt [d] ]
E [ gt [d]2 ] + ϵ

Adam

| 43

Using only 1. the algorithm would look like that:

compute gt := ∇w Ê (wt , L)
st [d] = α1 st−1 + (1 − α1 )(gt [d])2 #EMA of gt [d]2
st = (st [0], . . . , st [D − 1]) = (st [d])d #vectorization
mt = α2 mt−1 + (1 − α2 )gt #EMA of gt [d]vectorized
mt
wt+1 = wt − ηt √
st + ϵ
√
1/ st : (element-wise division for every dimension st [d])

Adam

Point 2. turn all used/defined terms which use an EMA into a true weighted average by multiplying
them with an appropriate constant:
This is based on the observation, that the weights of every
EMA(us )t sum up to 1 − αt+1 .
Therefore whenever applying an EMA term, it must be divided by 1 − αt+1 , in order to yield a true
weighted average. An EMA is used here in two steps: once when computing term, a second time when
computing wt+1 .

| 44

Adam

| 45

The final ADAM algorithm is:
Parameters η, ϵ, α1 , α2
m0 = 0, s0 = 0
compute gt := ∇w Ê (wt , L)
st [d] = α1 st−1 [d] + (1 − α1 )(gt [d])2

#element − wise

mt = α2 mt−1 + (1 − α2 )gt
ct,1 = 1 − α1t , ct,2 = 1 − α2t
wt+1 = wt − ηt p

mt /ct,1
st /ct,2 + ϵ

√
1/ st : (element-wise division for every dimension st [d])

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 46

AdamW

https://arxiv.org/abs/1711.05101 Loshchilov & Hutter, ICLR 2019
can be remembered as:
Adam with a modification
⊙ do not put weight decay inside the momentum. Apply the weight decay directly on the
weight during the update

| 47

AdamW

| 48

Parameters η, ϵ, α1 , α2
m0 = 0, s0 = 0
compute gt := ∇w Ê (wt , L)
st [d] = α1 st−1 [d] + (1 − α1 )(gt [d])2

#element − wise

mt = α2 mt−1 + (1 − α2 )gt
ct,1 = 1 − α1t , ct,2 = 1 − α2t
mt /ct,1
− ληt wt
wt+1 = wt − ηt p
st /ct,2 + ϵ
√
1/ st : (element-wise division for every dimension) st [d])

AdamW

| 49

The difference is to Adam as above (from the paper)?

source: AdamW paper https://arxiv.org/abs/1711.05101

AdamW

| 50

So the difference is:
wt+1 =
vs wt+1 =

mt /ct,1
dt /ct,2 + ϵ
mt /ct,1
wt − ηt p
− ληt wt
dt /ct,2 + ϵ

wt − ηt p

Many toolboxes do not do real weight decay, but add a ℓ2 -regularizer term and let the gradient perform
implicitly weight decays then (see purple).
When implemented as ℓ2 -regularizer, then the effect of the ℓ2 -regularizer gets swallowed and smoothed
out in/by the EMA terms. EMA was designed to smooth out large changes in gradient, now it
smoothens out the weight decay effect too :) .
Important:
AdamW compared to Adam performs a stronger weight decay when gradients are larger.

Contents

1 Chain rule as flow of gradients through a graph
2 Optimizers: Applying the gradients for parameter updates in smarter ways

Weight decay
Momentum term
Nesterov momentum
Exponential moving average (EMA)
RMSProp
Adam
AdamW: Adam with decoupled weight decay
How valuable are these methods?

| 51

Final thoughts

For every set of data you need to validate whether using them makes sense:
https://arxiv.org/pdf/1705.08292.pdf
My personal observation is that Adam converges faster in the beginning but SGD catches up later on.
It seems that switching later to SGD can be beneficial:
https://arxiv.org/pdf/1712.07628.pdf

| 52

Final thoughts

Important:
If you want to use different solvers, remember to save not only the model but also the solver
state
Where are those in pytorch?
torch.optim

| 53

Final thoughts

A paper on effects of newer learning rate heuristics:
https://arxiv.org/pdf/1810.13243.pdf
Why all these arxiv.org papers ?
Learning from lectures ???
Better: Learn to effectively process/filter papers: ICML, NeurIPS, ICLR, CVPR, ICCV, ACL,
EMNLP, ICASSP, SIGGRAPH, ... and other rank A and rank B conferences.

| 54

