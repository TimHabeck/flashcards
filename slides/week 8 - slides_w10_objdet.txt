Intro to DL4MSc: Object Detection
Alexander Binder
August 18, 2025

links

know where to look for
⊙ https://d2l.ai/chapter computer-vision/bounding-box.html
⊙ https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/
⊙ https://lilianweng.github.io/posts/2017-10-29-object-recognition-part-1/
⊙ https://lilianweng.github.io/posts/2017-12-15-object-recognition-part-2/
⊙ https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/

|2

Single bounding box

|3

⊙ ground truth: a class label and a
bounding box (c, (hul , wul , hlr , wlr ))
⊙ class label can be replaced by
probability vector in case of
disagreement between annotators
((p0 , . . . , pK −1 ), (hul , wul , hlr , wlr ))
⊙ goal: predict a bounding box
((p0 , . . . , pK −1 ), (hul , wul , hlr , wlr ))
· hul , wul upper left coord
· hlr , wlr lower right coord
· (p0 , . . . , pK −1 ) class probs, pk
prob for class k

Single bounding box

(0.0,0.0)

|4

w

⊙ c0 = background,
c1 = cat,
c2 = lion
⊙ maybe
(c, (hul , wul , hlr , wlr ))

h

(1, (0.15, 0.45, 1.0, 0.8))
⊙ or maybe
((p0 , . . . , pK −1 ), (hul , wul , hlr , wlr ))
((0.0, 0.9, 0.1), (0.15, 0.45, 1.0, 0.8))
(1.0,1.0)

Single bounding box (not used in practice in that way!)

Pool 3x3 / s=2

input image: (1, 3, 224, 224)

Pool 3x3 / s=2

input image: (1, 3, 224, 224)

|5

Multiple bounding boxes (not used in practice in that way!)

Pool 3x3 / s=2

input image: (1, 3, 224, 224)

Pool 3x3 / s=2

input image: (1, 3, 224, 224)

|6

...

|7

⊙ Start off a vanilla backbone with classification head. Example
https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py
⊙ outputs logits for C classes
def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
self.fc = nn.Linear(512 * block.expansion, num_classes)
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
x = self.avgpool(x)
x = torch.flatten(x, 1)
x = self.fc(x)
return x

...

|8

if you want to predict a single bounding box
⊙ need to predict 4 bbox coordinates ( sometimes done as one for each of the C classes)
⊙ and logits for C classes and 1 background class = C + 1 classes
⊙ total output: C + 1 + 4

...

|9

if you want to predict a single bounding box
⊙ total output: C + 1 + 4
⊙ remove spatial pooling (why?).
⊙ modify input dimension to match number of channels in last feature map times spatial dimensions
def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
# change HERE !!!!
h_dim =
w_dim =
self.fc = nn.Linear(512 * h_dim * w_dim, num_classes +1 +4)
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
#x = self.avgpool(x)
x = torch.flatten(x, 1)
x = self.fc(x)
return x

...

| 10

if you want to predict 5 bounding boxes
⊙ total output: C + 1 + 4
⊙ remove spatial pooling (why?).
⊙ modify input dimension to match number of channels in last feature map times spatial dimensions
def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
# change HERE !!!!
h_dim =
w_dim =
self.fc = nn.Linear(512 * h_dim * w_dim, 5*(num_classes +1 +4))
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
#x = self.avgpool(x)
x = torch.flatten(x, 1)
x = self.fc(x)
return x

Multiple bounding boxes (not used in practice in that way!)

Pool 3x3 / s=2

input image: (1, 3, 224, 224)

⊙ Object detection does not use a single FC layer. Reason: too many parameters
· will see a better solution in later slides
· for now as a simple demo so that we can predict multiple boxes
⊙ next: how to measure performance of predictions ?

| 11

Outline

1 Performance Measurement for Evaluation at Inference Time
2 Performance Measurement for Loss Computation at Training Time
3 Towards architectures for bounding box prediction
4 Inference Time: Non-maximum suppression and thresholding
5 Further

| 12

Multiple bounding boxes: how to measure performance of predictions ?
Situation:
⊙ L ground truth boxes y = {y(l) }L−1
l=0 where y(l) = (cl , bl ), bl 4 dim box coordinates.
⊙ P predicted boxes f (x ) = {ŷ(p) }P−1
p=0 where ŷ(p) = (p̂p , b̂p )
⊙ l, p is not a sample index. It is an index for a box within one image !
Tasks:
⊙ have to decide which predicted box to assign to which ground truth box
see the Figure in https://github.com/rafaelpadilla/Object-Detection-Metrics
⊙ after an assignment ŷ(l) ↔ y(k) : have to measure two properties
· is the predicted class confidence p̂l (c) high for the ground truth class c = ck of a box y(k) ?
· are the predicted boxes b̂l and the ground truth boxes bk overlapping highly?
This is much more complicated than in classification.

| 13

Multiple bounding boxes: how to measure performance of predictions ?

⊙ gt has multiple boxes for one image, we predict multiple boxes
⊙ we will use average precision as ranking measure
⊙ measurement uses 2 steps:
· step 1: match predicted boxes to gt boxes:
for every gt box, find at most 1 best matching predicted box. This will also define what is a
TP and what is a FP
https://github.com/rafaelpadilla/Object-Detection-Metrics
· step 2: given the match assignments + the defined TP,FP, next compute the average
precision by varying the threshold t for the class confidence ccat (x ) ≥ t
https://github.com/rafaelpadilla/Object-Detection-Metrics

| 14

Multiple bounding boxes: how to measure performance of predictions ?

IoU

IoU(b, b̂) =

area(b ∩ b̂)
area(b ∪ b̂)

| 15

Multiple bounding boxes: how to measure performance of predictions ?

Task: We need for Average Precision the count of TPs and of the FPs for a given confidence threshold
t
Difference in Average Precision for Object detection vs Classification:
⊙ classification: for a given threstold t, the ground truth class label decides whether a sample will
be TP or FP
· f (x ) ≥ t and y = 1 ⇒ TP
· f (x ) ≥ t and y = 0 ⇒ FP
⊙ object detection: TP (and FP) defined by the test whether IoU of overlap of boxes is above a
threshold (and whether another predicted box has a higher IoU for a given ground truth box)
⊙ Average Precision for Object detection has two thresholds: IoU threshold (fixed) and class
confidence threshold (varied for computation)

| 16

Outline

1 Performance Measurement for Evaluation at Inference Time
2 Performance Measurement for Loss Computation at Training Time
3 Towards architectures for bounding box prediction
4 Inference Time: Non-maximum suppression and thresholding
5 Further

| 17

Compute a loss

⊙ one difference in match/assign direction between inference and training time:
· Inference time: need to measure whether ground truth boxes are matched by predicted ones.
⇒ assign predicted → ground truth boxes
· For training we need to penalize predicted boxes with bad fit.
⇒ assign predicted ← ground truth boxes
Need to decide for every predicted box: match a ground truth box or not ?
⊙ Step 1: Bipartite Matching: Compute an assignment of ground truth to predicted boxes (!):
· rules: Assign for every predicted box p̂k , b̂k 0, 1 or a few ground truth boxes (ci , bi ) .
⊙ Step 2: compute the final loss L(f (x )(k) , yσ(k) ) between an assigned predicted box (k) and a
matched/assigned ground truth box σ(k)

| 18

Compute a loss: when matched

Lets start at step 2, we have assigned a ground truth box to a predicted box already. Two requirements
⊙ need to measure whether class confidence is high for the ground truth label of the matched
ground truth box
⊙ need to measure whether boxes overlap well
Overall structure of the loss:
L = Lcls + λLloc , λ is a weighting term to balance.
⊙ Two cases (1): f (x )(k) = (p̂, b̂) has been assigned a ground truth box y(l) = (c, b)
⊙ Two cases (2): f (x )(k) = (p̂, b̂) has not been assigned any ground truth box

| 19

Compute a loss: when matched

| 20

Lets start at step 2, we have assigned a ground truth box to a predicted box already
⊙ Step 2: compute the final loss L(f (x )(σ(k)) , y(k) ) between an assigned predicted box (σ(k)) and a
ground truth box (k)
⊙ Faster R-CNN https://arxiv.org/abs/1506.01497:
if f (x )(k) = (p̂, b̂) has been assigned a ground truth box y(k) = (c, b)
L(f (x )(k) , y(k) ) = Lcls (p̂, c) + λLloc (b̂, b)
− log p̂(c)
| {z }

=

+λLloc (b̂, b)

neg log of ground truth cls

X

1

L -loss would be: Lloc (b̂, b) =

|b̂(i) − b(i)|

i∈{x ,y ,w ,h}

X

here: Lloc (b̂, b) =

smoothL1 (|b̂(i) − b(i)|)

i∈{x ,y ,w ,h}

(
smoothL1 (z) =

|z| − 0.5
0.5z 2

if |z| ≥ 1
if |z| < 1

Compute a loss: when matched

| 21

Lets start at step 2, we have assigned a ground truth box to a predicted box already
⊙ Step 2: compute the final loss L(f (x )(σ(k)) , y(k) ) between an assigned predicted box (σ(k)) and a
ground truth box (k)
⊙ Faster R-CNN: if f (x )(k) has been assigned no ground truth box:
L(f (x )(k) , y(k) ) = Lcls (p̂, c = (1, 0, . . .)) =

− log p̂(0)
| {z }

neg log of background cls

· force it to predict background as class, if no ground truth box was assigned
· Lloc = 0 in this case

Compute a loss: when matched

| 22

Lets start at step 2, we have assigned a ground truth box to a predicted box already
⊙ Step 2: compute the final loss L(f (x )(σ(k)) , y(k) ) between an assigned predicted box (σ(k)) and a
ground truth box (k)
⊙ DETR: if f (x )(k) = (p̂, b̂) has been assigned a ground truth box y(k) = (c, b)
L(f (x )(k) , y(k) ) = Lcls (p̂, c) + λLloc (b̂, b)
=

− log p̂(c)
| {z }

+λLloc (b̂, b)

neg log of ground truth cls

Lloc (b̂, b) = λL1

X

|b̂(i) − b(i)| + λiou LGIoU (b̂, b)

i∈{x ,y ,w ,h}

LGIoU (b̂, b) = 1 − IoU(b̂, b) +

ar (C \ (A ∪ B))
, C smallest enclosing convex shape for A∪B
ar (C )

the second term in LGIoU (https://arxiv.org/pdf/1902.09630) adds a penalty if the two shapes are
disjoint and far away. In that case the largest enclosing convex shape gets larger
⊙ if f (x )(k) has been assigned no ground truth box: L(f (x )(k) , y(k) ) =

− log p̂(0)
| {z }

neg log of background cls

0.1

Compute a loss: when matched

Lets start at step 2, we have assigned a ground truth box to a predicted box already
⊙ Step 2: compute the final loss L(f (x )(σ(k)) , y(k) ) between an assigned predicted box (σ(k)) and a
ground truth box (k)
⊙ SSD: page 5 in the SSD paper https://arxiv.org/pdf/1512.02325 - very similar to Faster R-CNN
· Lcls is the neg-log probability for the ground truth class, if a predicted box has a match from
a ground truth class, and if not, then it is the neg-log probability for the background class
· Lloc is a smooth L1 loss, however a predicted box can be matched by multiple ground truth
boxes, so smooth L1 losses are summed over all matches

| 23

Compute a loss: when matched

Overall similar structure:
⊙ Lcls + λLloc
⊙ Lcls is a loss of p̂ to the ground-truth class label, if predicted box has a match from groundtruth
⊙ Lcls is a loss of p̂ to the background class label, if predicted box has no match from groundtruth
⊙ Lloc is non-zero only if there is a match from groundtruth
⊙ Lloc measures some distance between the predicted box and the matched gt box

| 24

Compute a loss: Bipartite matching

Step 1: Bipartite Matching: Compute an assignment of ground truth to predicted boxes (!):
⊙ one simple greedy option:
see 14.4.3.1 in https://d2l.ai/chapter computer-vision/anchor.html
· compute a table of IoUs IoU(b, b̂) between all pairs (b, b̂) ground truth and predicted boxes
· iterate: select largest IoU as matched pair, then delete corresponding row and column,
iterate again
· for remaining predicted boxes: find gt box with largest IoU, assign/match it if IoU is above a
threshold

| 25

Compute a loss: Bipartite matching

Step 1: Bipartite Matching: Compute an assignment of ground truth to predicted boxes (!):
⊙ another greedy way, similar to the above: page 4/5 in https://arxiv.org/pdf/1506.01497, page 4
in https://arxiv.org/pdf/1612.03144 or page 5 in https://arxiv.org/pdf/1512.02325 (Matching
strategy)
· assign if IoU is above some threshold (0.7 for faster r-cnn) to some gt box
· or assign if the predicted box has for some gt box the highest IoU among all predicted boxes

| 26

Compute a loss: Bipartite matching

| 27

Step 1: Bipartite Matching: Compute an assignment of ground truth to predicted boxes (!):
⊙ one option (DETR https://arxiv.org/pdf/2005.12872): try to exactly compute the argmin loss
over assignments
⊙ usually one has more predictions than ground truth boxes. So add fake gt boxes (c = ∅, b = ∅)
until both sets have the same number of elements
⊙ next compute a matching assignment σ ∗ (a permutation, bijective mapping)
σ ∗ = argminσ:{0:N}→{0:N}

N−1
X
k=0

using for example the so-called Hungarian algorithm.
· Note: matches to fake boxes have 0 cost.
· Lmatch is similar to L in step 2

Lmatch (f (x )(σ(k)) , y(k) )

Outline

1 Performance Measurement for Evaluation at Inference Time
2 Performance Measurement for Loss Computation at Training Time
3 Towards architectures for bounding box prediction
4 Inference Time: Non-maximum suppression and thresholding
5 Further

| 28

next steps

We have seen how to evaluate prediction quality at inference time.
We have seen how to compute a loss for training.
Next: architectures to predict bounding boxes

| 29

Multiple bounding boxes (not used in practice in that way!)

Pool 3x3 / s=2

input image: (1, 3, 224, 224)

not used like that in practice:
⊙ too many parameters
⊙ non-locality in prediction of boxes

| 30

Anchor boxes

| 31

CNN

⊙ idea 1: attach a 3x3 (or 1x1) convolution with nb ∗ (4 + C + 1) output channels to the feature
map.
⊙ it predicts nb many boxes at every spatial location of the feature map
⊙ one layer of the input feature map: C channels, so input is 1x1xC for a 1x1 convolution at every
spatial location of the feature map

Anchor boxes

⊙ idea 2: different output boxes for different aspect ratios
⊙ during training match ground truth at a given position only to the box with the most similar
aspect ratio

| 32

What are anchor boxes?

⊙ a template for a bounding box
⊙ a region relative to location in a feature map (and input image), typically defined by its aspect
ratio (height/width)
⊙ the bbox head typically predicts usually offsets relative to the coordinates of a location in a
feature map and of an anchor box, see https:
//lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/#bounding-box-regression
https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/

| 33

What are anchor boxes?

| 34

Example of network heads predicting coordinates relative to an anchor box:
Given:
⊙ anchor box center coordinates d cx , d cy
⊙ anchor box height and width d h , d w
⊙ predicted box coordinates: center g cx , g cy , height and width g h , g w
network is predicting coordinates relative to an anchor box:
⊙ network head predicts relative offsets: gbcx , gbcy , relative height and width gbh , gbw in log-space
gbcx = (g cx − d cx )/d w , gbcy = (g cy − d cy )/d h
gbh = log g h − log d h , gbw = log g w − log d w ,

SSD as example detector

CNN

⊙ is that really used ? https://arxiv.org/pdf/1512.02325

| 35

SSD as example detector

⊙ is that really used ? https://arxiv.org/pdf/1512.02325 SSD, Liu et al.

⊙ yes, but attached at with several conv 3x3, stride 2 adapters. Reason: to decrease feature map
resolution ⇒ predicting larger boxes at lower feature map resolutions

| 36

...

| 37

doing that in code:
⊙ predict a bounding box for every (h, w ) in the last feature map.
⊙ note feature map fmap[b,:,h,w] at (h, w ) is a tensor with K channels
⊙ change: replace class nn.Linear by class nn.Conv2d()
⊙ use one convolution with C + 1 output channels,
⊙ use one convolution with 4 output channels,
def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
# change HERE !!!!
K= 512
self.conv_o1 = nn.Conv2d(K, num_classes+1,(3,3))
self.conv_o2 = nn.Conv2d(K, 4,(3,3)) #4 for bbox
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
#x = self.avgpool(x) #change HERE
classes = self.conv_o1(x)
bboxes = self.conv_o2(x)
return classes, bboxes

...

| 38

doing that in code:
⊙ predict multiple bounding boxes for every (h, w ) in the last feature map.
⊙ note feature map fmap[b,:,h,w] at (h, w ) is a tensor with K channels
⊙ use one convolution with (C + 1) ∗ nb output channels,
⊙ use one convolution with 4 ∗ nb output channels,
def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
# change HERE !!!!
K= 512
self.conv_o1 = nn.Conv2d(K, n_b*(num_classes+1),(3,3))
self.conv_o2 = nn.Conv2d(K, n_b*4,(3,3)) #4 for bbox
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
#x = self.avgpool(x) #change HERE
classes = self.conv_o1(x)
bboxes = self.conv_o2(x)
return classes, bboxes

...

| 39

def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
# change HERE !!!!
K= 512
self.conv_o1 = nn.Conv2d(K, n_b*(num_classes+1),(3,3))
self.conv_o2 = nn.Conv2d(K, n_b*4,(3,3)) #4 for bbox
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
#x = self.avgpool(x) #change HERE
classes = self.conv_o1(x)
bboxes = self.conv_o2(x)
return classes, bboxes

⊙ is this close to real code ? check lines 223 and 231 in
https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py in
class PredictionConvolutions

...

| 40

def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
# change HERE !!!!
K= 512
self.conv_o1 = nn.Conv2d(K, n_b*(num_classes+1),(3,3))
self.conv_o2 = nn.Conv2d(K, n_b*4,(3,3)) #4 for bbox
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
#x = self.avgpool(x) #change HERE
classes = self.conv_o1(x)
bboxes = self.conv_o2(x)
return classes, bboxes

⊙ is this close to real code ? check lines 108 and 117 (and 62) in
https://github.com/pytorch/vision/blob/main/torchvision/models/detection/ssd.py in
class SSDClassificationHead, class SSDRegressionHead

...

| 41

⊙ almost a (simplified) one-stage detector. Missing parts?
⊙ sometimes some conv-batch-norm-relu blocks in between (not in vanilla SSD, but in faster rcnn )
⊙ How to deal with too many predictions: NMS - see slides on NMS
⊙ multiple box sizes
def __init__(self,...):
# more code here
self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])
# change HERE !!!!
K= 512
self.conv_o1 = nn.Conv2d(K, n_b*(num_classes+1),(3,3))
self.conv_o2 = nn.Conv2d(K, n_b*4,(3,3)) #4 for bbox
def _forward_impl(self, x: Tensor) -> Tensor:
# See note [TorchScript super()]
x = self.conv1(x)
x = self.bn1(x)
x = self.relu(x)
x = self.maxpool(x)
x = self.layer1(x)
x = self.layer2(x)
x = self.layer3(x)
x = self.layer4(x)
#x = self.avgpool(x) #change HERE
classes = self.conv_o1(x)
bboxes = self.conv_o2(x)
return classes, bboxes

Multiple box sizes

One idea shown here is: line 189 class PredictionConvolutions
https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py
⊙ do the above prediction with 2 conv layers not on a single feature map
⊙ do the above prediction with 2 conv layers on several feature maps extracted from your backbone
network
⊙ idea: boxes of the same size over a feature map encode larger objects for smaller feature maps:
· if input is 224 and feature map is 112, then one pixel of the feature map encodes 2x2 in the
input image,
· if feature map is 56, then one pixel of the feature map encodes 4x4 in the input image
⊙ other idea shown for SSD ... several blocks of 3x3 Conv, stride 2

| 42

Outline

1 Performance Measurement for Evaluation at Inference Time
2 Performance Measurement for Loss Computation at Training Time
3 Towards architectures for bounding box prediction
4 Inference Time: Non-maximum suppression and thresholding
5 Further

| 43

NMS

Problem: too many boxes predicted !
(even if you remove those with too low class confidence ...)
⊙ need to filter for inference

| 44

NMS

| 45

https://www.youtube.com/watch?v=VAo84c1hQX8
The coarse idea:
⊙ run in for-loop over every class c
⊙ create a ”work-list” of all predicted boxes of this class. create list of ”to-keep” boxes
⊙ throw away boxes with too low probability for this class c
⊙ iterate until ”work-list” is empty:
· find highest probability box in work list, throw away all boxes with large IoU overlap to this
box
· move current highest probability box to list of tokeep boxes

NMS

| 46

https://www.youtube.com/watch?v=VAo84c1hQX8

run for every class separately:
⊙ remove all boxes with class confidence below some threshold
⊙ init: create work-list of all predicted boxes, create empty
to-keep-list
⊙ iterate while work-list is not empty:
· curbox = find box in work-list with highest confidence
· remove from work-list all boxes with IoU overlap to curbox over a
certain threshold
· add curbox to to-keep-list
uses two thresholds: confidence threshold, IoU threshold for overlap

Outline

1 Performance Measurement for Evaluation at Inference Time
2 Performance Measurement for Loss Computation at Training Time
3 Towards architectures for bounding box prediction
4 Inference Time: Non-maximum suppression and thresholding
5 Further

| 47

Fusing information from multiple feature maps

key idea: Feature pyramid network, Fig 3 in
https://arxiv.org/abs/1612.03144
⊙ low res feature maps deeper in the network
encode more complex concepts
⊙ high res feature maps earlier in the network
encode more simple concepts
⊙ want at multiple resolutions a trainable fusion
of these two extremes

| 48

Fusing information from multiple feature maps
key idea: Feature pyramid network, Fig 3 in
https://arxiv.org/abs/1612.03144
⊙ using upsampling (e.g. copy one feature map
element into 2x2 ... to double the resolution,
then interpolate at inserted positions)
⊙ or Transposed convolutions
https://d2l.ai/chapter computer-vision/
transposed-conv.html
⊙ goal: to get the feature map to higher size,
then sum of feature maps (upsampled and
from backbone those with the target
resolution)
⊙ predict at multiple feature map resolutions,
because boxes of same scale encode larger
objects at lower resolutions)

| 49

From one stage to 2-stage detection

| 50

⊙ you have a magic region proposal network
(RPN), it returns you a list of candidate boxes
⊙ RPN is like a detector-type neural net: it
outputs box coordinates and only a
one-dimensional objectness probability.
⊙ objectness is a measure for how likely the box
overlaps with an object, no matter what class
⊙ trained like an object detector but only with
object-vs-background as classification

From one stage to 2-stage detection

| 51

Faster R-CNN as long standing former state of the
art:
Ren et al. https://arxiv.org/pdf/1506.01497

From one stage to 2-stage detection

⊙ you have a magic region proposal network
(RPN), it returns you a list of candidate boxes

| 52

⊙ change: you use two class nn.Linear
output heads again, not class nn.Conv2d
... Why that ?
· no prediction at every spatial coordinate
· input is a list of feature vectors at
certain locations, with certain aspect
ratios and size. It is a tensor of shape
(num batch, num boxes, num channels)
· torch.nn.Linear can deal with an
input tensor of shape
(num batch, num boxes, num channels)

From one stage to 2-stage detection

⊙ you have a magic region proposal network
(RPN), it returns you a list of candidate boxes

| 53

⊙ you use two class nn.Linear output heads
again
⊙ output dimensions are again num_classes+1
(class probabilities for the candidate box) and
num_classes * 4 (bbox coordinate offsets
for the candidate box)
self.cls_score = nn.Linear(in_channels,
num_classes)
self.bbox_pred = nn.Linear(in_channels,
num_classes * 4)

See:
https://github.com/pytorch/vision/blob/main/
torchvision/models/detection/faster rcnn.py#L359

From one stage to 2-stage detection

⊙ you have a magic region proposal network
(RPN), it returns you a list of candidate boxes

| 54

⊙ you use two class nn.Linear output heads
again
· apply them onto feature vector from
every pooled region
self.cls_score = nn.Linear(in_channels,
num_classes)
self.bbox_pred = nn.Linear(in_channels,
num_classes * 4)

⊙ missing step: one needs to pool a feature map
over the ROI region.
· Usually avg- or max-pooling e.g. ROI
pooling layer as explained in https:
//viso.ai/deep-learning/faster-r-cnn-2/

Alternatives to anchor boxes

| 55

anchor-free detection methods
example FCOS:
⊙ predict for one location (x , y ): object class,
l, r , t, b offsets from (x , y ), for the case if (x , y )
would be the center of a box +centerness
⊙ FCOS https://arxiv.org/pdf/1904.01355,
⊙ DETR https://arxiv.org/pdf/2005.12872
⊙ out of exams: YOLOv1, CornerNet
https://arxiv.org/pdf/1808.01244,
CenterNet
https://arxiv.org/pdf/1904.08189

Alternatives to anchor boxes

anchor-free detection methods
example FCOS: predict for one location (x , y ):
⊙ object class confidence if (x , y ) would be the
center of a box ,
⊙ l, r , t, b offsets from (x , y ), for the case if
(x , y ) would be the center of a box
⊙ centerness - normalized distance (in [0, 1]) to
box center,
to be multiplied with object class confidence,
then NMS will filter boxes with low centerness

| 56

Alternatives to anchor boxes

| 57

anchor-free detection methods
example FCOS:
⊙ predict for each location (x , y ):

⊙ FCOS https://arxiv.org/pdf/1904.01355,
⊙ DETR https://arxiv.org/pdf/2005.12872
⊙ out of exams: YOLOv1, CornerNet
https://arxiv.org/pdf/1808.01244,
CenterNet https://arxiv.org/pdf/1904.08189

· object class prob,
· l, r , t, b offsets from (x , y ), for the case
if (x , y ) would be the center of a box,
· centerness

