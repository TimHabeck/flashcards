id,front_html,back_html,source_file,tag
"20260121-1618-01-1","What are the three fundamental properties of the inner product \(u \cdot v\)?","<ul><li><strong>Linearity</strong>: \((a_1 u^{(1)} + a_2 u^{(2)}) \cdot v = a_1 (u^{(1)} \cdot v) + a_2 (u^{(2)} \cdot v)\)</li><li><strong>Symmetry</strong>: \(u \cdot v = v \cdot u\)</li><li><strong>Positive Definiteness</strong>: \(v \cdot v > 0\) for \(v \neq 0\)</li></ul>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-2","How is the angle \(\theta\) between two vectors \(u\) and \(v\) related to their inner product?","The inner product defines the angle via the formula: <p>\(\cos(\theta) = \frac{u \cdot v}{\|u\|_2 \|v\|_2}\)</p>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-3","For unit vectors \(\|u\| = \|v\| = 1\), what does the inner product \(u \cdot v\) represent?","It represents a <strong>similarity measure</strong> based on the angle between the vectors: <ul><li>\(u \cdot v = 1\) if \(u = v\) (angle 0)</li><li>\(u \cdot v = 0\) if \(u \perp v\) (angle 90°)</li><li>\(u \cdot v = -1\) if \(u = -v\) (angle 180°)</li></ul>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning inner-product"
"20260121-1618-01-4","What is the mechanism of the simplest linear classifier \(f(x) = w \cdot x + b\)?","It computes the inner product between the <strong>weight vector</strong> \(w\) and <strong>input vector</strong> \(x\) plus a <strong>bias</strong> \(b\). The classifier assigns large values to inputs that are <strong>geometrically similar</strong> (small angle) to the weight vector \(w\).","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning linear-classifier"
"20260121-1618-01-5","Why is the sign function \(s(x) = \text{sign}(w \cdot x + b)\) insufficient for ""uncertainty-aware"" classification?","The sign function only provides a hard class label (-1 or 1). To encode <strong>uncertainty</strong>, we need a mapping to \([0, 1]\) where: <ul><li>\(f(x) \approx 0 \implies s(f(x)) \approx 0.5\) (Uncertain)</li><li>\(f(x) \gg 0 \implies s(f(x)) \approx 1\) (Confident Class 1)</li><li>\(f(x) \ll 0 \implies s(f(x)) \approx 0\) (Confident Class 0)</li></ul>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-6","What is the definition of the logistic sigmoid function \(\sigma(u)\)?","<blockquote>\(\sigma(u) = \frac{1}{1 + \exp(-u)} = \frac{\exp(u)}{1 + \exp(u)}\)</blockquote>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-7","How does the scaling factor \(c > 0\) affect the shape of the scaled sigmoid \(s(cu)\)?","Increasing \(c\) makes the transition from 0 to 1 <strong>steeper</strong>. As \(c \to \infty\), the sigmoid approaches a <strong>step function</strong>.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning sigmoid"
"20260121-1618-01-8","What is ""Probit regression"" in the context of binary classification?","A regression model that uses the <strong>Cumulative Distribution Function (CDF)</strong> of a Normal Distribution (Gaussian) as the activation function, instead of the logistic sigmoid.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning probit"
"20260121-1618-01-9","What is the formula for the Binary Cross-Entropy (BCE) loss for a single sample with label \(y \in \{0, 1\}\) and prediction \(s(x)\)?","<blockquote>\(e(x, y) = -y \ln(s(x)) - (1 - y) \ln(1 - s(x))\)</blockquote>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-10","How do you interpret the Binary Cross-Entropy loss in terms of probability?","It is the <strong>negative log-probability</strong> of the ground truth class: <ul><li>If \(y=1\), loss is \(-\ln(P(Y=1|x))\)</li><li>If \(y=0\), loss is \(-\ln(P(Y=0|x))\)</li></ul>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-11","What happens to the Binary Cross-Entropy loss if the model predicts \(s(x)=0\) but the ground truth is \(y=1\)?","The loss approaches <strong>infinity</strong> (\(\lim_{x \to 0, x > 0} -\ln(x) = \infty\)).","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning cross-entropy"
"20260121-1618-01-12","Why is the neg-logarithm a good choice for a loss function in gradient-based optimization?","<ul><li>Low loss when prediction is close to ground truth (\(-\ln(1) = 0\)).</li><li>High loss when far from ground truth.</li><li>It is <strong>almost everywhere differentiable</strong>, which is required for gradient descent.</li></ul>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning optimization"
"20260121-1618-01-13","What is the Principle of Maximum Likelihood (MLP)?","A principle for choosing model parameters \(\theta\) such that the <strong>probability of observing the given data</strong> is maximized: <blockquote>\(\theta^* = \text{argmax}_\theta P(\text{Data}|\theta)\)</blockquote>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-14","Why is it common to minimize the negative log-likelihood instead of maximizing the likelihood itself?","<ul><li>Logarithm transforms <strong>products into sums</strong>, making differentiation easier.</li><li>It is numerically more stable (avoids vanishingly small products).</li><li>Log is a monotonic function, so it preserves the location of the optimum.</li></ul>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-15","What assumption is required to express the total probability of \(n\) samples as a product: \(P(Y_0, \dots, Y_n) = \prod P(Y_k)\)?","The assumption of <strong>statistical independence</strong> (specifically, conditional independence of \(Y\) given \(X\)).","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-16","[DERIVATION] What is the ""trick of pulling out a constant"" used in the derivation of cross-entropy from MLP?","When summing over multiple variables, \(P(Y_0|X_0)\) can be pulled out of summations over \(Y_1, \dots, Y_n\) because it is <strong>constant</strong> with respect to those variables: <p>\(\sum_{Y_0}\sum_{Y_1} c(Y_0)t(Y_1) = \sum_{Y_0} c(Y_0) \sum_{Y_1} t(Y_1)\)</p>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning maximum-likelihood"
"20260121-1618-01-17","How are labels typically represented in multi-class classification where classes are mutually exclusive?","Using a <strong>one-hot vector</strong> \(\vec{y} \in \mathbb{R}^C\), where \(y[k] = 1\) if the sample belongs to class \(k\), and 0 otherwise.","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1618-01-18","What is the formula for Multi-class Cross-Entropy loss for a sample with one-hot label \(\vec{y}\) and predicted probabilities \(P(Y=k|x)\)?","<blockquote>\(\text{Loss} = - \sum_{k=0}^{C-1} y[k] \ln(P(Y=k|x))\)</blockquote>","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
"20260121-1618-01-19","In Multi-class Cross-Entropy, how does the sum simplify for a specific ground truth label \(y\)?","Since only one index \(k\) in the one-hot vector is 1, the sum collapses to the <strong>negative log-probability of the correct class</strong>: \(-\ln(P(Y=y|x))\).","week 1 - slides_l2_logreg.txt","week-1 logistic-regression machine-learning multi-class"
